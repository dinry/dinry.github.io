<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dinry</title>
  
  <subtitle>notebook</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://dinry.github.io/"/>
  <updated>2019-10-08T07:37:19.588Z</updated>
  <id>http://dinry.github.io/</id>
  
  <author>
    <name>dinry</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>花书第二周--机器学习基本概念</title>
    <link href="http://dinry.github.io/%E8%8A%B1%E4%B9%A6%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%881%EF%BC%89/"/>
    <id>http://dinry.github.io/花书第二周（1）/</id>
    <published>2019-10-08T05:53:29.000Z</published>
    <updated>2019-10-08T07:37:19.588Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="机器学习算法">机器学习算法</span></h1><ol><li>什么是学习？对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务P上由性能度量P衡量的性能有所提升。</li><li>什么是机器学习算法？能够从数据中学习的算法</li></ol><h2><span id="11-任务t">1.1 任务T</span></h2><h4><span id="机器学习的任务是什么">机器学习的任务是什么？</span></h4><p>机器学习系统应该如何处理样本，即对严办进行一个复杂的非线性变化从而得到正确的结果</p><p>我们通常会将样本表示成一个向量 $x \in \mathbb{R}^n$, 其中向量的每一个元素是一个特征</p><h4><span id="常见的任务有哪些">常见的任务有哪些？</span></h4><ul><li>分类： $f: \mathbb{R}^n \to {1,...,k}$ （输出离散，输出为概率分布）</li><li>回归： $f: \mathbb{R}^n \to \mathbb{R}$ （输出连续）</li><li>转录：OCR, ASR</li><li>机器翻译： seq2seq</li><li>结构化输出：输出值之间内部密切相关，如语法树</li><li>异常检测</li><li>合成与检测</li><li>缺失值填充</li><li>去噪</li><li>密度估计</li></ul><p>本节重点讲了分类回归问题的区别</p><p>度量：</p><p>分类：precision, recall, auc, roc, f1</p><p>回归: mse</p><h2><span id="12-经验">1.2 经验</span></h2><p>无监督学习：</p><p>含有很多特征的数据集，但是没有label</p><p>监督学习：</p><p>（线性回归，LDA,SVM)数据集有label or tag</p><h1><span id="容量过拟合和欠拟合">容量，过拟合和欠拟合</span></h1><p>训练集：测试集：衡量模型的好坏</p><ul><li><p>underfitting: 参数太少，模型简单</p></li><li><p>appropriate capacity: 泛化能力最好</p></li><li><p>overfitting: 模型复杂，训练集误差小，测试集误差大</p></li></ul><p>模型泛化：模型容量</p><p>泛化误差：测试误差</p><h2><span id="model原则">model原则：</span></h2><ul><li>剃刀原则： 若有多个假设与观察一致，选择最简单的</li><li>没有免费午餐定理：不存在能够在所有可能的分类问题中性能均为最优的算法</li><li>解决方案：尽可能深入了解分布，寻找先验知识</li><li>正则化：降低泛化误差而非训练误差，L1,L2（为什么？）</li></ul><p>$J(w)=MSE_{train}+\lambda w^{\top}w$</p><h2><span id="超参数和验证集">超参数和验证集</span></h2><p>超参数：用于挑选超参数的数据子集成为验证集，通常8：2</p><p>验证集：交叉验证，留出法，k折交叉验证</p><p>实际工作经验：训练集，交叉验证集，测试集</p><p>训练集：训练数据</p><p>交叉验证集：判断学习率是否要调整，何时结束训练，每一个epoch都测试</p><p>一般来讲，训练数据每过一个epoch, 都要在交叉验证集上看一下</p><p>性能：损失函数</p><p>测试集：判断模型的性能好坏，最后用</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;机器学习算法&quot;&gt;机器学习算法&lt;/span&gt;&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;什么是学习？
对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务P上由性能度量P衡量的性能有所提升。&lt;/li&gt;
&lt;li&gt;什么是机器学
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>花书作业3（梯度下降小程序）</title>
    <link href="http://dinry.github.io/%E8%8A%B1%E4%B9%A6%E4%BD%9C%E4%B8%9A3/"/>
    <id>http://dinry.github.io/花书作业3/</id>
    <published>2019-10-06T08:43:50.000Z</published>
    <updated>2019-10-06T08:47:06.777Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">def f2(x,y):</span><br><span class="line">    return np.exp(x*x+(y-2)*(y-2))</span><br><span class="line">def hx1(x,y):</span><br><span class="line">    return 2*x*np.exp(x*x+(y-2)*(y-2))</span><br><span class="line">def hx2(x,y):</span><br><span class="line">    return 2*(y-2)*np.exp(x*x+(y-2)*(y-2))</span><br><span class="line">X1 = np.arange(0,1,0.1)</span><br><span class="line">X2 = np.arange(0,1,0.1)</span><br><span class="line">X1, X2 = np.meshgrid(X1, X2) # 生成xv、yv，将X1、X2变成n*m的矩阵，方便后面绘图</span><br><span class="line">#pdb.set_trace()</span><br><span class="line">Y = np.array(list(map(lambda t : f2(t[0],t[1]),zip(X1.flatten(),X2.flatten()))))</span><br><span class="line">Y.shape = X1.shape # 1600的Y图还原成原来的（40,40）</span><br><span class="line"></span><br><span class="line">x1 = 1</span><br><span class="line"></span><br><span class="line">x2 = 1</span><br><span class="line"></span><br><span class="line">alpha = 0.01</span><br><span class="line"></span><br><span class="line">GD_X1 = [x1]</span><br><span class="line"></span><br><span class="line">GD_X2 = [x2]</span><br><span class="line"></span><br><span class="line">GD_Y = [f2(x1, x2)]</span><br><span class="line"></span><br><span class="line">y_change = f2(x1, x2)</span><br><span class="line"></span><br><span class="line">iter_num = 0</span><br><span class="line"></span><br><span class="line">while (y_change &gt; 1e-20):</span><br><span class="line">    tmp_x1 = x1 - alpha * hx1(x1, x2)</span><br><span class="line"></span><br><span class="line">    tmp_x2 = x2 - alpha * hx2(x1, x2)</span><br><span class="line"></span><br><span class="line">    tmp_y = f2(tmp_x1, tmp_x2)</span><br><span class="line"></span><br><span class="line">    f_change = np.absolute(tmp_y - f2(x1, x2))</span><br><span class="line">    y_change=f_change</span><br><span class="line"></span><br><span class="line">    x1 = tmp_x1</span><br><span class="line"></span><br><span class="line">    x2 = tmp_x2</span><br><span class="line"></span><br><span class="line">    GD_X1.append(x1)</span><br><span class="line"></span><br><span class="line">    GD_X2.append(x2)</span><br><span class="line"></span><br><span class="line">    GD_Y.append(tmp_y)</span><br><span class="line"></span><br><span class="line">    iter_num += 1</span><br><span class="line"></span><br><span class="line">print(u&quot;最终结果为:(%.5f, %.5f, %.5f)&quot; % (x1, x2, f2(x1, x2)))</span><br><span class="line"></span><br><span class="line">print(u&quot;迭代过程中X的取值，迭代次数:%d&quot; % iter_num)</span><br><span class="line"></span><br><span class="line">print(GD_X1)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(facecolor=&apos;w&apos;, figsize=(20, 18))</span><br><span class="line"></span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line">ax.plot_surface(X1, X2, Y, rstride=1, cstride=1, cmap=plt.cm.jet)</span><br><span class="line"></span><br><span class="line">ax.plot(GD_X1, GD_X2, GD_Y, &apos;ko-&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(&apos;x&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(&apos;y&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_zlabel(&apos;z&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_title(u&apos;函数;\n学习率:%.3f; 最终解:(%.3f, %.3f, %.3f);迭代次数:%d&apos; % (alpha, x1, x2, f2(x1, x2), iter_num))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p><p><img src="/%E8%8A%B1%E4%B9%A6%E4%BD%9C%E4%B8%9A3/1.JPG" alt><img src="/%E8%8A%B1%E4%B9%A6%E4%BD%9C%E4%B8%9A3/2.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span cl
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="Deep learning" scheme="http://dinry.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计，误差的高斯分布与最小二乘估计的等价性,最优化</title>
    <link href="http://dinry.github.io/MLE/"/>
    <id>http://dinry.github.io/MLE/</id>
    <published>2019-10-04T02:42:22.000Z</published>
    <updated>2019-10-05T06:51:41.691Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="极大似然估计">极大似然估计</span></h1><p>假设随机变量 $X-P(x;\theta)$</p><p>现有样本 $x_1$, $x_2$, ... $x_N$</p><p>定义似然函数为 $L=P(x_1;\theta)P(x_2;\theta) ... P(x_N;\theta)$</p><p>对数似然函数为 $\hat{L}=ln[P(x_1;\theta)P(x_2;\theta) ... P(x_N;\theta)]$</p><p>极大似然估计为 max L</p><p>高斯分布 $P(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$</p><p>$L=ln[\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_1-\mu)^2}{2\sigma^2}},\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_2-\mu)^2}{2\sigma^2}},...,\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_N-\mu)^2}{2\sigma^2}}]$</p><p>$\frac{\partial L}{\partial \mu}=0  \to \mu=\frac{x_1+x_2+...+x_N}{N}$</p><p>$\frac{\partial L}{\partial \sigma}=0 \to \sigma^2=\frac{\sum_{i=1}^N(x_i-\mu)^2}{N}$ (有偏方差)</p><h1><span id="误差的高斯分布与最小二乘估计的等价性">误差的高斯分布与最小二乘估计的等价性</span></h1><p>$x_1,x_2,...,x_N, x_i \in \mathbb{R}^n$</p><p>$y_1,y_2,...,y_N, y_i \in \mathbb{R}^n$</p><p>$\hat{y_i}=w^\top x_i, w \in \mathbb{R}^n$</p><p>拟合误差： $e_i=y_i - w^\top x_i$</p><p>若设$e_i-\frac{1}{\sqrt{2\pi}}e^{\frac{e_i^2}{2}}$</p><p>似然函数： $L=ln[\frac{1}{\sqrt{2\pi}}e^{\frac{e_1^2}{2}} \frac{1}{\sqrt{2\pi}}e^{\frac{e_2^2}{2}}...\frac{1}{\sqrt{2\pi}}e^{\frac{e_N^2}{2}}]=-Nln\sqrt{2\pi}-\frac{1}{2}(e_1^2+e_2^2+...+e_N^2)$</p><p>最大化L等价于最小化 $e_1^2+e_2^2+...+e_N^2$</p><p>$min(y_1-w^\top x_1)^2+(y_2-w^\top x_2)^2+...+(y_N-w^\top x_N)^2=J$</p><p>$\frac{\partial J}{\partial w}=0 \to \sum_{i=1}^{N}x_iy_i=\sum_{i=1}^N w^\top x_i x_i$</p><h1><span id="无约束优化">无约束优化</span></h1><p>无约束优化问题是机器学习中最普遍，最简单的优化问题： $x^*=min_{x}f(x)$</p><h2><span id="梯度下降">梯度下降</span></h2><p>沿负梯度方向</p><h2><span id="牛顿法">牛顿法</span></h2><p>$f(x_{t+1})=f(x_t)+f^&quot;(x_t)(x_{t+1}-x_t)$</p><h2><span id="拟牛顿">拟牛顿</span></h2><h2><span id="共轭梯度">共轭梯度</span></h2><h2><span id="收敛速度">收敛速度</span></h2><p>梯度下降是一次收敛，牛顿是二次收敛（速度快，但接近最优点才收敛,否则发散）</p><h1><span id="有约束最优化">有约束最优化</span></h1><h2><span id="拉格朗日乘子法约束为等式">拉格朗日乘子法(约束为等式)</span></h2><p>$min_x f(x)$</p><p>$s.t.g(x)=0$</p><p>$L(x,\lambda)=f(x)+\lambda g(x)$</p><h2><span id="kkt-约束为不等式表示一个范围">KKT （约束为不等式，表示一个范围）</span></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;极大似然估计&quot;&gt;极大似然估计&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;假设随机变量 $X-P(x;\theta)$&lt;/p&gt;
&lt;p&gt;现有样本 $x_1$, $x_2$, ... $x_N$&lt;/p&gt;
&lt;p&gt;定义似然函数为 $L=P(x_1;\theta)P(x_2;
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>花书打卡2 （极大似然估计以及优化理论）</title>
    <link href="http://dinry.github.io/%E8%8A%B1%E4%B9%A6%E6%89%93%E5%8D%A12/"/>
    <id>http://dinry.github.io/花书打卡2/</id>
    <published>2019-10-03T06:36:06.000Z</published>
    <updated>2019-10-05T00:38:18.665Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="chapter-3-概率与信息论">Chapter 3: 概率与信息论</span></h1><h2><span id="31-概率的使用">3.1 概率的使用</span></h2><p>机器学习通常必须处理不确定量，有时也要处理随机量，概率用来量化这些不确定性</p><p>不确定性的三种来源：</p><ul><li>被建模系统内在的随机性</li><li>不完全观测</li><li>不完全建模</li></ul><h2><span id="32-随机变量">3.2 随机变量</span></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;chapter-3-概率与信息论&quot;&gt;Chapter 3: 概率与信息论&lt;/span&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;span id=&quot;31-概率的使用&quot;&gt;3.1 概率的使用&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;机器学习通常必须处理不确定量，有时也要处理随机量，概率用来
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>python获取交集，并集，差集的方法</title>
    <link href="http://dinry.github.io/%E8%8E%B7%E5%8F%96%E4%BA%A4%E9%9B%86%EF%BC%8C%E5%B9%B6%E9%9B%86%EF%BC%8C%E5%B7%AE%E9%9B%86%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/获取交集，并集，差集的方法/</id>
    <published>2019-09-21T03:01:53.000Z</published>
    <updated>2019-09-21T03:04:44.294Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="交集">交集</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#方法一:</span><br><span class="line">a=[2,3,4,5]</span><br><span class="line">b=[2,5,8]</span><br><span class="line">tmp = [val for val in a if val in b]</span><br><span class="line">print tmp</span><br><span class="line">#[2, 5]</span><br><span class="line"></span><br><span class="line">#方法二</span><br><span class="line">print list(set(a).intersection(set(b)))</span><br><span class="line"></span><br><span class="line">#方法二比方法一快很多！</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p><h1><span id="并集">并集</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print list(set(a).union(set(b)))</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p><h1><span id="获取两个-list-的差集">获取两个 list 的差集</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print list(set(b).difference(set(a))) # b中有而a中没有的      非常高效！</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;交集&quot;&gt;交集&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span 
      
    
    </summary>
    
      <category term="python" scheme="http://dinry.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://dinry.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Word Embedding</title>
    <link href="http://dinry.github.io/embedding/"/>
    <id>http://dinry.github.io/embedding/</id>
    <published>2019-09-19T10:33:25.000Z</published>
    <updated>2019-09-20T00:56:22.540Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="dimension-reduction">dimension reduction</span></h1><p><img src="/embedding/1.JPG" alt></p><h1><span id="word-embedding">word Embedding</span></h1><ul><li>Machine learn the meaning of words from reading a lot of documents without supervision.</li><li>Generating Word Vector is Unsupervised</li><li>A word can be understood by its context.<img src="/embedding/2.JPG" alt></li></ul><h1><span id="how-to-exploit-the-context">How to exploit the context?</span></h1><ul><li>count based: If two words $w_i$ and $w_j$ frequently co-occur, $V(w_i)$ and $V(w_j)$ would be close to each other.(Glove Vector)</li></ul><p>$V(w_i) \cdot V(w_j) \to N_{i,j}$, where number of times $w_i$ and $w_j$ in the same document.</p><ul><li>prediction based: predict next word based on previous words.</li></ul><p><img src="/embedding/3.JPG" alt></p><ul><li>take out he input of the neurons in the first layer.</li><li>use it to represent a word w</li><li>word vector. word embedding feature: V(w)具有相同上下文的单词具有相近的分布<img src="/embedding/4.JPG" alt><img src="/embedding/5.JPG" alt><img src="/embedding/6.JPG" alt>如何让两个weight一样？一样有什么好处？</li><li>Given the same initialization</li><li><img src="/embedding/7.JPG" alt></li><li>cross entropy: <img src="/embedding/8.JPG" alt></li></ul><h2><span id="two-class">two class:</span></h2><ul><li>Cbow</li><li>skip-gram<img src="/embedding/9.JPG" alt>结构信息：结构，包含关系等<img src="/embedding/10.JPG" alt><img src="/embedding/11.JPG" alt></li></ul><h1><span id="document-embedding">document Embedding</span></h1><p><img src="/embedding/12.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;dimension-reduction&quot;&gt;dimension reduction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/embedding/1.JPG&quot; alt&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;word-embedding&quot;&gt;wor
      
    
    </summary>
    
      <category term="NLP" scheme="http://dinry.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://dinry.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理NLP中的N-gram模型</title>
    <link href="http://dinry.github.io/n-gram/"/>
    <id>http://dinry.github.io/n-gram/</id>
    <published>2019-09-16T11:45:18.000Z</published>
    <updated>2019-09-17T01:03:58.949Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="naive-bayes">Naive Bayes</span></h1><p>见 https://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/， 这里再复习一下。</p><p>朴素贝叶斯的关键组成是贝叶斯公式与条件独立性假设。为了方便说明，我们举一个垃圾短信分类的例子。</p><p><strong>&quot;在家日赚百万，惊人秘密...&quot;</strong></p><p>$p(垃圾短信 \mid &quot;在家日赚百万&quot;)∝p(垃圾邮件)p(&quot;在家日赚百万&quot;\mid 垃圾短信)$</p><p>由条件独立性假设：</p><p>$p(&quot;在家日赚百万&quot; \mid J)=p(&quot;在&quot;,&quot;家&quot;,&quot;日&quot;,&quot;赚&quot;,&quot;百&quot;,&quot;万&quot;∣J)=p(&quot;在&quot; \mid J)p(&quot;家&quot;\mid J)p(&quot;日&quot;\mid J)p(&quot;赚&quot;\mid J)p(&quot;百&quot;\mid J)p(&quot;万&quot;\mid J)$</p><p>上面每一项条件概率都可以通过在训练数据的垃圾短信中统计每个字出现的次数得到，然而这里有一个问题，朴素贝叶斯将句子处理为一个 <strong>词袋模型（Bag-of-Words, BoW）</strong> ，以至于不考虑每个单词的顺序。这一点在中文里可能没有问题，因为有时候即使把顺序捣乱，我们还是能看懂这句话在说什么，但有时候不行，例如：</p><p><strong>我烤面筋 = 面筋烤我 ？</strong></p><p>那么有没有模型是考虑句子中单词之间的顺序的呢？有，N-gram就是。</p><h1><span id="n-gram">N-gram</span></h1><h2><span id="n-gram简介">N-gram简介</span></h2><p>在介绍N-gram之前，让我们回想一下**“联想”**的过程是怎样发生的。如果你是一个玩LOL的人，那么当我说“正方形打野”、“你是真的皮”，“你皮任你皮”这些词或词组时，你应该能想到的下一个词可能是“大司马”，而不是“五五开”。如果你不是LOL玩家，没关系，当我说“上火”、“金罐”这两个词，你能想到的下一个词应该更可能“加多宝”，而不是“可口可乐”。</p><p>N-gram正是基于这样的想法，它的第一个特点是某个词的出现依赖于其他若干个词，第二个特点是我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。</p><p>N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。</p><p><img src="/n-gram/1.JPG" alt></p><p>N-gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有 $Bi-gram(N=2)$ 和 $Tri-gram(N=3)$，一般已经够用了。例如在上面这句话里，我可以分解的 Bi-gram 和 Tri-gram ：</p><p>Bi-gram :  {I, love}, {love, deep}, {love, deep}, {deep, learning}</p><p>Tri-gram :  {I, love, deep}, {love, deep, learning}</p><h2><span id="n-gram中的概率计算">N-gram中的概率计算</span></h2><p>假设我们有一个由n个词组成的句子 $S=(w_1​,w_2​,⋯,w_n​)$，如何衡量它的概率呢？让我们假设，每一个单词 $w_i$ ​都要依赖于从第一个单词 $w_1$ ​到它之前一个单词 $w_{i−1}$​的影响：</p><p>$p(S)=p(w_1w_2⋯w_n)=p(w_1)p(w_2 \mid w_1)⋯p(w_n \mid w_{n−1}⋯w_2w_1)$</p><p>是不是很简单？是的，不过这个衡量方法有两个缺陷：</p><ul><li>参数空间过大，概率 $p(w_n \mid w_{n−1}⋯w_2w_1)$ 的参数有 $O(n)$ 个。</li><li>数据稀疏严重，词同时出现的情况可能没有，组合阶数高时尤其明显。</li></ul><p>为了解决第一个问题，我们引入马尔科夫假设（Markov Assumption）：一个词的出现仅与它之前的若干个词有关。</p><p>$p(w_1⋯w_n)=\prod p(w_i \mid w_{i-1}⋯w_1)=\prod p(w_i \mid w_{i-1}⋯w_{i-N+1})$</p><ul><li>如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 Bi-gram：$p(S)=p(w_1w_2⋯w_n)=p(w_1)p(w_2 \mid w_1)⋯p(w_n \mid w_{n-1})$</li><li>如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 Tri-gram</li></ul><p>N-gram的N可以取很高，然而现实中一般 bi-gram 和 tri-gram 就够用了。</p><p>那么，如何计算其中的每一项条件概率 $p(w_n \mid w_{n−1}⋯w_2w_1)$ 呢？答案是 <strong>极大似然估计（Maximum Likelihood Estimation，MLE）</strong>，说人话就是数频数：</p><p><img src="/n-gram/2.JPG" alt></p><h1><span id="n-gram-的用途">N-gram 的用途</span></h1><h2><span id="用途一词性标注">用途一：词性标注</span></h2><p><img src="/n-gram/3.JPG" alt></p><h2><span id="用途二垃圾短信分类">用途二：垃圾短信分类</span></h2><p><img src="/n-gram/4.JPG" alt></p><h2><span id="用途三分词器">用途三：分词器</span></h2><p><img src="/n-gram/5.JPG" alt></p><h2><span id="用途四机器翻译和语音识别">用途四：机器翻译和语音识别</span></h2><p><img src="/n-gram/6.JPG" alt></p><h1><span id="n-gram中n的确定">N-gram中N的确定</span></h1><p>为了确定N的取值，《Language Modeling with Ngrams》使用了 Perplexity 这一指标，该指标越小表示一个语言模型的效果越好。文章使用了华尔街日报的数据库，该数据库的字典大小为19,979，训练集包含 38 million 个词，测试集包含 1.5 million 个词。针对不同的N-gram，计算各自的 Perplexity。<img src="/n-gram/7.JPG" alt>结果显示，Tri-gram的Perplexity最小，因此它的效果是最好的。</p><h1><span id="n-gram中的数据平滑方法">N-gram中的数据平滑方法</span></h1><p>上面提到，N-gram的N越大，模型 Perplexity 越小，表示模型效果越好。这在直观意义上是说得通的，毕竟依赖的词越多，我们获得的信息量越多，对未来的预测就越准确。然而，语言是有极强的创造性的（Creative），当N变大时，更容易出现这样的状况：某些n-gram从未出现过，这就是稀疏问题。</p><p>n-gram最大的问题就是稀疏问题（Sparsity）。例如，在bi-gram中，若词库中有20k个词，那么两两组合其中的很多组合在语料库中都没有出现，根据极大似然估计得到的组合概率将会是0，从而整个句子的概率就会为0。最后的结果是，我们的模型只能计算零星的几个句子的概率，而大部分的句子算得的概率是0，这显然是不合理的。</p><p>因此，我们要进行数据平滑（data Smoothing），数据平滑的目的有两个：一个是使所有的N-gram概率之和为1，使所有的n-gram概率都不为0。它的本质，是重新分配整个概率空间，使已经出现过的n-gram的概率降低，补充给未曾出现过的n-gram。<img src="/n-gram/8.JPG" alt>关于N-gram的训练数据，如果你以为 <strong>“只要是英语就可以了”</strong>，那就大错特错了。文献《Language Modeling with Ngrams》**的作者做了个实验，分别用莎士比亚文学作品，以及华尔街日报作为训练集训练两个N-gram，他认为，两个数据集都是英语，那么用他们生成的文本应该也会有所重合。然而结果是，用两个语料库生成的文本没有任何重合性，即使在语法结构上也没有。  这告诉我们，N-gram的训练是很挑数据集的，你要训练一个问答系统，那就要用问答的语料库来训练，要训练一个金融分析系统，就要用类似于华尔街日报这样的语料库来训练。</p><h1><span id="n-gram的进化版nnlm">N-gram的进化版：NNLM</span></h1><p>NNLM 即 Neural Network based Language Model，由Bengio在2003年提出，它是一个很简单的模型，由四层组成，输入层、嵌入层、隐层和输出层。模型接收的输入是长度为nn的词序列，输出是下一个词的类别。首先，输入是单词序列的index序列，例如单词 I 在字典（大小为 $\mid V \mid$）中的index是10，单词 am 的 index 是23， Bengio 的 index 是65，则句子“I am Bengio”的index序列就是 10, 23, 65。嵌入层（Embedding）是一个大小为 $\mid V \mid \times K$ 的矩阵，从中取出第10、23、65行向量拼成 $3\times K$ 的矩阵就是Embedding层的输出了。隐层接受拼接后的Embedding层输出作为输入，以tanh为激活函数，最后送入带softmax的输出层，输出概率。</p><p>NNLM最大的缺点就是参数多，训练慢。另外，NNLM要求输入是定长n，定长输入这一点本身就很不灵活，同时不能利用完整的历史信息。</p><p><img src="/n-gram/9.JPG" alt></p><h1><span id="nnlm的进化版rnnlm">NNLM的进化版：RNNLM</span></h1><p>针对NNLM存在的问题，Mikolov在2010年提出了RNNLM，其结构实际上是用RNN代替NNLM里的隐层，这样做的好处包括减少模型参数、提高训练速度、接受任意长度输入、利用完整的历史信息。同时，RNN的引入意味着可以使用RNN的其他变体，像LSTM、BLSTM、GRU等等，从而在时间序列建模上进行更多更丰富的优化。</p><p>http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf</p><h1><span id="word2vec">Word2Vec</span></h1><p>https://www.jianshu.com/p/e91f061d6d91</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;见 https://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/， 这里再复习一下。&lt;/p&gt;
&lt;p&gt;朴素贝叶斯的关键组
      
    
    </summary>
    
      <category term="NLP" scheme="http://dinry.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>www2019_recommender system</title>
    <link href="http://dinry.github.io/www2019/"/>
    <id>http://dinry.github.io/www2019/</id>
    <published>2019-09-08T07:00:55.000Z</published>
    <updated>2019-09-16T11:53:46.999Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1cross-domain-recommendation-without-sharing-user-relevant-data">1.Cross-domain Recommendation Without Sharing User-relevant Data</span></h1><p>研究方向：cross-domain recommendation Task</p><p>Goal: combine data from different websites to improve recommendation task</p><p>Challenge:Despite many research efforts on this task, the main drawback is that they largely assume the data of different systems can be fully shared.</p><p>methods: NATR(short for Neural Attentive Transfer Recommendation)To avoid the leak of user privacy during the data sharing process, it consider sharing only the information of the item side, rather than user behavior data. Specifically, we transfer the item embeddings across domains, making it easier for two companies to reach a consensus (e.g., legal policy) on data sharing since the data to be shared is user-irrelevant and has no explicit semantics.</p><p>step:</p><p><img src="/www2019/1.JPG" alt>Our proposed solution, which has three steps, is illustrated in Figure 1.</p><ul><li>In the first step, an embedding-based recommender model, MF for example, is trained on the user-item interaction matrix of the auxiliary domain to obtain item embeddings.</li><li>In the second step, item embeddings of the auxiliary domain are sent to the target domain; note that only the embeddings of overlapped items are necessary to be sent, which are subjected to the data-sharing policy between two companies.</li><li>Finally, the target domain trains a recommender model with the consideration of the transferred item embeddings.</li></ul><h2><span id="contribution">contribution</span></h2><ul><li>We present a new paradigm for cross-domain recommendation without sharing user-relevant data, in which only item-side data can be shared across domains. To allow the transferring of CF signal, we propose to share the item embeddings which are learned from user-item interactions of the auxiliary domain.</li><li>We propose a new solution NATR to resolve the key challenges in leveraging transferred item embeddings. The twolevel attention design allows NATR to distill useful signal from transferred item embeddings, and appropriately combine them with the data of the target domain.</li><li>We conduct extensive experiments on two real-world datasets to demonstrate our proposed method. More ablation studies verify the efficacy of our designed components, and the utility of transferred item embeddings in addressing the data sparsity issue.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/2.JPG" alt></p><h1><span id="2dual-graph-attention-networks-for-deep-latent-representation-of-multifaceted-social-effects-in-recommender-systems">2.Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems</span></h1><h2><span id="abstract">Abstract</span></h2><p><img src="/www2019/4.JPG" alt></p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/5.JPG" alt></p><h2><span id="framework">framework</span></h2><p><img src="/www2019/3.JPG" alt></p><h1><span id="3exploiting-ratings-reviews-and-relationships-for-item-recommendations-in-topic-based-social-networks">3.Exploiting Ratings, Reviews and Relationships for Item Recommendations in Topic Based Social Networks</span></h1><h2><span id="abstract">Abstract</span></h2><p>Many e-commerce platforms today allow users to give their rating scores and reviews on items as well as to establish social relationships with other users. As a result, such platforms accumulate heterogeneous data including numeric scores, short textual reviews, and social relationships. HHowever, many recommender systems only consider historical user feedbacks in modeling user preferences. More specifically, most existing recommendation approaches only use rating scores but ignore reviews and social relationships in the user-generated data. In this paper, we propose TSNPF—a latent factor model to effectively capture user preferences and item features. Employing Poisson factorization, TSNPF fully exploits the wealth of information in rating scores, review text and social relationships altogether. It extracts topics of items and users from the review text and makes use of similarities between user pairs with social relationships, which results in a comprehensive understanding of user preferences. Experimental results on real-world datasets demonstrate that our TSNPF approach is highly effective at recommending items to users.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose a method based on Gamma-Poisson distribution to extract the topic intensities of items and users from usergenerated textual reviews. Compared to previous techniques, our method is able to address the usual problem of data scarcity.</li><li>We propose TSNPF, a conjugate graphical model based on Poisson factorization which only models non-zero observations in ratings, reviews and social relations simultaneously via interpretable user preferences and item attributes. In addition, we propose a closed form mean-field variational inference method to train TSNPF.</li><li>We evaluate the performance of TSNPF using three publicly available real datasets. The results show that TSNPF outperforms state-of-the-art alternatives.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/6.JPG" alt></p><p>主题提取与推荐系统的结合</p><h1><span id="4ghostlink-latent-network-inference-for-influence-aware-recommendationintroduction-写的不错">4.GhostLink: Latent Network Inference for Influence-aware Recommendation.(introduction 写的不错)</span></h1><h2><span id="abstract-probabilistic-graphical-model">Abstract (probabilistic graphical model)</span></h2><p>Social influence plays a vital role in shaping a user’s behavior in online communities dealing with items of fine taste like movies, food, and beer. For online recommendation, this implies that users’ preferences and ratings are influenced due to other individuals. Given only time-stamped reviews of users, can we find out whoinfluences- whom, and characteristics of the underlying influence network? Can we use this network to improve recommendation?</p><p>While prior works in social-aware recommendation have leveraged social interaction by considering the observed social network of users, many communities like Amazon, Beeradvocate, and Ratebeer do not have explicit user-user links.Therefore,we propose GhostLink, an unsupervised probabilistic graphical model, to automatically learn the latent influence network underlying a review community – given only the temporal traces (timestamps) of users’ posts and their content. Based on extensive experiments with four real-world datasets with 13 million reviews, we show that GhostLink improves item recommendation by around 23% over state-of-the-art methods that do not consider this influence. As additional use-cases, we show that GhostLink can be used to differentiate between users’ latent preferences and influenced ones, as well as to detect influential users based on the learned influence graph.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose an unsupervised probabilistic generative model GhostLink based on Latent Dirichlet Allocation to learn a latent influence graph in online communities without requiring explicit user-user links or a social network. This is the first work that solely relies on timestamped review data.</li><li>We propose an efficient algorithm based on Gibbs sampling to estimate the hidden parameters in GhostLink that empirically demonstrates fast convergence.</li><li>We perform large-scale experiments in four communities with 13 million reviews, 0.5 mil. items, and 1 mil. users where we show improved recommendation for item rating prediction by around 23% over state-of-the-art methods. Moreover, we analyze the properties of the influence graph and use it for use-cases like finding influential members in the community.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/7.JPG" alt></p><h1><span id="5graph-neural-networks-for-social-recommendation">5.Graph Neural Networks for Social Recommendation</span></h1><h2><span id="abstract">Abstract</span></h2><p>In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key.</p><p>However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the useruser social graph and the user-item graph).</p><p>To address the three aforementioned challenges simultaneously, in this paper,we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose a novel graph neural network GraphRec, which can model graph data in social recommendations coherently;</li><li>We provide a principled approach to jointly capture interactions and opinions in the user-item graph;</li><li>We introduce a method to consider heterogeneous strengths of social relations mathematically;</li><li>We demonstrate the effectiveness of the proposed framework on various real-world datasets.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/8.JPG" alt></p><h2><span id="performance">performance</span></h2><p><img src="/www2019/9.JPG" alt></p><h1><span id="6hierarchical-temporal-convolutional-networks-for-dynamic-recommender-systems工程应用">6.Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems(工程应用)</span></h1><h2><span id="abstract">Abstract</span></h2><p>Recommender systems that can learn from cross-session data to dynamically predict the next item a user will choose are crucial for online platforms. However, existing approaches often use out-ofthe-box sequence models which are limited by speed and memory consumption, are often infeasible for production environments, and usually do not incorporate cross-session information, which is crucial for effective recommendations.</p><p>Here we propose Hierarchical Temporal Convolutional Networks (HierTCN), a hierarchical deep learning architecture that makes dynamic recommendations based on users’ sequential multi-session interactions with items. HierTCN is designed for web-scale systems with billions of items and hundreds of millions of users. It consists of two levels of models: The high-level model uses Recurrent Neural Networks (RNN) to aggregate users’ evolving long-term interests across different sessions, while the low-level model is implemented with Temporal Convolutional Networks (TCN), utilizing both the long-term interests and the short-term interactions within sessions to predic  the next interaction.</p><h2><span id="contribution">contribution</span></h2><ul><li>HierTCN has a significant performance improvement over existing deep learning models by about 30% on a public XING dataset and 18% on a private large-scale Pinterest dataset.</li><li>Compared with RNN-based approaches, HierTCN is 2.5 times faster in terms of training time and allows for much easier gradient backpropagation.</li><li>Compared with CNN-based approaches, HierTCN requires roughly 10% data memory usage and allows for easy latent feature extraction.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/10.JPG" alt></p><h1><span id="7how-intention-informed-recommendations-modulate-choices-a-field-study-of-spokenword-content">7.How Intention Informed Recommendations Modulate Choices: A Field Study of SpokenWord Content</span></h1><h2><span id="abstract">Abstract</span></h2><p>People’s content choices are ideally driven by their intentions, aspirations, and plans.</p><p>However, in reality, choices may be modulated by recommendation systems which are typically trained to promote popular items and to reinforce users’ historical behavior. As a result, the utility and user experience of content consumption can be affected implicitly and undesirably.</p><p>To study this problem, we conducteda 2 × 2 randomized controlled field experiment (105 urbancollege students) to compare the effects of intention informed recommendationswith classical intention agnostic systems. The studywas conducted in the context of spokenwordweb content (podcasts)which is often consumed through subscription sites or apps. Wemodified a commercial podcast app to include (1) a recommenderthat takes into account users’ stated intentions at onboarding, and(2) a Collaborative Filtering (CF) recommender during daily use.Our study suggests that: (1) intention-aware recommendations cansignificantly raise users’ interactions (subscriptions and listening)with channels and episodes related to intended topics by over 24%,even if such a recommender is only used during onboarding, and (2)the CF-based recommender doubles users’ explorations on episodesfrom not-subscribed channels and improves satisfaction for usersonboarded with the intention-aware recommender.</p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/11.JPG" alt></p><h2><span id="framework">framework</span></h2><p><img src="/www2019/12.JPG" alt></p><h1><span id="8how-serendipity-improves-user-satisfaction-with-recommendations-a-large-scale-user-evaluation">8.How Serendipity Improves User Satisfaction with Recommendations? A Large-Scale User Evaluation</span></h1><h2><span id="abstract">Abstract</span></h2><p>Recommendation serendipity is being increasingly recognized asbeing equally important as the other beyond-accuracy objectives(such as novelty and diversity), in eliminating the “filter bubble”phenomenon of the traditional recommender systems.</p><p>However,little work has empirically verified the effects of serendipity onincreasing user satisfaction and behavioral intention.</p><p>In this paper,we report the results of a large-scale user survey (involving over3,000 users) conducted in an industrial mobile e-commerce setting.The study has identified the significant causal relationships fromnovelty, unexpectedness, relevance, and timeliness to serendipity,and from serendipity to user satisfaction and purchase intention.Moreover, our findings reveal that user curiosity plays a moderatingrole in strengthening the relationships from novelty to serendipityand from serendipity to satisfaction. Our third contribution lies inthe comparison of several recommender algorithms, which demonstratesthe significant improvements of the serendipity-orientedalgorithm over the relevance- and novelty-oriented approaches interms of user perceptions. We finally discuss the implications ofthis experiment, which include the feasibility of developing a moreprecise metric for measuring recommendation serendipity, andthe potential benefit of a curiosity-based personalized serendipitystrategy for recommender systems.<img src="/www2019/13.JPG" alt></p><h1><span id="9improving-outfit-recommendation-with-co-supervision-of-fashion-generation-图像衣服类">9.Improving Outfit Recommendation with Co-supervision of Fashion Generation (图像：衣服类)</span></h1><h2><span id="abstract">Abstract</span></h2><p>The task of fashion recommendation includes twomain challenges:visual understanding and visual matching. Visual understandingaims to extract effective visual features. Visual matching aims tomodel a human notion of compatibility to compute a match betweenfashion items. Most previous studies rely on recommendationloss alone to guide visual understanding and matching. Althoughthe features captured by thesemethods describe basic characteristics(e.g., color, texture, shape) of the input items, they arenot directly related to the visual signals of the output items (to berecommended). This is problematic because the aesthetic characteristics(e.g., style, design), based on which we can directly inferthe output items, are lacking. Features are learned under the recommendationloss alone, where the supervision signal is simplywhether the given two items are matched or not.</p><p>To address this problem, we propose a neural co-supervisionlearning framework, called the FAshion RecommendationMachine(FARM). FARM improves visual understanding by incorporatingthe supervision of generation loss, which we hypothesize to beable to better encode aesthetic information. FARMenhances visualmatching by introducing a novel layer-to-layer matching mechanismto fuse aesthetic information more effectively, and meanwhileavoiding paying too much attention to the generation qualityand ignoring the recommendation performance.</p><p>Extensive experiments on two publicly available datasets showthat FARM outperforms state-of-the-art models on outfit recommendation,in terms of AUC and MRR. Detailed analyses of generatedand recommended items demonstrate that FARM can encodebetter features and generate high quality images as references toimprove recommendation performance.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose a neural co-supervision learning framework, FARM, for outfit recommendation that simultaneously yields recommendation and generation.</li><li>We propose a layer-to-layer matching mechanism that acts as a bridge between generation and recommendation, and improves recommendation by leveraging generation features.</li><li>Our proposed approach is shown to be effective in experiments on two large-scale datasets.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/14.JPG" alt><img src="/www2019/15.JPG" alt></p><h1><span id="10jointly-learning-explainable-rules-for-recommendation-with-knowledge-graph">10.Jointly Learning Explainable Rules for Recommendation with Knowledge Graph</span></h1><h2><span id="abstract">Abstract</span></h2><p>Explainability and effectiveness are two key aspects for building recommendersystems. Prior efforts mostly focus on incorporating sideinformation to achieve better recommendation performance.</p><p>However,these methods have some weaknesses: (1) prediction of neuralnetwork-based embedding methods are hard to explain and debug;(2) symbolic, graph-based approaches (e.g., meta path-based models)require manual efforts and domain knowledge to define patternsand rules, and ignore the item association types (e.g. substitutableand complementary).</p><p>In this paper, we propose a novel joint learningframework to integrate induction of explainable rules from knowledgegraph with construction of a rule-guided neural recommendationmodel. The framework encourages two modules to complementeach other in generating effective and explainable recommendation:</p><ol><li>inductive rules, mined from item-centric knowledge graphs,summarize common multi-hop relational patterns for inferring differentitem associations and provide human-readable explanationfor model prediction; 2) recommendation module can be augmentedby induced rules and thus have better generalization ability dealingwith the cold-start issue.</li></ol><p>Extensive experiments1 show that ourproposed method has achieved significant improvements in itemrecommendation over baselines on real-world datasets. Our modeldemonstrates robust performance over “noisy&quot; item knowledgegraphs, generated by linking item names to related entities.</p><h2><span id="contribution">contribution</span></h2><ul><li>We utilize a large-scale knowledge graph to derive rules between items from item associations.</li><li>We propose a joint optimization framework that induces rules from knowledge graphs and recommends items based on the rules at the same time.</li><li>We conduct extensive experiments on real-world datasets. Experimental results prove the effectiveness of our framework in accurate and explainable recommendation.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/16.JPG" alt></p><h1><span id="11jointly-leveraging-intent-and-interaction-signals-to-predict-user-satisfaction-with-slate-recommendations">11.Jointly Leveraging Intent and Interaction Signals to Predict User Satisfaction with Slate Recommendations.</span></h1><h2><span id="abstract">Abstract</span></h2><p>Detecting and understanding implicit measures of user satisfactionare essential for enhancing recommendation quality. When usersinteract with a recommendation system, they leave behind finegrained traces of interaction signals, which contain valuable informationthat could help gauging user satisfaction. User interactionwith such systems is often motivated by a specific need or intent, oftennot explicitly specified by the user, but can nevertheless informon how the user interacts with, and the extent to which the user issatisfied by the recommendations served. In this work, we considera complex recommendation scenario, called Slate Recommendation,wherein a user is presented with an ordered set of collections, calledslates, in a specific page layout. We focus on the context of musicstreaming and leverage fine-grained user interaction signals totackle the problem of predicting user satisfaction.</p><p>We hypothesize that user interactions are conditional on thespecific intent users have when interacting with a recommendationsystem, and highlight the need for explicitly considering userintent when interpreting interaction signals. We present diverseapproaches to identify user intents (interviews, surveys and a quantitativeapproach) and identify a set of common intents users have ina music streaming recommendation setting. Additionally, we identifythe importance of shared learning across intents and propose amulti-level hierarchical model for user satisfaction prediction thatleverages user intent information alongside interaction signals. Ourfindings from extensive experiments on a large scale real world datademonstrate (i) the utility of considering different interaction signals,(ii) the role of intents in interpreting user interactions and (iii)the interplay between interaction signals and intents in predictinguser satisfaction.</p><h1><span id="12modeling-heart-rate-and-activity-data-for-personalized-fitness-recommendation健康推荐">12.Modeling Heart Rate and Activity Data for Personalized Fitness Recommendation(健康推荐)</span></h1><h2><span id="abstract">Abstract</span></h2><p>Activity logs collected from wearable devices (e.g. Apple Watch,Fitbit, etc.) are a promising source of data to facilitate a wide rangeof applications such as personalized exercise scheduling, workoutrecommendation, and heart rate anomaly detection.</p><p>However,such data are heterogeneous, noisy, diverse in scale and resolution,and have complex interdependencies, making them challenging tomodel.</p><p>In this paper, we develop context-aware sequential modelsto capture the personalized and temporal patterns of fitness data.</p><p>Specifically, we propose FitRec – an LSTM-based model that capturestwo levels of context information: context within a specificactivity, and context across a user’s activity history.</p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/17.JPG" alt></p><h2><span id="framework">framework</span></h2><p><img src="/www2019/18.JPG" alt></p><h1><span id="13modeling-item-specific-temporal-dynamics-of-repeat-consumption-for-recommender-systems">13.Modeling Item-Specific Temporal Dynamics of Repeat Consumption for Recommender Systems</span></h1><h2><span id="abstract">Abstract</span></h2><p>Repeat consumption is a common scenario in daily life, such asrepurchasing items and revisiting websites, and is a critical factorto be taken into consideration for recommender systems. Temporaldynamics play important roles in modeling repeat consumption.It is noteworthy that for items with distinct lifetimes, consumingtendency for the next one fluctuates differently with time. Forexample, users may repurchase milk weekly, but it is possible torepurchase mobile phone after a long period of time. Therefore,how to adaptively incorporate various temporal patterns of repeatconsumption into a holistic recommendation model has been a newand important problem.</p><p>In this paper, we propose a novel unified model with introducingHawkes Process into Collaborative Filtering (CF). Differentfrom most previous work which ignores various time-varying patternsof repeat consumption, the model explicitly addresses twoitem-specific temporal dynamics: (1) short-term effect and (2) lifetimeeffect, which is named as Short-Term and Life-Time RepeatConsumption (SLRC) model. SLRC learns importance of the twofactors for each item dynamically by interpretable parameters.</p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/19.JPG" alt></p><h1><span id="14multi-task-feature-learning-for-knowledge-graph-enhanced-recommendationcross-domain-recommendation">14.Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation(cross-domain recommendation)</span></h1><h2><span id="abstract">Abstract</span></h2><p>Collaborative filtering often suffers from sparsity and cold startproblems in real recommendation scenarios, therefore, researchersand engineers usually use side information to address the issuesand improve the performance of recommender systems.</p><p>In thispaper, we consider knowledge graphs as the source of side information.</p><p>We propose MKR, a Multi-task feature learning approachfor Knowledge graph enhanced Recommendation. MKR is a deepend-to-end framework that utilizes knowledge graph embeddingtask to assist recommendation task.</p><h2><span id="contribution">contribution</span></h2><p>We propose MKR, a Multi-task feature learning approachfor Knowledge graph enhanced Recommendation. MKR is a deepend-to-end framework that utilizes knowledge graph embeddingtask to assist recommendation task. knowledge graph embedding, as shown in theoretical analysis and experiment results.</p><h2><span id="framework">framework</span></h2><p><img src="/www2019/20.JPG" alt></p><h1><span id="15multimodal-review-generation-for-recommender-systems">15.Multimodal Review Generation for Recommender Systems</span></h1><h2><span id="abstract">Abstract</span></h2><p>Key to recommender systems is learning user preferences, whichare expressed through various modalities. In online reviews, forinstance, this manifests in numerical rating, textual content, as wellas visual images. In this work, we hypothesize that modelling thesemodalities jointly would result in a more holistic representation ofa review towards more accurate recommendations. Therefore, wepropose Multimodal Review Generation (MRG), a neural approachthat simultaneously models a rating prediction component and areview text generation component. We hypothesize that the shareduser and item representations would augment the rating predictionwith richer information from review text, while sensitizingthe generated review text to sentiment features based on user anditem of interest. Moreover, when review photos are available, visualfeatures could inform the review text generation further. Comprehensiveexperiments on real-life datasets from several major UScities show that the proposed model outperforms comparable multimodalbaselines, while an ablation analysis establishes the relativecontributions of the respective components of the joint model.</p><h2><span id="contribution">contribution</span></h2><p>We design the MRG model (see Section 3), whichjointly models rating prediction and text generation at the reviewlevel by incorporating LSTM cells with a novel fusion gate as akind of soft attention to weigh the relative contributions of sentimentfeatures and visual features that provide context to the textgeneration. We also describe the learning and inference algorithmsrespectively.</p><h2><span id="framework">framework</span></h2><p><img src="/www2019/21.JPG" alt></p><h1><span id="16personalized-bundle-list-recommendation">16.Personalized Bundle List Recommendation</span></h1><h2><span id="abstract">Abstract</span></h2><p>Product bundling, offering a combination of items to customers,is one of the marketing strategies commonly used in online ecommerceand offline retailers. A high-quality bundle generalizesfrequent items of interest, and diversity across bundles boosts theuser-experience and eventually increases transaction volume.</p><p>Inthis paper, we formalize the personalized bundle list recommendationas a structured prediction problem and propose a bundlegeneration network (BGN), which decomposes the problem intoquality/diversity parts by the determinantal point processes (DPPs).BGN uses a typical encoder-decoder framework with a proposedfeature-aware softmax to alleviate the inadequate representationof traditional softmax, and integrates the masked beam search andDPP selection to produce high-quality and diversified bundle listwith an appropriate bundle size.<img src="/www2019/22.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;1cross-domain-recommendation-without-sharing-user-relevant-data&quot;&gt;1.Cross-domain Recommendation Without Sharing User-relevant D
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
  </entry>
  
  <entry>
    <title>tensorflow学习率衰减</title>
    <link href="http://dinry.github.io/tensorflow%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/"/>
    <id>http://dinry.github.io/tensorflow学习率衰减/</id>
    <published>2019-08-16T03:01:00.000Z</published>
    <updated>2019-08-16T06:31:43.560Z</updated>
    
    <content type="html"><![CDATA[<p>在神经网络的训练过程中，学习率(learning rate)控制着参数的更新速度，tf.train类下面的五种不同的学习速率的衰减方法。</p><ul><li>tf.train.exponential_decay</li><li>tf.train.inverse_time_decay</li><li>tf.train.natural_exp_decay</li><li>tf.train.piecewise_constant</li><li>tf.train.polynomial_decay</li></ul><ol><li>首先使用较大学习率(目的：为快速得到一个比较优的解);</li><li>然后通过迭代逐步减小学习率(目的：为使模型在训练后期更加稳定);</li></ol><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.train.exponential_decay(</span><br><span class="line">    learning_rate,初始学习率</span><br><span class="line">    global_step,当前迭代次数</span><br><span class="line">    decay_steps,衰减速度（在迭代到该次数时学习率衰减为earning_rate * decay_rate）</span><br><span class="line">    decay_rate,学习率衰减系数，通常介于<span class="number">0</span><span class="number">-1</span>之间。</span><br><span class="line">    staircase=False,(默认值为False,当为True时，（global_step/decay_steps）则被转化为整数) ,选择不同的衰减方式。</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在神经网络的训练过程中，学习率(learning rate)控制着参数的更新速度，tf.train类下面的五种不同的学习速率的衰减方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf.train.exponential_decay&lt;/li&gt;
&lt;li&gt;tf.train.inverse_ti
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day24</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/"/>
    <id>http://dinry.github.io/西瓜书day24/</id>
    <published>2019-08-03T14:10:12.000Z</published>
    <updated>2019-08-03T14:27:51.510Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="主成分分析pca">主成分分析（PCA）</span></h1><p>PCA是一种最常用的降维方法</p><ul><li>最近重构性：样本点到这个超平面的距离都足够近</li><li>最大可分性：样本点在这个超平面上的投影能尽可能分开</li></ul><h4><span id="最近重构性">最近重构性</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/1.JPG" alt></p><h4><span id="最大可分性">最大可分性</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/2.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/3.JPG" alt></p><p>应用拉格朗日乘子法</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/4.JPG" alt></p><h4><span id="结果">结果</span></h4><p>降维导致 d-d'个特征值的特征向量被舍弃了，舍弃这部分信息能使样本的采样密度增大，另外，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将他们舍弃能在一定程度上起到去噪的作用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;主成分分析pca&quot;&gt;主成分分析（PCA）&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;PCA是一种最常用的降维方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近重构性：样本点到这个超平面的距离都足够近&lt;/li&gt;
&lt;li&gt;最大可分性：样本点在这个超平面上的投影能尽可能分开&lt;/li&gt;
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day21</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day21/"/>
    <id>http://dinry.github.io/西瓜书day21/</id>
    <published>2019-08-03T14:09:55.000Z</published>
    <updated>2019-08-03T14:09:55.073Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tf.multinomial</title>
    <link href="http://dinry.github.io/tf-multinomial/"/>
    <id>http://dinry.github.io/tf-multinomial/</id>
    <published>2019-08-03T13:31:58.000Z</published>
    <updated>2019-08-03T13:56:44.689Z</updated>
    
    <content type="html"><![CDATA[<p>明明按概率，亲测却非常随机</p><p>tf.multinomial(logits, num_samples, seed=None, name=None)</p><p>从multinomial分布中采样，样本个数是num_samples，每个样本被采样的概率由logits给出</p><h4><span id="parametrs">parametrs:</span></h4><ul><li><p>logits: 2-D Tensor with shape [batch_size, num_classes]. Each slice [i, :] represents the unnormalized log probabilities for all classes.2维量，shape是 [batch_size, num_classes]，每一行都是关于种类的未归一化的对数概率</p></li><li><p>num_samples: 0-D. Number of independent samples to draw for each row slice.标量，表示采样的个数，更重要的是，它限制了返回张量中元素的范围{：0，1，2，…，num_samples-1 }</p></li></ul><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">samples = tf.multinomial(tf.log([[<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>]]), <span class="number">5</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(samples)</span><br><span class="line"></span><br><span class="line"># 运行结果：array([[2, 1, 2, 2, 0]])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;明明按概率，亲测却非常随机&lt;/p&gt;
&lt;p&gt;tf.multinomial(logits, num_samples, seed=None, name=None)&lt;/p&gt;
&lt;p&gt;从multinomial分布中采样，样本个数是num_samples，每个样本被采样的概率由logit
      
    
    </summary>
    
      <category term="deep learning" scheme="http://dinry.github.io/categories/deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow实现梯度下降各种方法</title>
    <link href="http://dinry.github.io/tensorflow%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%90%84%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/tensorflow实现梯度下降各种方法/</id>
    <published>2019-08-03T03:14:07.000Z</published>
    <updated>2019-08-03T04:54:33.110Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="不使用tensorflow任何梯度下降方法">不使用tensorflow任何梯度下降方法</span></h1><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf8 -*-</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"># Import MNIST data</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line">b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line">new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h1><span id="使用tfgradients实现梯度下降">使用tf.gradients实现梯度下降</span></h1><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 使用随机梯度下降</span><br><span class="line">vars=tf.trainable_variables()</span><br><span class="line">vars_grad=tf.gradients(loss_op,vars)</span><br><span class="line">vars_new=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(vars)):</span><br><span class="line">    vars_new.append(vars[i].assign(vars[i]-learning_rate*vars_grad[i])) # 权重更新</span><br><span class="line">sess.run(vars_new, feed_dict=&#123;<span class="attr">X</span>: batch_x, <span class="attr">Y</span>: batch_y, <span class="attr">keep_prob</span>: <span class="number">0.8</span>&#125;)</span><br></pre></td></tr></table></figure></p><p>minist:<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line">W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line"></span><br><span class="line">new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line">new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h1><span id="使用tensorflow内置优化器">使用tensorflow内置优化器</span></h1><h4><span id="minimize">minimize</span></h4><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line"># W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line">#</span><br><span class="line"># new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line"># new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line">train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            # _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h4><span id="compute_gradients与apply_gradients">compute_gradients与apply_gradients</span></h4><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'D'</span>):</span><br><span class="line">    # Set model weights</span><br><span class="line">    W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">    # Construct model</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line"># W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line">#</span><br><span class="line"># new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line"># new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"># train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"># optimizer=tf.train.AdamOptimizer()</span><br><span class="line"># gradients=optimizer.compute_gradients(cost)</span><br><span class="line"># clipped_gradients = [(tf.clip_by_value(_[0], -1, 1), _[1]) for _ in gradients] # _[0] 对应dw ,_[1]对应db</span><br><span class="line"># train_op = optimizer.apply_gradients(clipped_gradients)</span><br><span class="line"># 或</span><br><span class="line"># train_op = optimizer.apply_gradients(gradients)</span><br><span class="line"></span><br><span class="line">tvars = tf.trainable_variables()</span><br><span class="line">d_params = [v <span class="keyword">for</span> v <span class="keyword">in</span> tvars <span class="keyword">if</span> v.name.startswith(<span class="string">'D/'</span>)]</span><br><span class="line">trainerD = tf.train.AdamOptimizer(learning_rate=<span class="number">0.0002</span>, beta1=<span class="number">0.5</span>)</span><br><span class="line">d_grads = trainerD.compute_gradients(cost, d_params)#Only update the weights for the discriminator network.</span><br><span class="line">train_op = trainerD.apply_gradients(d_grads)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            # _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;不使用tensorflow任何梯度下降方法&quot;&gt;不使用tensorflow任何梯度下降方法&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;s
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow中的参数初始化方法</title>
    <link href="http://dinry.github.io/tensorflow%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/tensorflow中的参数初始化方法/</id>
    <published>2019-08-03T02:10:57.000Z</published>
    <updated>2019-08-03T03:08:41.345Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="常量初始化">常量初始化</span></h1><p>tf中使用tf.constant_initializer(value)类生成一个初始值为常量value的tensor对象。constant_initializer类的构造函数定义：<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, value=<span class="number">0</span>, dtype=dtypes.float32, verify_shape=False):</span><br><span class="line">    self.value = value</span><br><span class="line">    self.dtype = dtypes.as_dtype(dtype)</span><br><span class="line">    self._verify_shape = verify_shape</span><br></pre></td></tr></table></figure></p><ul><li>value：指定的常量</li><li>dtype： 数据类型</li><li>verify_shape： 是否可以调整tensor的形状，默认可以调整</li></ul><p>example:<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">value = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">init = tf.constant_initializer(value)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">8</span>], initializer=init)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">#output:</span><br><span class="line">#[ 0.  1.  2.  3.  4.  5.  6.  7.]</span><br></pre></td></tr></table></figure></p><p>神经网络中经常使用常量初始化方法来初始化偏置值</p><p>当初始化一个维数很多的常量时，一个一个指定每个维数上的值很不方便，tf提供了 tf.zeros_initializer() 和 tf.ones_initializer() 类，分别用来初始化全0和全1的tensor对象。</p><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_zeros=tf.zeros_initializer()</span><br><span class="line">init_ones = tf.ones_initializer</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">8</span>], initializer=init_zeros)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">8</span>], initializer=init_ones)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#output:</span><br><span class="line"># [ 0.  0.  0.  0.  0.  0.  0.  0.]</span><br><span class="line"># [ 1.  1.  1.  1.  1.  1.  1.  1.]</span><br></pre></td></tr></table></figure></p><h1><span id="初始化为正态分布">初始化为正态分布</span></h1><p>初始化参数为正太分布在神经网络中应用的最多，可以初始化为标准正太分布和截断正太分布。</p><p>tf中使用 tf.random_normal_initializer() 类来生成一组符合标准正太分布的tensor。</p><p>tf中使用 tf.truncated_normal_initializer() 类来生成一组符合截断正太分布的tensor。</p><p>tf.random_normal_initializer 类和 tf.truncated_normal_initializer 的构造函数定义：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=None, dtype=dtypes.float32):</span><br><span class="line">    self.mean = mean</span><br><span class="line">    self.stddev = stddev</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = _assert_float_dtype(dtypes.as_dtype(dtype))</span><br></pre></td></tr></table></figure></p><ul><li>mean： 正太分布的均值，默认值0</li><li>stddev： 正太分布的标准差，默认值1</li><li>seed： 随机数种子，指定seed的值可以每次都生成同样的数据</li><li>dtype： 数据类型</li></ul><p>example:<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_random = tf.random_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line">init_truncated = tf.truncated_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_random)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_truncated)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line"></span><br><span class="line">#output:</span><br><span class="line"># [-0.40236568 -0.35864913 -0.94253045 -0.40153521  0.1552504   1.16989613</span><br><span class="line">#   0.43091929 -0.31410623  0.70080078 -0.9620409 ]</span><br><span class="line"># [ 0.18356581 -0.06860946 -0.55245203  1.08850253 -1.13627422 -0.1006074</span><br><span class="line">#   0.65564936  0.03948414  0.86558545 -0.4964745 ]</span><br></pre></td></tr></table></figure></p><h1><span id="初始化为均匀分布">初始化为均匀分布</span></h1><p>tf中使用 tf.random_uniform_initializer 类来生成一组符合均匀分布的tensor。</p><p>tf.random_uniform_initializer类构造函数定义：<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, minval=<span class="number">0</span>, maxval=None, seed=None, dtype=dtypes.float32):</span><br><span class="line">    self.minval = minval</span><br><span class="line">    self.maxval = maxval</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = dtypes.as_dtype(dtype)</span><br></pre></td></tr></table></figure></p><ul><li>minval: 最小值</li><li>maxval： 最大值</li><li>seed：随机数种子</li><li>dtype： 数据类型</li></ul><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_uniform = tf.random_uniform_initializer(minval=<span class="number">0</span>, maxval=<span class="number">10</span>, seed=None, dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_uniform)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [ 6.93343639  9.41196823  5.54009819  1.38017178  1.78720832  5.38881063</span><br><span class="line">#   3.39674473  8.12443542  0.62157512  8.36026382]</span><br></pre></td></tr></table></figure></p><p>从输出可以看到，均匀分布生成的随机数并不是从小到大或者从大到小均匀分布的，这里均匀分布的意义是每次从一组服从均匀分布的数里边随机抽取一个数。</p><p>tf中另一个生成均匀分布的类是 tf.uniform_unit_scaling_initializer()，构造函数是：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, factor=<span class="number">1.0</span>, seed=None, dtype=dtypes.float32):</span><br><span class="line">    self.factor = factor</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = _assert_float_dtype(dtypes.as_dtype(dtype))</span><br></pre></td></tr></table></figure></p><p>同样都是生成均匀分布，tf.uniform_unit_scaling_initializer 跟 tf.random_uniform_initializer 不同的地方是前者不需要指定最大最小值，是通过公式计算出来的：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max_val = math.sqrt(<span class="number">3</span> / input_size) * factor</span><br><span class="line">min_val = -max_val</span><br></pre></td></tr></table></figure></p><p>input_size是生成数据的维度，factor是系数。<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_uniform_unit = tf.uniform_unit_scaling_initializer(factor=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_uniform_unit)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [-1.65964031  0.59797513 -0.97036457 -0.68957627  1.69274557  1.2614969</span><br><span class="line">#   1.55491126  0.12639415  0.54466736 -1.56159735]</span><br></pre></td></tr></table></figure></p><h1><span id="初始化为变尺度正太-均匀分布">初始化为变尺度正太、均匀分布</span></h1><p>tf中tf.variance_scaling_initializer()类可以生成截断正太分布和均匀分布的tensor，增加了更多的控制参数。构造函数：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, scale=<span class="number">1.0</span>,</span><br><span class="line">               mode=<span class="string">"fan_in"</span>,</span><br><span class="line">               distribution=<span class="string">"normal"</span>,</span><br><span class="line">               seed=None,</span><br><span class="line">               dtype=dtypes.float32):</span><br><span class="line">    <span class="keyword">if</span> scale &lt;= <span class="number">0.</span>:</span><br><span class="line">      raise ValueError(<span class="string">"`scale` must be positive float."</span>)</span><br><span class="line">    <span class="keyword">if</span> mode not <span class="keyword">in</span> &#123;<span class="string">"fan_in"</span>, <span class="string">"fan_out"</span>, <span class="string">"fan_avg"</span>&#125;:</span><br><span class="line">      raise ValueError(<span class="string">"Invalid `mode` argument:"</span>, mode)</span><br><span class="line">    distribution = distribution.lower()</span><br><span class="line">    <span class="keyword">if</span> distribution not <span class="keyword">in</span> &#123;<span class="string">"normal"</span>, <span class="string">"uniform"</span>&#125;:</span><br><span class="line">      raise ValueError(<span class="string">"Invalid `distribution` argument:"</span>, distribution)</span><br><span class="line">    self.scale = scale</span><br><span class="line">    self.mode = mode</span><br><span class="line">    self.distribution = distribution</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = _assert_float_dtype(dtypes.as_dtype(dtype))</span><br></pre></td></tr></table></figure></p><ul><li>scale: 缩放尺度</li><li>mode： 有3个值可选，分别是 “fan_in”, “fan_out” 和 “fan_avg”，用于控制计算标准差 stddev的值</li><li>distribution： 2个值可选，”normal”或“uniform”，定义生成的tensor的分布是截断正太分布还是均匀分布</li></ul><p>distribution选‘normal’的时候，生成的是截断正太分布，标准差 stddev = sqrt(scale / n), n的取值根据mode的不同设置而不同：</p><ul><li>mode = &quot;fan_in&quot;， n为输入单元的结点数；</li><li>mode = &quot;fan_out&quot;，n为输出单元的结点数；</li><li>mode = &quot;fan_avg&quot;,n为输入和输出单元结点数的平均值;</li></ul><p>distribution选 ‘uniform’，生成均匀分布的随机数tensor，最大值 max_value和 最小值 min_value 的计算公式：</p><p>max_value = sqrt(3 * scale / n)</p><p>min_value = -max_value</p><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_variance_scaling_normal = tf.variance_scaling_initializer(scale=<span class="number">1.0</span>,mode=<span class="string">"fan_in"</span>,</span><br><span class="line">                                                        distribution=<span class="string">"normal"</span>,seed=None,dtype=tf.float32)</span><br><span class="line">init_variance_scaling_uniform = tf.variance_scaling_initializer(scale=<span class="number">1.0</span>,mode=<span class="string">"fan_in"</span>,</span><br><span class="line">                                                        distribution=<span class="string">"uniform"</span>,seed=None,dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_variance_scaling_normal)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_variance_scaling_uniform)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [ 0.55602223  0.36556259  0.39404872 -0.11241052  0.42891756 -0.22287074</span><br><span class="line">#   0.15629818  0.56271428 -0.15364751 -0.03651841]</span><br><span class="line"># [ 0.22965753 -0.1339919  -0.21013224  0.112804   -0.49030468  0.21375734</span><br><span class="line">#   0.24524075 -0.48397955  0.02254289 -0.07996771]</span><br></pre></td></tr></table></figure></p><h1><span id="其他初始化方式">其他初始化方式</span></h1><ul><li>tf.orthogonal_initializer() 初始化为正交矩阵的随机数，形状最少需要是二维的</li><li>tf.glorot_uniform_initializer() 初始化为与输入输出节点数相关的均匀分布随机数</li><li>tf.glorot_normal_initializer（） 初始化为与输入输出节点数相关的截断正太分布随机数</li></ul><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_orthogonal = tf.orthogonal_initializer(gain=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line">init_glorot_uniform = tf.glorot_uniform_initializer()</span><br><span class="line">init_glorot_normal = tf.glorot_normal_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">4</span>,<span class="number">4</span>], initializer=init_orthogonal)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_glorot_uniform)</span><br><span class="line">  z = tf.get_variable(<span class="string">'z'</span>, shape=[<span class="number">10</span>], initializer=init_glorot_normal)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  z.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line">  print(z.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [[ 0.41819954  0.38149482  0.82090431  0.07541249]</span><br><span class="line">#  [ 0.41401231  0.21400851 -0.38360971  0.79726893]</span><br><span class="line">#  [ 0.73776144 -0.62585676 -0.06246936 -0.24517137]</span><br><span class="line">#  [ 0.33077344  0.64572859 -0.41839844 -0.54641217]]</span><br><span class="line"># [-0.11182356  0.01995623 -0.0083192  -0.09200105  0.49967837  0.17083591</span><br><span class="line">#   0.37086374  0.09727859  0.51015782 -0.43838671]</span><br><span class="line"># [-0.50223351  0.18181904  0.43594137  0.3390047   0.61405027  0.02597036</span><br><span class="line">#   0.31719241  0.04096413  0.10962497 -0.13165198]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;常量初始化&quot;&gt;常量初始化&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;tf中使用tf.constant_initializer(value)类生成一个初始值为常量value的tensor对象。
constant_initializer类的构造函数定义：
&lt;figur
      
    
    </summary>
    
      <category term="deep learning" scheme="http://dinry.github.io/categories/deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>神经网络训练问题排查</title>
    <link href="http://dinry.github.io/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    <id>http://dinry.github.io/神经网络训练问题排查/</id>
    <published>2019-08-01T09:01:14.000Z</published>
    <updated>2019-08-01T11:06:18.611Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="数据标准化">数据标准化</span></h1><p>数据的分布情况如何？数据是否经过适当的缩放？总体上的规则是：</p><ul><li>如果数据是连续值：范围应当在-1到1、0到1，或者呈平均值为0、标准差为1的正态分布。实际的范围不用如此精确，但确保输入数据大致处于上述区间内会有助于训练。缩小过大的输入，放大过小的输入。</li><li>如果数据是离散的类别（以及对于分类问题的输出而言），则通常使用one-hot表示。也就是说，如果有三种类别，那么这三种不同类别的数据将分别以[1,0,0]、[0,1,0]、[0,0,1]的方式表示。</li></ul><p>请注意：训练数据和测试数据的标准化方法必须完全相同，这非常重要。</p><h1><span id="权重初始化">权重初始化</span></h1><p>您需要确保权重不会过大，也不会过小。Xavier权重初始化方法通常是比较好的选择。对于使用修正线性（relu）或带泄露的修正线性（leaky relu）激活函数的网络而言，RELU权重初始化方法比较合适。</p><h1><span id="epoch数量和迭代次数">Epoch数量和迭代次数</span></h1><p>一个epoch周期的定义是完整地遍历数据集一次。DL4J将迭代次数定义为每个微批次中的参数更新次数。</p><p>在训练中，一般应让训练持续多个epoch，而将迭代次数设为一次（.iterations(1)选项）；一般仅在对非常小的数据集进行完整批次的训练时才会采用大于1的迭代次数。</p><p>如果epoch数量太少，网络就没有足够的时间学会合适的参数；epoch数量太多则有可能导致网络对训练数据过拟合。选择epoch数量的方式之一是早停法。早停法还可以避免神经网络发生过拟合（即可以帮助网络更好地适应未曾见过的数据）。</p><h1><span id="学习速率">学习速率</span></h1><p>学习速率是最重要的超参数之一。如果学习速率过高或过低，网络可能学习效果非常差、学习速度非常慢，甚至完全没有进展。学习速率的取值范围一般在0.1到1e-6之间，最理想的速率通常取决于具体的数据（以及网络架构）。一种简单的建议是，一开始可以尝试三种不同的学习速率：1e-1、1e-3、1e-6，先大致了解一下应该设为怎样的值，然后再进一步微调。理想状态下，可以同时以不同的学习速率运行模型，以便节省时间。</p><p>选择合适的学习速率的常用方法是借助DL4J的可视化界面来将训练进程可视化。您需要关注损失随时间变化的情况以及更新值与参数的绝对值之比（通常可以先考虑1:1000左右的比例）。</p><h1><span id="策略与学习速率计划">策略与学习速率计划</span></h1><p>您可以选择为神经网络设定学习速率策略，让学习速率随着时间推移逐渐“放缓”，帮助网络收敛至更接近局部极小值的位置，进而取得更好的学习效果。一种常用的策略是学习速率计划（learning rate schedule）。</p><h1><span id="损失函数">损失函数</span></h1><p>神经网络不同层中的损失函数的作用包括预训练、学习改善权重，以及在分类问题中得出结果（位于输出层上）。（在上述例子中，分类发生在重写段中。）</p><p>网络目的决定了所用的损失函数类型。预训练可选择重构熵函数。分类可选择多类叉熵函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;数据标准化&quot;&gt;数据标准化&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;数据的分布情况如何？数据是否经过适当的缩放？总体上的规则是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果数据是连续值：范围应当在-1到1、0到1，或者呈平均值为0、标准差为1的正态分布。实际的范围不用如此精确
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="Deep learning" scheme="http://dinry.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习调参技巧</title>
    <link href="http://dinry.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/"/>
    <id>http://dinry.github.io/深度学习调参技巧/</id>
    <published>2019-08-01T04:48:50.000Z</published>
    <updated>2019-08-01T09:02:44.427Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="初始化">初始化</span></h1><p>一次惨痛的教训是用normal初始化cnn的参数，最后acc只能到70%多，仅仅改成xavier，acc可以到98%。还有一次给word embedding初始化，最开始使用了TensorFlow中默认的initializer（即glorot_uniform_initializer，也就是大家经常说的无脑使用xavier），训练速度慢不说，结果也不好。改为uniform，训练速度飙升，结果也飙升。所以，初始化就跟黑科技一样，用对了超参都不用调；没用对，跑出来的结果就跟模型有bug一样不忍直视。</p><p>记得刚开始研究深度学习时，做过两个小例子。一个是用tensorflow构建了一个十分简单的只有一个输入层和一个softmax输出层的Mnist手写识别网络，第一次我对权重矩阵W和偏置b采用的是正态分布初始化，一共迭代了20个epoch，当迭代完第一个epoch时，预测的准确度只有10%左右（和随机猜一样，Mnist是一个十分类问题），当迭代完二十个epoch，精度也仅仅达到了60%的样子。然后我仅仅是将权重矩阵W初始化方法改成了全为0的初始化，其他的参数均保持不变，结果在训练完第一个epoch后预测精度就达到了85%以上，最终20个epoch后精度达到92%。另一个例子是回归问题的预测，当时采用的SGD优化器，一开始学习率设定的0.1，模型可以正常训练，只是训练速度有些慢，我试着将学习率调整到0.3，希望可以加速训练速度，结果没迭代几轮loss就变成Nan了。于是从那时起我就深刻的感受到参数调节在深度学习模型训练中的重要意义。</p><p>其实上述问题产生的原因也很好理解，对于参数初始化，因为我们学习的本来就是权重W与偏置b，如果初始化足够好，直接就初始化到最优解，那都不用进行训练了。良好的初始化，可以让参数更接近最优解，这可以大大提高收敛速度，也可以防止落入局部极小。</p><h4><span id="tensorflow常用的初始化方法">tensorflow常用的初始化方法</span></h4><h1><span id="激活函数选择">激活函数选择：</span></h1><p>常用的激活函数有relu、leaky-relu、sigmoid、tanh等。对于输出层，多分类任务选用softmax输出，二分类任务选用sigmoid输出，回归任务选用线性输出。而对于中间隐层，则优先选择relu激活函数（relu激活函数可以有效的解决sigmoid和tanh出现的梯度弥散问题，多次实验表明它会比其他激活函数以更快的速度收敛）。另外，构建序列神经网络（RNN）时要优先选用tanh激活函数。</p><h1><span id="学习率设定">学习率设定：</span></h1><p>一般学习率从0.1或0.01开始尝试。学习率设置太大会导致训练十分不稳定，甚至出现Nan，设置太小会导致损失下降太慢。学习率一般要随着训练进行衰减。衰减系数设0.1，0.3，0.5均可，衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后自动进行衰减。</p><h1><span id="防止过拟合">防止过拟合：</span></h1><p>一般常用的防止过拟合方法有使用L1正则项、L2正则项、dropout、提前终止、数据集扩充等。如果模型在训练集上表现比较好但在测试集上表现欠佳可以选择增大L1或L2正则的惩罚力度（L2正则经验上首选1.0，超过10很少见），或增大dropout的随机失活概率（经验首选0.5）；或者当随着训练的持续在测试集上不增反降时，使用提前终止训练的方法。当然最有效的还是增大训练集的规模，实在难以获得新数据也可以使用数据集增强的方法，比如CV任务可以对数据集进行裁剪、翻转、平移等方法进行数据集增强，这种方法往往都会提高最后模型的测试精度。</p><h1><span id="优化器选择">优化器选择：</span></h1><p>如果数据是稀疏的，就用自适应方法，即 Adagrad, Adadelta, RMSprop, Adam。整体来讲，Adam 是最好的选择。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。</p><h1><span id="残差块与bn层">残差块与BN层</span></h1><p>如果你希望训练一个更深更复杂的网络，那么残差块绝对是一个重要的组件，它可以让你的网络训练的更深。</p><p>BN层具有加速训练速度，有效防止梯度消失与梯度爆炸，具有防止过拟合的效果，所以构建网络时最好要加上这个组件。</p><h1><span id="自动调参方法">自动调参方法：</span></h1><ul><li>Grid Search：网格搜索，在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果。其原理就像是在数组里找最大值。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</li><li>Random Search：经验上，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。另外Random Search往往会和由粗到细的调参策略结合使用，即在效果比较好的参数附近进行更加精细的搜索。</li><li>Bayesian Optimization：贝叶斯优化，考虑到了不同参数对应的 实验结果值，因此更节省时间，贝叶斯调参比Grid Search迭代次数少， 速度快；而且其针对非凸问题依然稳健。</li></ul><h1><span id="深度学习debug的流程策略">深度学习debug的流程策略</span></h1><p>既然消除模型中的错误很难，我们不如先从简单模型入手，然后逐渐增加模型的复杂度。</p><ul><li>从最简单模型入手；</li><li>成功搭建模型，重现结果；</li><li>分解偏差各项，逐步拟合数据；</li><li>用由粗到细随机搜索优化超参数；</li><li>如果欠拟合，就增大模型；如果过拟合，就添加数据或调整。</li></ul><p><img src="https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;初始化&quot;&gt;初始化&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;一次惨痛的教训是用normal初始化cnn的参数，最后acc只能到70%多，仅仅改成xavier，acc可以到98%。还有一次给word embedding初始化，最开始使用了TensorFlow中默认的
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="Deep learning" scheme="http://dinry.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>python-excel</title>
    <link href="http://dinry.github.io/python-excel/"/>
    <id>http://dinry.github.io/python-excel/</id>
    <published>2019-07-25T11:57:58.000Z</published>
    <updated>2019-07-25T12:42:39.803Z</updated>
    
    <content type="html"><![CDATA[<p>python存excel数据</p><pre><code class="language-js">import xlwtimport pdbworkbook=xlwt.Workbook(encoding='utf-8')booksheet=workbook.add_sheet('data', cell_overwrite_ok=True)DATA=(('学号','姓名','年龄','性别','成绩'),      ('1001','A','11','男','12'),      ('1002','B','12','女','22'),      ('1003','C','13','女','32'),      ('1004','D','14','男','52'),      )pdb.set_trace()for i,row in enumerate(DATA):    for j,col in enumerate(row):        booksheet.write(i,j,col)workbook.save('grade.xls')</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;python存excel数据&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;import xlwt
import pdb
workbook=xlwt.Workbook(encoding=&#39;utf-8&#39;)
booksheet=workbook.add_
      
    
    </summary>
    
      <category term="python" scheme="http://dinry.github.io/categories/python/"/>
    
    
      <category term="python语法" scheme="http://dinry.github.io/tags/python%E8%AF%AD%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day16，day17,day18,day19,day20(神经网络)</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/"/>
    <id>http://dinry.github.io/西瓜书day16/</id>
    <published>2019-07-24T07:14:48.000Z</published>
    <updated>2019-07-27T01:39:36.338Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="神经元模型">神经元模型</span></h1><p>神经网络中最基本的成分是神经元模型。</p><h2><span id="m-p神经元模型">M-P神经元模型</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/1.JPG" alt></p><h2><span id="激活函数">激活函数</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/2.JPG" alt></p><h1><span id="感知机与多层网络">感知机与多层网络</span></h1><h2><span id="感知机">感知机</span></h2><p>感知机由两层神经元组成，如图，<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/3.JPG" alt></p><p>感知机能够容易的实现逻辑与、或、非运算， $f(\sum_iw_ix_i-\theta)$, 假定f是阶跃函数，有</p><ul><li>与：令 $w_1=w_2=1,\theta=2$, 则 $y=f(1 \cdot x_1+1 \cdot x_2-2)$,仅在$x_1=x_2=1$ 时，y=1;</li><li>或：令 $w_1=w_2=1,\theta=0.5$, 则 $y=f(1 \cdot x_1+1 \cdot x_2-0.5)$,仅在$x_1=1 or x_2=1$ 时，y=1;</li><li>非：令令 $w_1=-06，w_2=0,\theta=-0.5$, 则 $y=f(-0.6 \cdot x_1+0 \cdot x_2+0.5)$,当$x_1=1$ 时，$y=0$,当 $x_1=0$ 时， $y=1$.</li></ul><p>给定训练数据集，权重与阈值可以通过学习得到。</p><p>感知机学习规则：对训练样例(x,y)，若当前感知机的输出为 $\hat{y}$, 则感知机权重将这样调整：</p><p>$w_i \leftarrow w_i +\Delta w_i$</p><p>$\Delta w_i=\eta(y-\hat{y})x_i$</p><p>其中 $\eta \in (0,1)$ 代表learning rate,.</p><h2><span id="感知机的局限性">感知机的局限性</span></h2><p>感知机只拥有一层功能神经元，学习能力非常有限，只能处理线性可分问题，不能解决抑或问题。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/4.JPG" alt>要解决非线性可分问题，需要考虑多层神经元，例如添加一个隐藏层的两层神经元可以解决“异或”问题。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/5.JPG" alt>神经网络的学习过程，就是根据训练数据来调整神经元之间的权重以及每个功能神经元的阈值。</p><h1><span id="误差逆传播算法bp">误差逆传播算法（BP）</span></h1><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/6.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/7.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/8.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/9.JPG" alt></p><h1><span id="全局最小与局部最小">全局最小与局部最小</span></h1><p>神经网络的训练过程可以看作参数寻优过程，在参数空间中寻找一组最优参数使神经网络在训练集上的误差达到最小。</p><p>局部最优解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的函数值</p><p>全局最小解：参数空间中所有点的误差函数值均不小于该点的函数值</p><p>基于梯度的搜索是使用最为广泛的参数寻优方法</p><h4><span id="局部最小跳出策略">局部最小跳出策略</span></h4><ul><li>以多组不同参数值初始化多个神经网络，取误差最小的参数</li><li>“模拟退火”：以一定概率接受比当前解更差的结果</li><li>随机梯度下降 在计算梯度时加入了随机因素，即使陷入局部极小，也有可能跳出。</li></ul><h1><span id="其他常见的神经网络">其他常见的神经网络</span></h1><h4><span id="rbf">RBF</span></h4><h4><span id="art">ART</span></h4><h4><span id="smo">SMO</span></h4><h4><span id="级联相关网络">级联相关网络</span></h4><h4><span id="elman网络">Elman网络</span></h4><h4><span id="boltzmann">Boltzmann</span></h4><h1><span id="神经网络">神经网络</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;神经元模型&quot;&gt;神经元模型&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;神经网络中最基本的成分是神经元模型。&lt;/p&gt;
&lt;h2&gt;&lt;span id=&quot;m-p神经元模型&quot;&gt;M-P神经元模型&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/%E8%A5%BF%E7%93%
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day13,day14,day15（贝叶斯分类器）</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/"/>
    <id>http://dinry.github.io/西瓜书day13/</id>
    <published>2019-07-19T23:55:41.000Z</published>
    <updated>2019-07-22T10:56:08.546Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="以多分类为例">以多分类为例</span></h1><h1><span id="贝叶斯判定准则">贝叶斯判定准则</span></h1><p>为最小化总体风险，只需在每个样本上选择能使条件风险 $R(c \mid x)$ 最小的类别标记，即为： $h^*(x)=argmin_{c \in y}R(c \mid x)=argmax_{c \in y}P(c \mid x)$,</p><p>此时，$h^*$ 为贝叶斯最优分类器。条件风险：</p><p>$R(c_i \mid x)=\sum_{j=1}^N \lambda_{ij}P(c_j \mid x)=1-P(c_i \mid x)$,</p><p>$\lambda_{ij}$ 是将一个真实标记为 $c_j$ 的样本误分类为 $c_i$ 所产生的损失。</p><h1><span id="多元正态分布的mle">多元正态分布的MLE</span></h1><p>概率模型的训练过程就是参数估计过程。</p><ul><li><p>频率主义学派：参数虽然未知，但却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值</p></li><li><p>贝叶斯学派：参数是为观察到的随机变量，其本身也可以有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。</p></li></ul><p>MLE源自频率主义学派。</p><p>$P(D_c \mid \theta_c)=\prod_{x \in D_c}P(x \mid \theta_c)$</p><p>其中 $D_c$ 表示训练集D中第c类样本组成的集合。</p><p>推导：</p><p>$LL(\theta_c)$</p><p>$=logP(D_c \mid \theta_c)$</p><p>$=\sum_{x \in D_c}logP(x \mid \theta_c)$</p><h2><span id="多元正态分布的概率密度函数">多元正态分布的概率密度函数：</span></h2><p>由于 $P(x \mid \theta_c)=P(x \mid c) -N(\mu_c,\sigma_c^2)$, 那么</p><p>$P(x \mid \theta_c)=\frac{1}{\sqrt{(2\pi)^d\mid\sum_c\mid}}exp(-\frac{1}{2}(x-\mu_c)^T\sum_c^{-1}(x-\mu_c))$</p><p>其中d表示xde维数，$\sum_c=\sigma_c^2$ 为对称正定协方差矩阵，$\mid\sum_c\mid$ 表示行列式，将上式代入对数似然函数可得</p><p>$LL(\theta_c)=\sum_{x \in D_c}ln[\frac{1}{\sqrt{(2\pi)^d\mid\sum_c\mid}}exp(-\frac{1}{2}(x-\mu_c)^T\sum_c^{-1}(x-\mu_c))]$</p><p>$=\sum_{i=1}^Nln[\frac{1}{\sqrt{(2\pi)^d\mid\sum_c\mid}}exp(-\frac{1}{2}(x_i-\mu_c)^T\sum_c^{-1}(x_i-\mu_c))]$</p><p>$=\sum_{i=1}^N{ln\frac{1}{\sqrt{(2\pi)^d}}+ln\frac{1}{\sqrt{\mid \sum_c \mid}}+ln[exp(-\frac{1}{2}(x_i-\mu_c)^T\sum_c^{-1}(x_i-\mu_c))]}$</p><p>$=-\frac{Nd}{2}ln(2\pi)-\frac{N}{2}ln\mid \sum_c\mid-\frac{1}{2}\sum_{i=1}^N(x_i-\mu_c)^T\sum_c^{-1}(x_i-\mu_c))$</p><p>由于参数 $\theta_c$ 的极大似然估计 $\hat{\theta_c}$ 为 $\hat{\theta_c}=argmax_{\theta_c}LL(\theta_c)$，</p><p>对 $LL(\theta_c)$ 关于 $\mu_c$ 求偏导：</p><p>$\frac{\partial LL(\theta_c)}{\partial \mu_c}=-\frac{1}{2}\sum_{i=1}^{N}\frac{\partial (x_i^T-\mu_c^T)\sum_c^{-1}(x_i-\mu_c)}{\partial \mu_c}$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}\frac{\partial [x_i^T\sum_c^{-1}x_i-x_i^T\sum_c^{-1}\mu_c-\mu_c^T\sum_c^{-1}x_i+\mu_c^T\sum_c^{-1}\mu_c]}{\partial \mu_c}$</p><p>由于 $x_i^T\sum_c^{-1}\mu_c=(x_i^T\sum_c^{-1}\mu_c)^T=\mu_c^T(\sum_c^T)^{-1}x_i=\mu_c^T\sum_c^{-1}x_i$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}\frac{\partial [x_i^T\sum_c^{-1}x_i-2x_i^T\sum_c^{-1}\mu_c+\mu_c^T\sum_c^{-1}\mu_c]}{\partial \mu_c}$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}[0-(2x_i^T\sum_c^{-1})^T+(\sum_c^{-1}+(\sum_c^{-1})^T)\mu_c]$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}[-(2\sum_c^{-1}x_i)+2\sum_c^{-1}\mu_c]$</p><p>$=\sum_{i=1}^{N}\Sigma_c^{-1}x_i-N\Sigma_c^{-1}\mu_c$</p><p>$=0$</p><p>$\hat{\mu_c}=\frac{\sum_{i=1}^Nx_i}{N}$</p><p>对 $LL(\theta_c)$ 关于 $\Sigma_c$ 求偏导：</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/4.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/5.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/6.JPG" alt></p><h2><span id="评估">评估</span></h2><p>这种参数化的方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。</p><h1><span id="朴素贝叶斯分类器">朴素贝叶斯分类器</span></h1><p>$h^*(x)=argmax P(c \mid x)=argmax \frac{P(c)P(x \mid c)}{P(x)}=argmaxP(c)P(x \mid c)$ ++++++属性条件独立性假设</p><p>属性条件独立性假设定义：$P(x \mid c)=P(x_1,x_2,...,x_d \mid c)=\prod_{i=1}^{d}P(x_i \mid c)$ -----(牺牲准确度换取计算效率)</p><h2><span id="naive-bayes">naive bayes</span></h2><p>$h^*(x)=argmaxP(c)\prod_{i=1}^dP(x_i \mid c)$</p><h2><span id="先验概率-pc">先验概率 $P(c)$</span></h2><p>$P(c)=\frac{\mid D_c \mid}{\mid D \mid}$</p><h2><span id="似然概率-px_i-mid-c">似然概率 $P(x_i \mid c)$</span></h2><h4><span id="连续值">连续值</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/2.JPG" alt></p><h4><span id="离散值">离散值</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/1.JPG" alt></p><h2><span id="laplacian-correction">Laplacian correction</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/3.JPG" alt>拉普拉斯修正避免了因训练集样本不充足而导致概率估值为0的问题，当训练集样本增大，这个误差会被忽略。</p><h1><span id="em算法">EM算法</span></h1><h2><span id="em算法的引入">EM算法的引入</span></h2><h4><span id="为什么需要em">为什么需要EM？</span></h4><p>训练样本含有隐变量Z</p><h2><span id="em算法的例子">EM算法的例子</span></h2><p>《统计学习方法》-三硬币模型  9.1</p><p>迭代求解参数,近似极大化</p><h2><span id="em算法的导出">EM算法的导出</span></h2><h4><span id="jensen-不等式">Jensen 不等式</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/7.JPG" alt></p><p>可以将a看作概率，则f表示为期望</p><p>Jensen不等式在概率论中的应用：$\varphi(E[X]) \leq E[\varphi(X)]$</p><h4><span id="推导">推导</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/8.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/9.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/10.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/11.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/12.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/13.JPG" alt></p><h2><span id="em算法求解例子">EM算法求解例子</span></h2><p>用EM求解三硬币</p><ul><li>E:求Q</li><li>M：寻找参数最大化期望</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;以多分类为例&quot;&gt;以多分类为例&lt;/span&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;span id=&quot;贝叶斯判定准则&quot;&gt;贝叶斯判定准则&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;为最小化总体风险，只需在每个样本上选择能使条件风险 $R(c \mid x)$ 最小的类别标记，即为： $
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day9,day10,day11,day12(支持向量机)</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/"/>
    <id>http://dinry.github.io/西瓜书day9/</id>
    <published>2019-07-16T00:53:00.000Z</published>
    <updated>2019-07-19T03:21:05.443Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="预备知识">预备知识</span></h1><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/3.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/4.JPG" alt></p><h1><span id="间隔与支持向量">间隔与支持向量</span></h1><p>分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/1.JPG" alt></p><p>直观来说，应该找位于两类训练样本“正中间”的划分超平面，因为其对训练样本局部扰动的“容忍”性最好。</p><p>在训练样本中，划分超平面可以通过线性方程 $\mathbb{w}^T\mathbb{x}+b=0$ 来描述。其中 $\mathbb{w}=(w_1;w_2;...;d_d)$ 为法向量，决定了超平面的方向；b为位移，决定了超平面与原点之间的距离。样本空间中任意点 $x$ 到超平面$(\mathbb{w},b)$ 的距离可写为：</p><p>$r=\frac{\mid \mathbb{w}^T\mathbb{x}+b \mid}{\mid \mid \mathbb{w} \mid \mid}$.</p><p>证明：</p><p>任意取超平面上一个点 $x'$，则点 $x$ 到超平面的距离等于向量 $(x-x')$ 在法向量 $w$（参考预备2）的投影长度（参考预备1）:</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/5.JPG" alt>注意：上式推导过程中，分子之所有取绝对值是由于向量内积可能小于零；另外，由于 $x'$ 是超平上面的点，因此 $\mathbb{w}^T\mathbb{x'}+b=0$，即 $b=-\mathbb{w}^T\mathbb{x'}$。</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/6.JPG" alt></p><p>注意到，距离超平面最近的训练样本可以使上式的等号成立，由6.2知这些训练样本到超平面的距离为：</p><p>$dist=\frac{\mid \mathbb{w}^T\mathbb{x}+b \mid}{\mid \mid \mathbb{w} \mid \mid}=\frac{1}{\mid \mid w \mid \mid}$.</p><p>那么很容易知道，两个异类支持向量到超平面的距离之和是 $\frac{2}{\mid \mid w \mid \mid}$<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/2.JPG" alt></p><h1><span id="支持向量基本型">支持向量基本型</span></h1><p>最大间隔超平面条件等同于最小化如下公式：</p><p>$min_{w,b} \frac{1}{2} \mid \mid \mathbb{w} \mid \mid^2$</p><p>s.t. $y_i(\mathbb{w}^T\mathbb{x}_i+b) \ge 1$, i=1,2,...,m.</p><p>式(6.6)的约束条件意思是训练样本线性可分，也就是说不存在被分类错误的样本，因此也就不存在欠拟合问题；已知优化式(6.6)目标函数是在寻找“最大间隔”的划分超平面，而“最大间隔”划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强，因此可将式(6.6)优化目标进一步解释为寻找最不可能过拟合的分类超平面，这一点与正则化不谋而合。</p><h1><span id="对偶问题">对偶问题</span></h1><h2><span id="拉格朗日乘子法">拉格朗日乘子法</span></h2><p>此出假设优化问题一定有解<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/7.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/8.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/9.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/10.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/11.JPG" alt></p><h1><span id="核函数">核函数</span></h1><p>使训练样本在高维空间可分的映射函数。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/12.JPG" alt>$f(x)=\mathbb{w}^T \phi(x)+b$, 此时w的维度与 $\phi(x)$ 同。</p><p>核函数可以分解成两个向量的内积。要想了解某个核函数是如何将原始特征空间映射到更高维的特征空间的，只需要将核函数分解为两个表达形式完全一样的向量 $\phi(x_i)$ 和 $\phi(x_j)$ 即可（有时很难分解）。以下是LIBSVM中的几个核函数：<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/13.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/14.JPG" alt>遗留问题：核函数的几何意义是什么？核矩阵正定核函数就存在？</p><h1><span id="软间隔与正则化">软间隔与正则化</span></h1><p>软间隔的引入：</p><p>在前面的学习中，一直假设训练样本在样本空间或特征空间是线性可分的，要求所有样本都必须正确划分，称为“硬间隔”，然而现实中很难确定核函数使训练样本线性可分，缓解这一问题的方法是允许SVM在一些样本上出错，因此，引入软间隔：允许某些样本不满足约束 $y_i(w^Tx_i+b) \geq 1$<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/15.JPG" alt></p><h2><span id="预备知识替代损失函数">预备知识：替代损失函数</span></h2><ul><li>凸函数</li><li>连续函数<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/16.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/19.JPG" alt></li></ul><h2><span id="软间隔优化目标函数">软间隔优化目标函数</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/17.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/18.JPG" alt></p><p>引入松弛变量后的目标函数</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/20.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/21.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;预备知识&quot;&gt;预备知识&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/3.JPG&quot; alt&gt;
&lt;img src=&quot;/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/4.
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
</feed>
