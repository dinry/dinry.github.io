<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dinry</title>
  
  <subtitle>notebook</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://dinry.github.io/"/>
  <updated>2019-11-04T10:48:36.188Z</updated>
  <id>http://dinry.github.io/</id>
  
  <author>
    <name>dinry</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CnGAN  Generative Adversarial Networks for Cross-network user preference generation for non-overlapped users</title>
    <link href="http://dinry.github.io/CnGAN/"/>
    <id>http://dinry.github.io/CnGAN/</id>
    <published>2019-11-02T06:14:12.000Z</published>
    <updated>2019-11-04T10:48:36.188Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="intro">Intro</span></h1><p>cross-network Recommendation(are more robust against cold-start and data sparsity issues)</p><h2><span id="problem">problem</span></h2><p>However, despite the growing success of cross-network recommender solutions, the majority of existing solutions can only be applied to users that exist in multiple networks (overlapped users). The remaining non-overlapped users, which form the majority are unable to enjoy the benefits of cross-network solutions.</p><h2><span id="contribution">contribution</span></h2><ul><li>To the best of our knowledge, this is the first attempt to apply a GAN based model to generate missing source network preferences for non-overlapped users.</li><li>We propose CnGAN, a novel GAN based model which includes a novel content loss function and user-based pairwise loss function for the generator and recommender tasks.</li><li>We carry out extensive experiments to demonstrate the effectiveness of CnGAN to conduct recommendations for non-overlapped users and improve the overall quality of recommendations compared to state-of-the-art methods.</li></ul><h1><span id="model">model</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;intro&quot;&gt;Intro&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;cross-network Recommendation(are more robust against cold-start and data sparsity issues)&lt;/p&gt;
&lt;h2&gt;
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
  </entry>
  
  <entry>
    <title>Deep Learning Based Recommender System A Survey and New Perspectives</title>
    <link href="http://dinry.github.io/Survey/"/>
    <id>http://dinry.github.io/Survey/</id>
    <published>2019-11-01T01:33:38.000Z</published>
    <updated>2019-11-04T10:46:42.543Z</updated>
    
    <content type="html"><![CDATA[<p>This article aimsto provide a comprehensive review of recent research efforts on deep learning-based recommender systems.</p><h1><span id="intro">Intro</span></h1><ul><li>We conduct a comprehensive review for recommendation models based on deep learning techniques and propose a classification scheme to position and organize the current work;</li><li>we provide an overview and summary for the state of the art;</li><li>we discuss the challenges and open issues, and identify the new trends and future directions in this research field to share the vision and expand the horizons of deep learning-based recommender systems research.</li></ul><h1><span id="overview-of-recommender-systems-and-deep-learning">overview of recommender systems and deep Learning</span></h1><ul><li>Introduction to the basic terminology and concepts regarding recommender systems and deep learning techniques.</li><li>The reasons and motivations of introducing deep neural networks into recommender systems.</li></ul><h2><span id="recommender">Recommender</span></h2><ul><li>CF</li><li>CB</li><li>Hrbrid</li></ul><h2><span id="deep-learning-techniques">Deep learning techniques</span></h2><p>In this subsection, we clarify a diverse array of architectural paradigms that are closely related to this survey.</p><ul><li>MLP(The Multilayer Perceptron): a feed-forward neural network with multiple (one or more) hidden layers between the input and output layers.</li><li>AE(An Autoencoder):an unsupervised model attempting to reconstruct its input data in the output layer</li><li>CNN:a special kind of feedforward neural network with convolution layers and pooling operations. It can capture the global and local features and significantly enhances efficiency and accuracy. It performs well in processing data with grid-like topology.</li><li>RNN: is suitable for modelling sequential data. Unlike the feedforward neural network, there are loops and memories in RNN to remember former computations. Variants such as Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks are often deployed in practice to overcome the vanishing gradient problem.</li><li>RBM：a two-layer neural network consisting of avisible layer and a hidden layer.</li></ul><h2><span id="why-deep-neural-networks-for-recommendation">Why Deep Neural Networks for Recommendation?</span></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This article aims
to provide a comprehensive review of recent research efforts on deep learning-based recommender systems.&lt;/p&gt;
&lt;h1&gt;&lt;span 
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
  </entry>
  
  <entry>
    <title>RippleNet  Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
    <link href="http://dinry.github.io/ripplenet/"/>
    <id>http://dinry.github.io/ripplenet/</id>
    <published>2019-10-30T02:41:20.000Z</published>
    <updated>2019-11-04T10:47:23.099Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="introduction">introduction</span></h1><p>CF--&gt;sparsity, the cold start--&gt;side information--&gt;knowledge graph</p><h2><span id="kgs-strength">KG's strength</span></h2><ul><li>KG introduces semantic relatedness among items, which can help find their latent connections and improve the precision of recommended items;</li><li>KG consists of relations with various types, which is helpful for extending a user’s interests reasonably and increasing the diversity of recommended items;</li><li>KG connects a user’s historical records andthe recommended ones, thereby bringing explainability to recommendersystems.</li></ul><h2><span id="kgs-categories">KG's categories:</span></h2><ol><li>Embedding-based methods: DKN, CKE, SHINE------Embedding-based methods show high flexibility in utilizing KGto assist recommender systems, but the adopted KGE algorithmsin these methods are usually more suitable for in-graph applicationssuch as link prediction than for recommendation, thusthe learned entity embeddings are less intuitive and effective tocharacterize inter-item relations.</li><li>Path-based methods: explorethe various patterns of connections among items in KG toprovide additional guidance for recommendations. PER, Meta-GraphBased Recommendation.------Path-basedmethods make use of KG in a more natural and intuitive way, butthey rely heavily on manually designed meta-paths, which is hardto optimize in practice. Another concern is that it is impossibleto design hand-crafted meta-paths in certain scenarios (e.g., newsrecommendation) where entities and relations are not within onedomain.</li></ol><h2><span id="ripplenetctr-prediction">RippleNet(CTR prediction)</span></h2><p>The major differencebetween RippleNet and existing literature is that RippleNet combinesthe advantages of the above mentioned two types of methods:(1) RippleNet incorporates the KGE methods into recommendation naturally by preference propagation; (2) RippleNet can automaticallydiscover possible paths from an item in a user’s history to acandidate item, without any sort of hand-crafted design.</p><h2><span id="contribution">contribution</span></h2><ul><li>To the best of our knowledge, this is the first work to combineembedding-based and path-based methods in KG-awarerecommendation.</li><li>We propose RippleNet, an end-to-end framework utilizingKG to assist recommender systems. RippleNet automaticallydiscovers users’ hierarchical potential interests by iterativelypropagating users’ preferences in the KG.</li><li>We conduct experiments on three real-world recommendationscenarios, and the results prove the efficacy</li></ul><p><img src="/ripplenet/1.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;introduction&quot;&gt;introduction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;CF--&amp;gt;sparsity, the cold start--&amp;gt;side information--&amp;gt;knowledge graph&lt;/p&gt;
&lt;h2&gt;
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
      <category term="KG+rec" scheme="http://dinry.github.io/tags/KG-rec/"/>
    
  </entry>
  
  <entry>
    <title>Algorithms for Non-negative Matrix factorization</title>
    <link href="http://dinry.github.io/NMF/"/>
    <id>http://dinry.github.io/NMF/</id>
    <published>2019-10-29T04:56:19.000Z</published>
    <updated>2019-10-31T10:44:39.485Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
      <category term="MF" scheme="http://dinry.github.io/tags/MF/"/>
    
  </entry>
  
  <entry>
    <title>matrix factorization techniques for recommender systems</title>
    <link href="http://dinry.github.io/CF-MF/"/>
    <id>http://dinry.github.io/CF-MF/</id>
    <published>2019-10-28T01:42:31.000Z</published>
    <updated>2019-10-29T04:47:33.881Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="three-strength-of-mf-conclusion">Three strength of MF (conclusion):</span></h1><ul><li>combining good scalability with predictive accuracy.</li><li>offering much flexibility for modeling various rela-life situation.</li><li>allowing incorporation of additional information.</li><li>lower space complexity.</li></ul><h1><span id="recommender-system-strategies">recommender system strategies</span></h1><ol><li>content-based filtering: which creates a profile for each user or product to characterize its nature. However, it requires gathering external information that miht not be available or easy to collect.</li><li>collaborative filtering: which relies only on past user behaviors (previous transactions or ratings)-without requiring the creation of explicit profiles. Although it is domain free and more accurate than content-based filtering, it suffers from the cold start problem, due to its inability to address the system's new products and users.</li></ol><h1><span id="cf">CF</span></h1><ol><li>the neighborhood methods: computing the relatioships between items or, between users.</li></ol><ul><li>item-based CF: evaluates a user's preference for an item based on ratings of &quot;neighboring&quot; items(tend to get similar ratings when rated by the same user) by the same user</li><li>user-based CF:</li></ul><ol start="2"><li>the latent factor model: tries to explain the ratings by characterizing both items and users on, say, 20 to 100 factors inferred from the ratings patterns.</li></ol><h1><span id="mf">MF</span></h1><p>Some of the most successful realization of latent factor models are based on MF, which characterizes both items and users by vectors of factors infered from item rating patterns( combining good scalabilitywith predictive accuracy and offer flexibility).</p><p>Explicit feedback comprises a sparse matrix. Implicit feedback usually represents a densely filled matrix.</p><h1><span id="a-basic-mf-model">A basic MF model</span></h1><p>Matrix factorization models map both users and itemsto a joint latent factor space of dimensionality $f$, such thatuser-item interactions are modeled as inner products inthat space. Accordingly, each item $i$ is associated with a vector $q_i \in \mathbb{R}^f$, and each user $u$ is associatedwith a vector $p_u \in \mathbb{R}^f$. For a given item$i$, the elements of $q_i$ measure the extent towhich the item possesses those factors,positive or negative. For a given user $u$,the elements of $p_u$ measure the extent ofinterest the user has in items that are highon the corresponding factors, again, positiveor negative. The resulting dot product,$q_i^\top p_u$, captures the interaction between user$u$ and item $i$—the user’s overall interest inthe item’s characteristics.</p><p>loss: $min \sum_{(u,i) \in K}(r_{ui}-q_i^\top p_u)^2+\lambda(\mid \mid q_i \mid \mid^2+\mid \mid p_u \mid \mid^2)$</p><h1><span id="learning-algorithms">learning algorithms</span></h1><ul><li><p>stochastic gradient descent</p></li><li><p>Alternating least squares(ALS)</p></li></ul><h1><span id="adding-biases">Adding biases</span></h1><p>$q_i^\top p_u$ tries to capture the interactions between users and items that produce the different rating values. However, much of the observed variation in rating values is due to effects associated with either users or items, known as biases or intercepts, independent of any interactions.</p><p>For example, CF data exhibits large systematic tendencies for some users to give higher ratings than others, and for some items to receive higher ratings than others. Thus, it is unwise to explain the full rating value by an interaction of the form $q_i^\top p_u$. Instead, the system tries to identify the portion of these values that individual user or item biases can explain. A first-order approximation of the bias involved in rating $b_{ui}=\mu +b_i +b_u$ accounts for the user and item effects.</p><p>loss: $min \sum_{(u,i) \in K}(r_{ui}-\mu-b_u-b_i-q_i^\top p_u)^2+\lambda(\mid \mid q_i \mid \mid^2+\mid \mid p_u \mid \mid^2+b_u^2+b_i^2)$</p><h1><span id="additional-input-sources">Additional input sources</span></h1><p>A way to relieve cold start problem is to incorporate additional sources of information about the users.</p><p>$\hat{r}<em>{ui}=\mu+b_i+b_u+q_i^\top[p_u+\mid N(u) \mid^{-0.5}\sum</em>{i \in N(u)}x_i+\sum_{a \in A(u)}y_a]$</p><h1><span id="temporal-dynamics">temporal dynamics</span></h1><p>preference changes over time.</p><p>$\hat{r}_{ui}(t)=\mu+b_i(t)+b_u(t)+q_i^\top p_u(t)$</p><h1><span id="inputs-with-varying-confidence-levels">inputs with varying confidence levels</span></h1><p>In several setups, not all observed ratings deserve the same weight or confidence.</p><p>$min \sum_{(u,i) \in K}c_{ui}(r_{ui}-\mu-b_u-b_i-q_i^\top p_u)^2+\lambda(\mid \mid q_i \mid \mid^2+\mid \mid p_u \mid \mid^2+b_u^2+b_i^2)$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;three-strength-of-mf-conclusion&quot;&gt;Three strength of MF (conclusion):&lt;/span&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;combining good scalability with predi
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
      <category term="MF" scheme="http://dinry.github.io/tags/MF/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统经典论文</title>
    <link href="http://dinry.github.io/recsys-paper/"/>
    <id>http://dinry.github.io/recsys-paper/</id>
    <published>2019-10-28T01:34:37.000Z</published>
    <updated>2019-10-28T01:41:44.749Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="综述类">综述类:</span></h1><p>1、Towards theNext Generation of Recommender Systems: A Survey of the State-of-the-Art andPossible Extensions。最经典的推荐算法综述</p><p>2、Collaborative Filtering Recommender Systems. JB Schafer 关于协同过滤最经典的综述</p><p>3、Hybrid Recommender Systems: Survey and Experiments</p><p>4、项亮的博士论文《动态推荐系统关键技术研究》</p><p>5、个性化推荐系统的研究进展.周涛等</p><p>6、Recommender systems L Lü, M Medo, CH Yeung, YC Zhang, ZK Zhang, T ZhouPhysics Reports 519 (1), 1-49 （https://arxiv.org/abs/1202.1112）个性化推荐系统评价方法综述.周涛等</p><h1><span id="协同过滤">协同过滤：</span></h1><p>1.matrix factorization techniques for recommender systems. Y Koren</p><p>2.Using collaborative filtering to weave an information Tapestry. David Goldberg （协同过滤第一次被提出）</p><p>3.Item-Based Collaborative Filtering Recommendation Algorithms. Badrul Sarwar , George Karypis, Joseph Konstan .etl</p><p>4.Application of Dimensionality Reduction in Recommender System – A Case Study. Badrul M. Sarwar, George Karypis, Joseph A. Konstan etl</p><p>5.Probabilistic Memory-Based Collaborative Filtering. Kai Yu, Anton Schwaighofer, Volker Tresp, Xiaowei Xu,and Hans-Peter Kriegel</p><p>6.Recommendation systems:a probabilistic analysis. Ravi Kumar Prabhakar Raghavan.etl</p><p>7.Amazon.com recommendations: item-to-item collaborative filtering. Greg Linden, Brent Smith, and Jeremy York</p><p>8.Evaluation of Item-Based Top- N Recommendation Algorithms. George Karypis</p><p>9.Probabilistic Matrix Factorization. Ruslan Salakhutdinov</p><p>10.Tensor Decompositions,Alternating Least Squares and other Tales. Pierre Comon, Xavier Luciani, André De Almeida</p><h1><span id="基于内容的推荐">基于内容的推荐：</span></h1><p>1.Content-Based Recommendation Systems. Michael J. Pazzani and Daniel Billsus</p><h1><span id="基于标签的推荐">基于标签的推荐：</span></h1><p>1.Tag-Aware Recommender Systems: A State-of-the-Art Survey. Zi-Ke Zhang(张子柯), Tao Zhou(周 涛), and Yi-Cheng Zhang(张翼成)</p><h1><span id="推荐评估指标">推荐评估指标：</span></h1><p>1、推荐系统评价指标综述. 朱郁筱，吕琳媛</p><p>2、Accurate is not always good：How Accuacy Metrics have hurt Recommender Systems</p><p>3、Evaluating Recommendation Systems. Guy Shani and Asela Gunawardana</p><p>4、Evaluating Collaborative Filtering Recommender Systems. JL Herlocker</p><h1><span id="推荐多样性和新颖性">推荐多样性和新颖性：</span></h1><p>Improving recommendation lists through topic diversification. Cai-Nicolas ZieglerSean M. McNee, Joseph A.Konstan,Georg Lausen</p><p>Fusion-based Recommender System for Improving Serendipity</p><p>Maximizing Aggregate Recommendation Diversity：A Graph-Theoretic Approach</p><p>The Oblivion Problem：Exploiting forgotten items to improve Recommendation diversity</p><p>A Framework for Recommending Collections</p><p>Improving Recommendation Diversity. Keith Bradley and Barry Smyth</p><h1><span id="推荐系统中的隐私性保护">推荐系统中的隐私性保护：</span></h1><p>1、Collaborative Filtering with Privacy. John Canny</p><p>2、Do You Trust Your Recommendations? An Exploration Of Security and Privacy Issues in Recommender Systems. Shyong K “Tony” Lam, Dan Frankowski, and John Ried.</p><p>3、Privacy-Enhanced Personalization. Alfred Kobsa.etl</p><p>4、Differentially Private Recommender Systems：Building Privacy into theNetflix Prize Contenders. Frank McSherry and Ilya Mironov Microsoft Research,Silicon Valley Campus</p><p>5、When being Weak is Brave: Privacy Issues in Recommender Systems. Naren Ramakrishnan, Benjamin J. Keller,and Batul J. Mirza</p><h1><span id="推荐冷启动问题">推荐冷启动问题：</span></h1><p>1.Tied Boltzmann Machines for Cold Start Recommendations. Asela Gunawardana.etl</p><p>2.Pairwise Preference Regression for Cold-start Recommendation. Seung-Taek Park, Wei Chu</p><p>3.Addressing Cold-Start Problem in Recommendation Systems. Xuan Nhat Lam.etl</p><p>4.Methods and Metrics for Cold-Start Recommendations. Andrew I. Schein, Alexandrin P opescul, Lyle H. U ngar</p><p>bandit(老虎机算法,可缓解冷启动问题):</p><p>1、Bandits and Recommender Systems. Jeremie Mary, Romaric Gaudel, Philippe Preux</p><p>2、Multi-Armed Bandit Algorithms and Empirical Evaluation</p><h1><span id="基于社交网络的推荐">基于社交网络的推荐：</span></h1><p>Social Recommender Systems. Ido Guy and David Carmel</p><p>A Social Networ k-Based Recommender System(SNRS). Jianming He and Wesley W. Chu</p><p>Measurement and Analysis of Online Social Networks.</p><p>Referral Web：combining social networks and collaborative filtering</p><h1><span id="基于知识的推荐">基于知识的推荐：</span></h1><p>1、Knowledge-based recommender systems. Robin Burke</p><p>2、Case-Based Recommendation. Barry Smyth</p><p>3、Constraint-based Recommender Systems: Technologies and Research Issues. A. Felfernig. R. Burke</p><h1><span id="其他">其他：</span></h1><p>Trust-aware Recommender Systems. Paolo Massa and Paolo Avesani</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;综述类&quot;&gt;综述类:&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;1、Towards the
Next Generation of Recommender Systems: A Survey of the State-of-the-Art and
Possible E
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
      <category term="paper" scheme="http://dinry.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>graph-attention-networks</title>
    <link href="http://dinry.github.io/graph-attention-networks/"/>
    <id>http://dinry.github.io/graph-attention-networks/</id>
    <published>2019-10-08T11:18:38.000Z</published>
    <updated>2019-10-21T05:12:04.355Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>花书第二周--机器学习基本概念</title>
    <link href="http://dinry.github.io/%E8%8A%B1%E4%B9%A6%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%881%EF%BC%89/"/>
    <id>http://dinry.github.io/花书第二周（1）/</id>
    <published>2019-10-08T05:53:29.000Z</published>
    <updated>2019-10-08T07:37:19.588Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="机器学习算法">机器学习算法</span></h1><ol><li>什么是学习？对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务P上由性能度量P衡量的性能有所提升。</li><li>什么是机器学习算法？能够从数据中学习的算法</li></ol><h2><span id="11-任务t">1.1 任务T</span></h2><h4><span id="机器学习的任务是什么">机器学习的任务是什么？</span></h4><p>机器学习系统应该如何处理样本，即对严办进行一个复杂的非线性变化从而得到正确的结果</p><p>我们通常会将样本表示成一个向量 $x \in \mathbb{R}^n$, 其中向量的每一个元素是一个特征</p><h4><span id="常见的任务有哪些">常见的任务有哪些？</span></h4><ul><li>分类： $f: \mathbb{R}^n \to {1,...,k}$ （输出离散，输出为概率分布）</li><li>回归： $f: \mathbb{R}^n \to \mathbb{R}$ （输出连续）</li><li>转录：OCR, ASR</li><li>机器翻译： seq2seq</li><li>结构化输出：输出值之间内部密切相关，如语法树</li><li>异常检测</li><li>合成与检测</li><li>缺失值填充</li><li>去噪</li><li>密度估计</li></ul><p>本节重点讲了分类回归问题的区别</p><p>度量：</p><p>分类：precision, recall, auc, roc, f1</p><p>回归: mse</p><h2><span id="12-经验">1.2 经验</span></h2><p>无监督学习：</p><p>含有很多特征的数据集，但是没有label</p><p>监督学习：</p><p>（线性回归，LDA,SVM)数据集有label or tag</p><h1><span id="容量过拟合和欠拟合">容量，过拟合和欠拟合</span></h1><p>训练集：测试集：衡量模型的好坏</p><ul><li><p>underfitting: 参数太少，模型简单</p></li><li><p>appropriate capacity: 泛化能力最好</p></li><li><p>overfitting: 模型复杂，训练集误差小，测试集误差大</p></li></ul><p>模型泛化：模型容量</p><p>泛化误差：测试误差</p><h2><span id="model原则">model原则：</span></h2><ul><li>剃刀原则： 若有多个假设与观察一致，选择最简单的</li><li>没有免费午餐定理：不存在能够在所有可能的分类问题中性能均为最优的算法</li><li>解决方案：尽可能深入了解分布，寻找先验知识</li><li>正则化：降低泛化误差而非训练误差，L1,L2（为什么？）</li></ul><p>$J(w)=MSE_{train}+\lambda w^{\top}w$</p><h2><span id="超参数和验证集">超参数和验证集</span></h2><p>超参数：用于挑选超参数的数据子集成为验证集，通常8：2</p><p>验证集：交叉验证，留出法，k折交叉验证</p><p>实际工作经验：训练集，交叉验证集，测试集</p><p>训练集：训练数据</p><p>交叉验证集：判断学习率是否要调整，何时结束训练，每一个epoch都测试</p><p>一般来讲，训练数据每过一个epoch, 都要在交叉验证集上看一下</p><p>性能：损失函数</p><p>测试集：判断模型的性能好坏，最后用</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;机器学习算法&quot;&gt;机器学习算法&lt;/span&gt;&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;什么是学习？
对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务P上由性能度量P衡量的性能有所提升。&lt;/li&gt;
&lt;li&gt;什么是机器学
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>花书作业3（梯度下降小程序）</title>
    <link href="http://dinry.github.io/%E8%8A%B1%E4%B9%A6%E4%BD%9C%E4%B8%9A3/"/>
    <id>http://dinry.github.io/花书作业3/</id>
    <published>2019-10-06T08:43:50.000Z</published>
    <updated>2019-10-06T08:47:06.777Z</updated>
    
    <content type="html"><![CDATA[<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">def f2(x,y):</span><br><span class="line">    return np.exp(x*x+(y-2)*(y-2))</span><br><span class="line">def hx1(x,y):</span><br><span class="line">    return 2*x*np.exp(x*x+(y-2)*(y-2))</span><br><span class="line">def hx2(x,y):</span><br><span class="line">    return 2*(y-2)*np.exp(x*x+(y-2)*(y-2))</span><br><span class="line">X1 = np.arange(0,1,0.1)</span><br><span class="line">X2 = np.arange(0,1,0.1)</span><br><span class="line">X1, X2 = np.meshgrid(X1, X2) # 生成xv、yv，将X1、X2变成n*m的矩阵，方便后面绘图</span><br><span class="line">#pdb.set_trace()</span><br><span class="line">Y = np.array(list(map(lambda t : f2(t[0],t[1]),zip(X1.flatten(),X2.flatten()))))</span><br><span class="line">Y.shape = X1.shape # 1600的Y图还原成原来的（40,40）</span><br><span class="line"></span><br><span class="line">x1 = 1</span><br><span class="line"></span><br><span class="line">x2 = 1</span><br><span class="line"></span><br><span class="line">alpha = 0.01</span><br><span class="line"></span><br><span class="line">GD_X1 = [x1]</span><br><span class="line"></span><br><span class="line">GD_X2 = [x2]</span><br><span class="line"></span><br><span class="line">GD_Y = [f2(x1, x2)]</span><br><span class="line"></span><br><span class="line">y_change = f2(x1, x2)</span><br><span class="line"></span><br><span class="line">iter_num = 0</span><br><span class="line"></span><br><span class="line">while (y_change &gt; 1e-20):</span><br><span class="line">    tmp_x1 = x1 - alpha * hx1(x1, x2)</span><br><span class="line"></span><br><span class="line">    tmp_x2 = x2 - alpha * hx2(x1, x2)</span><br><span class="line"></span><br><span class="line">    tmp_y = f2(tmp_x1, tmp_x2)</span><br><span class="line"></span><br><span class="line">    f_change = np.absolute(tmp_y - f2(x1, x2))</span><br><span class="line">    y_change=f_change</span><br><span class="line"></span><br><span class="line">    x1 = tmp_x1</span><br><span class="line"></span><br><span class="line">    x2 = tmp_x2</span><br><span class="line"></span><br><span class="line">    GD_X1.append(x1)</span><br><span class="line"></span><br><span class="line">    GD_X2.append(x2)</span><br><span class="line"></span><br><span class="line">    GD_Y.append(tmp_y)</span><br><span class="line"></span><br><span class="line">    iter_num += 1</span><br><span class="line"></span><br><span class="line">print(u&quot;最终结果为:(%.5f, %.5f, %.5f)&quot; % (x1, x2, f2(x1, x2)))</span><br><span class="line"></span><br><span class="line">print(u&quot;迭代过程中X的取值，迭代次数:%d&quot; % iter_num)</span><br><span class="line"></span><br><span class="line">print(GD_X1)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(facecolor=&apos;w&apos;, figsize=(20, 18))</span><br><span class="line"></span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line">ax.plot_surface(X1, X2, Y, rstride=1, cstride=1, cmap=plt.cm.jet)</span><br><span class="line"></span><br><span class="line">ax.plot(GD_X1, GD_X2, GD_Y, &apos;ko-&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(&apos;x&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(&apos;y&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_zlabel(&apos;z&apos;)</span><br><span class="line"></span><br><span class="line">ax.set_title(u&apos;函数;\n学习率:%.3f; 最终解:(%.3f, %.3f, %.3f);迭代次数:%d&apos; % (alpha, x1, x2, f2(x1, x2), iter_num))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p><p><img src="/%E8%8A%B1%E4%B9%A6%E4%BD%9C%E4%B8%9A3/1.JPG" alt><img src="/%E8%8A%B1%E4%B9%A6%E4%BD%9C%E4%B8%9A3/2.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span cl
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="Deep learning" scheme="http://dinry.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计，误差的高斯分布与最小二乘估计的等价性,最优化</title>
    <link href="http://dinry.github.io/MLE/"/>
    <id>http://dinry.github.io/MLE/</id>
    <published>2019-10-04T02:42:22.000Z</published>
    <updated>2019-10-05T06:51:41.691Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="极大似然估计">极大似然估计</span></h1><p>假设随机变量 $X-P(x;\theta)$</p><p>现有样本 $x_1$, $x_2$, ... $x_N$</p><p>定义似然函数为 $L=P(x_1;\theta)P(x_2;\theta) ... P(x_N;\theta)$</p><p>对数似然函数为 $\hat{L}=ln[P(x_1;\theta)P(x_2;\theta) ... P(x_N;\theta)]$</p><p>极大似然估计为 max L</p><p>高斯分布 $P(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$</p><p>$L=ln[\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_1-\mu)^2}{2\sigma^2}},\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_2-\mu)^2}{2\sigma^2}},...,\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_N-\mu)^2}{2\sigma^2}}]$</p><p>$\frac{\partial L}{\partial \mu}=0  \to \mu=\frac{x_1+x_2+...+x_N}{N}$</p><p>$\frac{\partial L}{\partial \sigma}=0 \to \sigma^2=\frac{\sum_{i=1}^N(x_i-\mu)^2}{N}$ (有偏方差)</p><h1><span id="误差的高斯分布与最小二乘估计的等价性">误差的高斯分布与最小二乘估计的等价性</span></h1><p>$x_1,x_2,...,x_N, x_i \in \mathbb{R}^n$</p><p>$y_1,y_2,...,y_N, y_i \in \mathbb{R}^n$</p><p>$\hat{y_i}=w^\top x_i, w \in \mathbb{R}^n$</p><p>拟合误差： $e_i=y_i - w^\top x_i$</p><p>若设$e_i-\frac{1}{\sqrt{2\pi}}e^{\frac{e_i^2}{2}}$</p><p>似然函数： $L=ln[\frac{1}{\sqrt{2\pi}}e^{\frac{e_1^2}{2}} \frac{1}{\sqrt{2\pi}}e^{\frac{e_2^2}{2}}...\frac{1}{\sqrt{2\pi}}e^{\frac{e_N^2}{2}}]=-Nln\sqrt{2\pi}-\frac{1}{2}(e_1^2+e_2^2+...+e_N^2)$</p><p>最大化L等价于最小化 $e_1^2+e_2^2+...+e_N^2$</p><p>$min(y_1-w^\top x_1)^2+(y_2-w^\top x_2)^2+...+(y_N-w^\top x_N)^2=J$</p><p>$\frac{\partial J}{\partial w}=0 \to \sum_{i=1}^{N}x_iy_i=\sum_{i=1}^N w^\top x_i x_i$</p><h1><span id="无约束优化">无约束优化</span></h1><p>无约束优化问题是机器学习中最普遍，最简单的优化问题： $x^*=min_{x}f(x)$</p><h2><span id="梯度下降">梯度下降</span></h2><p>沿负梯度方向</p><h2><span id="牛顿法">牛顿法</span></h2><p>$f(x_{t+1})=f(x_t)+f^&quot;(x_t)(x_{t+1}-x_t)$</p><h2><span id="拟牛顿">拟牛顿</span></h2><h2><span id="共轭梯度">共轭梯度</span></h2><h2><span id="收敛速度">收敛速度</span></h2><p>梯度下降是一次收敛，牛顿是二次收敛（速度快，但接近最优点才收敛,否则发散）</p><h1><span id="有约束最优化">有约束最优化</span></h1><h2><span id="拉格朗日乘子法约束为等式">拉格朗日乘子法(约束为等式)</span></h2><p>$min_x f(x)$</p><p>$s.t.g(x)=0$</p><p>$L(x,\lambda)=f(x)+\lambda g(x)$</p><h2><span id="kkt-约束为不等式表示一个范围">KKT （约束为不等式，表示一个范围）</span></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;极大似然估计&quot;&gt;极大似然估计&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;假设随机变量 $X-P(x;\theta)$&lt;/p&gt;
&lt;p&gt;现有样本 $x_1$, $x_2$, ... $x_N$&lt;/p&gt;
&lt;p&gt;定义似然函数为 $L=P(x_1;\theta)P(x_2;
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>花书打卡2 （极大似然估计以及优化理论）</title>
    <link href="http://dinry.github.io/%E8%8A%B1%E4%B9%A6%E6%89%93%E5%8D%A12/"/>
    <id>http://dinry.github.io/花书打卡2/</id>
    <published>2019-10-03T06:36:06.000Z</published>
    <updated>2019-10-05T00:38:18.665Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="chapter-3-概率与信息论">Chapter 3: 概率与信息论</span></h1><h2><span id="31-概率的使用">3.1 概率的使用</span></h2><p>机器学习通常必须处理不确定量，有时也要处理随机量，概率用来量化这些不确定性</p><p>不确定性的三种来源：</p><ul><li>被建模系统内在的随机性</li><li>不完全观测</li><li>不完全建模</li></ul><h2><span id="32-随机变量">3.2 随机变量</span></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;chapter-3-概率与信息论&quot;&gt;Chapter 3: 概率与信息论&lt;/span&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;span id=&quot;31-概率的使用&quot;&gt;3.1 概率的使用&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;机器学习通常必须处理不确定量，有时也要处理随机量，概率用来
      
    
    </summary>
    
      <category term="花书" scheme="http://dinry.github.io/categories/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="deep learning" scheme="http://dinry.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>python获取交集，并集，差集的方法</title>
    <link href="http://dinry.github.io/%E8%8E%B7%E5%8F%96%E4%BA%A4%E9%9B%86%EF%BC%8C%E5%B9%B6%E9%9B%86%EF%BC%8C%E5%B7%AE%E9%9B%86%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/获取交集，并集，差集的方法/</id>
    <published>2019-09-21T03:01:53.000Z</published>
    <updated>2019-09-21T03:04:44.294Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="交集">交集</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#方法一:</span><br><span class="line">a=[2,3,4,5]</span><br><span class="line">b=[2,5,8]</span><br><span class="line">tmp = [val for val in a if val in b]</span><br><span class="line">print tmp</span><br><span class="line">#[2, 5]</span><br><span class="line"></span><br><span class="line">#方法二</span><br><span class="line">print list(set(a).intersection(set(b)))</span><br><span class="line"></span><br><span class="line">#方法二比方法一快很多！</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p><h1><span id="并集">并集</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print list(set(a).union(set(b)))</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p><h1><span id="获取两个-list-的差集">获取两个 list 的差集</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print list(set(b).difference(set(a))) # b中有而a中没有的      非常高效！</span><br><span class="line">```js</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;交集&quot;&gt;交集&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span 
      
    
    </summary>
    
      <category term="python" scheme="http://dinry.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://dinry.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Word Embedding</title>
    <link href="http://dinry.github.io/embedding/"/>
    <id>http://dinry.github.io/embedding/</id>
    <published>2019-09-19T10:33:25.000Z</published>
    <updated>2019-09-20T00:56:22.540Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="dimension-reduction">dimension reduction</span></h1><p><img src="/embedding/1.JPG" alt></p><h1><span id="word-embedding">word Embedding</span></h1><ul><li>Machine learn the meaning of words from reading a lot of documents without supervision.</li><li>Generating Word Vector is Unsupervised</li><li>A word can be understood by its context.<img src="/embedding/2.JPG" alt></li></ul><h1><span id="how-to-exploit-the-context">How to exploit the context?</span></h1><ul><li>count based: If two words $w_i$ and $w_j$ frequently co-occur, $V(w_i)$ and $V(w_j)$ would be close to each other.(Glove Vector)</li></ul><p>$V(w_i) \cdot V(w_j) \to N_{i,j}$, where number of times $w_i$ and $w_j$ in the same document.</p><ul><li>prediction based: predict next word based on previous words.</li></ul><p><img src="/embedding/3.JPG" alt></p><ul><li>take out he input of the neurons in the first layer.</li><li>use it to represent a word w</li><li>word vector. word embedding feature: V(w)具有相同上下文的单词具有相近的分布<img src="/embedding/4.JPG" alt><img src="/embedding/5.JPG" alt><img src="/embedding/6.JPG" alt>如何让两个weight一样？一样有什么好处？</li><li>Given the same initialization</li><li><img src="/embedding/7.JPG" alt></li><li>cross entropy: <img src="/embedding/8.JPG" alt></li></ul><h2><span id="two-class">two class:</span></h2><ul><li>Cbow</li><li>skip-gram<img src="/embedding/9.JPG" alt>结构信息：结构，包含关系等<img src="/embedding/10.JPG" alt><img src="/embedding/11.JPG" alt></li></ul><h1><span id="document-embedding">document Embedding</span></h1><p><img src="/embedding/12.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;dimension-reduction&quot;&gt;dimension reduction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/embedding/1.JPG&quot; alt&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;word-embedding&quot;&gt;wor
      
    
    </summary>
    
      <category term="NLP" scheme="http://dinry.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://dinry.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理NLP中的N-gram模型</title>
    <link href="http://dinry.github.io/n-gram/"/>
    <id>http://dinry.github.io/n-gram/</id>
    <published>2019-09-16T11:45:18.000Z</published>
    <updated>2019-09-17T01:03:58.949Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="naive-bayes">Naive Bayes</span></h1><p>见 https://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/， 这里再复习一下。</p><p>朴素贝叶斯的关键组成是贝叶斯公式与条件独立性假设。为了方便说明，我们举一个垃圾短信分类的例子。</p><p><strong>&quot;在家日赚百万，惊人秘密...&quot;</strong></p><p>$p(垃圾短信 \mid &quot;在家日赚百万&quot;)∝p(垃圾邮件)p(&quot;在家日赚百万&quot;\mid 垃圾短信)$</p><p>由条件独立性假设：</p><p>$p(&quot;在家日赚百万&quot; \mid J)=p(&quot;在&quot;,&quot;家&quot;,&quot;日&quot;,&quot;赚&quot;,&quot;百&quot;,&quot;万&quot;∣J)=p(&quot;在&quot; \mid J)p(&quot;家&quot;\mid J)p(&quot;日&quot;\mid J)p(&quot;赚&quot;\mid J)p(&quot;百&quot;\mid J)p(&quot;万&quot;\mid J)$</p><p>上面每一项条件概率都可以通过在训练数据的垃圾短信中统计每个字出现的次数得到，然而这里有一个问题，朴素贝叶斯将句子处理为一个 <strong>词袋模型（Bag-of-Words, BoW）</strong> ，以至于不考虑每个单词的顺序。这一点在中文里可能没有问题，因为有时候即使把顺序捣乱，我们还是能看懂这句话在说什么，但有时候不行，例如：</p><p><strong>我烤面筋 = 面筋烤我 ？</strong></p><p>那么有没有模型是考虑句子中单词之间的顺序的呢？有，N-gram就是。</p><h1><span id="n-gram">N-gram</span></h1><h2><span id="n-gram简介">N-gram简介</span></h2><p>在介绍N-gram之前，让我们回想一下**“联想”**的过程是怎样发生的。如果你是一个玩LOL的人，那么当我说“正方形打野”、“你是真的皮”，“你皮任你皮”这些词或词组时，你应该能想到的下一个词可能是“大司马”，而不是“五五开”。如果你不是LOL玩家，没关系，当我说“上火”、“金罐”这两个词，你能想到的下一个词应该更可能“加多宝”，而不是“可口可乐”。</p><p>N-gram正是基于这样的想法，它的第一个特点是某个词的出现依赖于其他若干个词，第二个特点是我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。</p><p>N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。</p><p><img src="/n-gram/1.JPG" alt></p><p>N-gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有 $Bi-gram(N=2)$ 和 $Tri-gram(N=3)$，一般已经够用了。例如在上面这句话里，我可以分解的 Bi-gram 和 Tri-gram ：</p><p>Bi-gram :  {I, love}, {love, deep}, {love, deep}, {deep, learning}</p><p>Tri-gram :  {I, love, deep}, {love, deep, learning}</p><h2><span id="n-gram中的概率计算">N-gram中的概率计算</span></h2><p>假设我们有一个由n个词组成的句子 $S=(w_1​,w_2​,⋯,w_n​)$，如何衡量它的概率呢？让我们假设，每一个单词 $w_i$ ​都要依赖于从第一个单词 $w_1$ ​到它之前一个单词 $w_{i−1}$​的影响：</p><p>$p(S)=p(w_1w_2⋯w_n)=p(w_1)p(w_2 \mid w_1)⋯p(w_n \mid w_{n−1}⋯w_2w_1)$</p><p>是不是很简单？是的，不过这个衡量方法有两个缺陷：</p><ul><li>参数空间过大，概率 $p(w_n \mid w_{n−1}⋯w_2w_1)$ 的参数有 $O(n)$ 个。</li><li>数据稀疏严重，词同时出现的情况可能没有，组合阶数高时尤其明显。</li></ul><p>为了解决第一个问题，我们引入马尔科夫假设（Markov Assumption）：一个词的出现仅与它之前的若干个词有关。</p><p>$p(w_1⋯w_n)=\prod p(w_i \mid w_{i-1}⋯w_1)=\prod p(w_i \mid w_{i-1}⋯w_{i-N+1})$</p><ul><li>如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 Bi-gram：$p(S)=p(w_1w_2⋯w_n)=p(w_1)p(w_2 \mid w_1)⋯p(w_n \mid w_{n-1})$</li><li>如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 Tri-gram</li></ul><p>N-gram的N可以取很高，然而现实中一般 bi-gram 和 tri-gram 就够用了。</p><p>那么，如何计算其中的每一项条件概率 $p(w_n \mid w_{n−1}⋯w_2w_1)$ 呢？答案是 <strong>极大似然估计（Maximum Likelihood Estimation，MLE）</strong>，说人话就是数频数：</p><p><img src="/n-gram/2.JPG" alt></p><h1><span id="n-gram-的用途">N-gram 的用途</span></h1><h2><span id="用途一词性标注">用途一：词性标注</span></h2><p><img src="/n-gram/3.JPG" alt></p><h2><span id="用途二垃圾短信分类">用途二：垃圾短信分类</span></h2><p><img src="/n-gram/4.JPG" alt></p><h2><span id="用途三分词器">用途三：分词器</span></h2><p><img src="/n-gram/5.JPG" alt></p><h2><span id="用途四机器翻译和语音识别">用途四：机器翻译和语音识别</span></h2><p><img src="/n-gram/6.JPG" alt></p><h1><span id="n-gram中n的确定">N-gram中N的确定</span></h1><p>为了确定N的取值，《Language Modeling with Ngrams》使用了 Perplexity 这一指标，该指标越小表示一个语言模型的效果越好。文章使用了华尔街日报的数据库，该数据库的字典大小为19,979，训练集包含 38 million 个词，测试集包含 1.5 million 个词。针对不同的N-gram，计算各自的 Perplexity。<img src="/n-gram/7.JPG" alt>结果显示，Tri-gram的Perplexity最小，因此它的效果是最好的。</p><h1><span id="n-gram中的数据平滑方法">N-gram中的数据平滑方法</span></h1><p>上面提到，N-gram的N越大，模型 Perplexity 越小，表示模型效果越好。这在直观意义上是说得通的，毕竟依赖的词越多，我们获得的信息量越多，对未来的预测就越准确。然而，语言是有极强的创造性的（Creative），当N变大时，更容易出现这样的状况：某些n-gram从未出现过，这就是稀疏问题。</p><p>n-gram最大的问题就是稀疏问题（Sparsity）。例如，在bi-gram中，若词库中有20k个词，那么两两组合其中的很多组合在语料库中都没有出现，根据极大似然估计得到的组合概率将会是0，从而整个句子的概率就会为0。最后的结果是，我们的模型只能计算零星的几个句子的概率，而大部分的句子算得的概率是0，这显然是不合理的。</p><p>因此，我们要进行数据平滑（data Smoothing），数据平滑的目的有两个：一个是使所有的N-gram概率之和为1，使所有的n-gram概率都不为0。它的本质，是重新分配整个概率空间，使已经出现过的n-gram的概率降低，补充给未曾出现过的n-gram。<img src="/n-gram/8.JPG" alt>关于N-gram的训练数据，如果你以为 <strong>“只要是英语就可以了”</strong>，那就大错特错了。文献《Language Modeling with Ngrams》**的作者做了个实验，分别用莎士比亚文学作品，以及华尔街日报作为训练集训练两个N-gram，他认为，两个数据集都是英语，那么用他们生成的文本应该也会有所重合。然而结果是，用两个语料库生成的文本没有任何重合性，即使在语法结构上也没有。  这告诉我们，N-gram的训练是很挑数据集的，你要训练一个问答系统，那就要用问答的语料库来训练，要训练一个金融分析系统，就要用类似于华尔街日报这样的语料库来训练。</p><h1><span id="n-gram的进化版nnlm">N-gram的进化版：NNLM</span></h1><p>NNLM 即 Neural Network based Language Model，由Bengio在2003年提出，它是一个很简单的模型，由四层组成，输入层、嵌入层、隐层和输出层。模型接收的输入是长度为nn的词序列，输出是下一个词的类别。首先，输入是单词序列的index序列，例如单词 I 在字典（大小为 $\mid V \mid$）中的index是10，单词 am 的 index 是23， Bengio 的 index 是65，则句子“I am Bengio”的index序列就是 10, 23, 65。嵌入层（Embedding）是一个大小为 $\mid V \mid \times K$ 的矩阵，从中取出第10、23、65行向量拼成 $3\times K$ 的矩阵就是Embedding层的输出了。隐层接受拼接后的Embedding层输出作为输入，以tanh为激活函数，最后送入带softmax的输出层，输出概率。</p><p>NNLM最大的缺点就是参数多，训练慢。另外，NNLM要求输入是定长n，定长输入这一点本身就很不灵活，同时不能利用完整的历史信息。</p><p><img src="/n-gram/9.JPG" alt></p><h1><span id="nnlm的进化版rnnlm">NNLM的进化版：RNNLM</span></h1><p>针对NNLM存在的问题，Mikolov在2010年提出了RNNLM，其结构实际上是用RNN代替NNLM里的隐层，这样做的好处包括减少模型参数、提高训练速度、接受任意长度输入、利用完整的历史信息。同时，RNN的引入意味着可以使用RNN的其他变体，像LSTM、BLSTM、GRU等等，从而在时间序列建模上进行更多更丰富的优化。</p><p>http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf</p><h1><span id="word2vec">Word2Vec</span></h1><p>https://www.jianshu.com/p/e91f061d6d91</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;见 https://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/， 这里再复习一下。&lt;/p&gt;
&lt;p&gt;朴素贝叶斯的关键组
      
    
    </summary>
    
      <category term="NLP" scheme="http://dinry.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>www2019_recommender system</title>
    <link href="http://dinry.github.io/www2019/"/>
    <id>http://dinry.github.io/www2019/</id>
    <published>2019-09-08T07:00:55.000Z</published>
    <updated>2019-09-16T11:53:46.999Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="1cross-domain-recommendation-without-sharing-user-relevant-data">1.Cross-domain Recommendation Without Sharing User-relevant Data</span></h1><p>研究方向：cross-domain recommendation Task</p><p>Goal: combine data from different websites to improve recommendation task</p><p>Challenge:Despite many research efforts on this task, the main drawback is that they largely assume the data of different systems can be fully shared.</p><p>methods: NATR(short for Neural Attentive Transfer Recommendation)To avoid the leak of user privacy during the data sharing process, it consider sharing only the information of the item side, rather than user behavior data. Specifically, we transfer the item embeddings across domains, making it easier for two companies to reach a consensus (e.g., legal policy) on data sharing since the data to be shared is user-irrelevant and has no explicit semantics.</p><p>step:</p><p><img src="/www2019/1.JPG" alt>Our proposed solution, which has three steps, is illustrated in Figure 1.</p><ul><li>In the first step, an embedding-based recommender model, MF for example, is trained on the user-item interaction matrix of the auxiliary domain to obtain item embeddings.</li><li>In the second step, item embeddings of the auxiliary domain are sent to the target domain; note that only the embeddings of overlapped items are necessary to be sent, which are subjected to the data-sharing policy between two companies.</li><li>Finally, the target domain trains a recommender model with the consideration of the transferred item embeddings.</li></ul><h2><span id="contribution">contribution</span></h2><ul><li>We present a new paradigm for cross-domain recommendation without sharing user-relevant data, in which only item-side data can be shared across domains. To allow the transferring of CF signal, we propose to share the item embeddings which are learned from user-item interactions of the auxiliary domain.</li><li>We propose a new solution NATR to resolve the key challenges in leveraging transferred item embeddings. The twolevel attention design allows NATR to distill useful signal from transferred item embeddings, and appropriately combine them with the data of the target domain.</li><li>We conduct extensive experiments on two real-world datasets to demonstrate our proposed method. More ablation studies verify the efficacy of our designed components, and the utility of transferred item embeddings in addressing the data sparsity issue.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/2.JPG" alt></p><h1><span id="2dual-graph-attention-networks-for-deep-latent-representation-of-multifaceted-social-effects-in-recommender-systems">2.Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems</span></h1><h2><span id="abstract">Abstract</span></h2><p><img src="/www2019/4.JPG" alt></p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/5.JPG" alt></p><h2><span id="framework">framework</span></h2><p><img src="/www2019/3.JPG" alt></p><h1><span id="3exploiting-ratings-reviews-and-relationships-for-item-recommendations-in-topic-based-social-networks">3.Exploiting Ratings, Reviews and Relationships for Item Recommendations in Topic Based Social Networks</span></h1><h2><span id="abstract">Abstract</span></h2><p>Many e-commerce platforms today allow users to give their rating scores and reviews on items as well as to establish social relationships with other users. As a result, such platforms accumulate heterogeneous data including numeric scores, short textual reviews, and social relationships. HHowever, many recommender systems only consider historical user feedbacks in modeling user preferences. More specifically, most existing recommendation approaches only use rating scores but ignore reviews and social relationships in the user-generated data. In this paper, we propose TSNPF—a latent factor model to effectively capture user preferences and item features. Employing Poisson factorization, TSNPF fully exploits the wealth of information in rating scores, review text and social relationships altogether. It extracts topics of items and users from the review text and makes use of similarities between user pairs with social relationships, which results in a comprehensive understanding of user preferences. Experimental results on real-world datasets demonstrate that our TSNPF approach is highly effective at recommending items to users.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose a method based on Gamma-Poisson distribution to extract the topic intensities of items and users from usergenerated textual reviews. Compared to previous techniques, our method is able to address the usual problem of data scarcity.</li><li>We propose TSNPF, a conjugate graphical model based on Poisson factorization which only models non-zero observations in ratings, reviews and social relations simultaneously via interpretable user preferences and item attributes. In addition, we propose a closed form mean-field variational inference method to train TSNPF.</li><li>We evaluate the performance of TSNPF using three publicly available real datasets. The results show that TSNPF outperforms state-of-the-art alternatives.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/6.JPG" alt></p><p>主题提取与推荐系统的结合</p><h1><span id="4ghostlink-latent-network-inference-for-influence-aware-recommendationintroduction-写的不错">4.GhostLink: Latent Network Inference for Influence-aware Recommendation.(introduction 写的不错)</span></h1><h2><span id="abstract-probabilistic-graphical-model">Abstract (probabilistic graphical model)</span></h2><p>Social influence plays a vital role in shaping a user’s behavior in online communities dealing with items of fine taste like movies, food, and beer. For online recommendation, this implies that users’ preferences and ratings are influenced due to other individuals. Given only time-stamped reviews of users, can we find out whoinfluences- whom, and characteristics of the underlying influence network? Can we use this network to improve recommendation?</p><p>While prior works in social-aware recommendation have leveraged social interaction by considering the observed social network of users, many communities like Amazon, Beeradvocate, and Ratebeer do not have explicit user-user links.Therefore,we propose GhostLink, an unsupervised probabilistic graphical model, to automatically learn the latent influence network underlying a review community – given only the temporal traces (timestamps) of users’ posts and their content. Based on extensive experiments with four real-world datasets with 13 million reviews, we show that GhostLink improves item recommendation by around 23% over state-of-the-art methods that do not consider this influence. As additional use-cases, we show that GhostLink can be used to differentiate between users’ latent preferences and influenced ones, as well as to detect influential users based on the learned influence graph.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose an unsupervised probabilistic generative model GhostLink based on Latent Dirichlet Allocation to learn a latent influence graph in online communities without requiring explicit user-user links or a social network. This is the first work that solely relies on timestamped review data.</li><li>We propose an efficient algorithm based on Gibbs sampling to estimate the hidden parameters in GhostLink that empirically demonstrates fast convergence.</li><li>We perform large-scale experiments in four communities with 13 million reviews, 0.5 mil. items, and 1 mil. users where we show improved recommendation for item rating prediction by around 23% over state-of-the-art methods. Moreover, we analyze the properties of the influence graph and use it for use-cases like finding influential members in the community.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/7.JPG" alt></p><h1><span id="5graph-neural-networks-for-social-recommendation">5.Graph Neural Networks for Social Recommendation</span></h1><h2><span id="abstract">Abstract</span></h2><p>In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key.</p><p>However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the useruser social graph and the user-item graph).</p><p>To address the three aforementioned challenges simultaneously, in this paper,we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose a novel graph neural network GraphRec, which can model graph data in social recommendations coherently;</li><li>We provide a principled approach to jointly capture interactions and opinions in the user-item graph;</li><li>We introduce a method to consider heterogeneous strengths of social relations mathematically;</li><li>We demonstrate the effectiveness of the proposed framework on various real-world datasets.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/8.JPG" alt></p><h2><span id="performance">performance</span></h2><p><img src="/www2019/9.JPG" alt></p><h1><span id="6hierarchical-temporal-convolutional-networks-for-dynamic-recommender-systems工程应用">6.Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems(工程应用)</span></h1><h2><span id="abstract">Abstract</span></h2><p>Recommender systems that can learn from cross-session data to dynamically predict the next item a user will choose are crucial for online platforms. However, existing approaches often use out-ofthe-box sequence models which are limited by speed and memory consumption, are often infeasible for production environments, and usually do not incorporate cross-session information, which is crucial for effective recommendations.</p><p>Here we propose Hierarchical Temporal Convolutional Networks (HierTCN), a hierarchical deep learning architecture that makes dynamic recommendations based on users’ sequential multi-session interactions with items. HierTCN is designed for web-scale systems with billions of items and hundreds of millions of users. It consists of two levels of models: The high-level model uses Recurrent Neural Networks (RNN) to aggregate users’ evolving long-term interests across different sessions, while the low-level model is implemented with Temporal Convolutional Networks (TCN), utilizing both the long-term interests and the short-term interactions within sessions to predic  the next interaction.</p><h2><span id="contribution">contribution</span></h2><ul><li>HierTCN has a significant performance improvement over existing deep learning models by about 30% on a public XING dataset and 18% on a private large-scale Pinterest dataset.</li><li>Compared with RNN-based approaches, HierTCN is 2.5 times faster in terms of training time and allows for much easier gradient backpropagation.</li><li>Compared with CNN-based approaches, HierTCN requires roughly 10% data memory usage and allows for easy latent feature extraction.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/10.JPG" alt></p><h1><span id="7how-intention-informed-recommendations-modulate-choices-a-field-study-of-spokenword-content">7.How Intention Informed Recommendations Modulate Choices: A Field Study of SpokenWord Content</span></h1><h2><span id="abstract">Abstract</span></h2><p>People’s content choices are ideally driven by their intentions, aspirations, and plans.</p><p>However, in reality, choices may be modulated by recommendation systems which are typically trained to promote popular items and to reinforce users’ historical behavior. As a result, the utility and user experience of content consumption can be affected implicitly and undesirably.</p><p>To study this problem, we conducteda 2 × 2 randomized controlled field experiment (105 urbancollege students) to compare the effects of intention informed recommendationswith classical intention agnostic systems. The studywas conducted in the context of spokenwordweb content (podcasts)which is often consumed through subscription sites or apps. Wemodified a commercial podcast app to include (1) a recommenderthat takes into account users’ stated intentions at onboarding, and(2) a Collaborative Filtering (CF) recommender during daily use.Our study suggests that: (1) intention-aware recommendations cansignificantly raise users’ interactions (subscriptions and listening)with channels and episodes related to intended topics by over 24%,even if such a recommender is only used during onboarding, and (2)the CF-based recommender doubles users’ explorations on episodesfrom not-subscribed channels and improves satisfaction for usersonboarded with the intention-aware recommender.</p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/11.JPG" alt></p><h2><span id="framework">framework</span></h2><p><img src="/www2019/12.JPG" alt></p><h1><span id="8how-serendipity-improves-user-satisfaction-with-recommendations-a-large-scale-user-evaluation">8.How Serendipity Improves User Satisfaction with Recommendations? A Large-Scale User Evaluation</span></h1><h2><span id="abstract">Abstract</span></h2><p>Recommendation serendipity is being increasingly recognized asbeing equally important as the other beyond-accuracy objectives(such as novelty and diversity), in eliminating the “filter bubble”phenomenon of the traditional recommender systems.</p><p>However,little work has empirically verified the effects of serendipity onincreasing user satisfaction and behavioral intention.</p><p>In this paper,we report the results of a large-scale user survey (involving over3,000 users) conducted in an industrial mobile e-commerce setting.The study has identified the significant causal relationships fromnovelty, unexpectedness, relevance, and timeliness to serendipity,and from serendipity to user satisfaction and purchase intention.Moreover, our findings reveal that user curiosity plays a moderatingrole in strengthening the relationships from novelty to serendipityand from serendipity to satisfaction. Our third contribution lies inthe comparison of several recommender algorithms, which demonstratesthe significant improvements of the serendipity-orientedalgorithm over the relevance- and novelty-oriented approaches interms of user perceptions. We finally discuss the implications ofthis experiment, which include the feasibility of developing a moreprecise metric for measuring recommendation serendipity, andthe potential benefit of a curiosity-based personalized serendipitystrategy for recommender systems.<img src="/www2019/13.JPG" alt></p><h1><span id="9improving-outfit-recommendation-with-co-supervision-of-fashion-generation-图像衣服类">9.Improving Outfit Recommendation with Co-supervision of Fashion Generation (图像：衣服类)</span></h1><h2><span id="abstract">Abstract</span></h2><p>The task of fashion recommendation includes twomain challenges:visual understanding and visual matching. Visual understandingaims to extract effective visual features. Visual matching aims tomodel a human notion of compatibility to compute a match betweenfashion items. Most previous studies rely on recommendationloss alone to guide visual understanding and matching. Althoughthe features captured by thesemethods describe basic characteristics(e.g., color, texture, shape) of the input items, they arenot directly related to the visual signals of the output items (to berecommended). This is problematic because the aesthetic characteristics(e.g., style, design), based on which we can directly inferthe output items, are lacking. Features are learned under the recommendationloss alone, where the supervision signal is simplywhether the given two items are matched or not.</p><p>To address this problem, we propose a neural co-supervisionlearning framework, called the FAshion RecommendationMachine(FARM). FARM improves visual understanding by incorporatingthe supervision of generation loss, which we hypothesize to beable to better encode aesthetic information. FARMenhances visualmatching by introducing a novel layer-to-layer matching mechanismto fuse aesthetic information more effectively, and meanwhileavoiding paying too much attention to the generation qualityand ignoring the recommendation performance.</p><p>Extensive experiments on two publicly available datasets showthat FARM outperforms state-of-the-art models on outfit recommendation,in terms of AUC and MRR. Detailed analyses of generatedand recommended items demonstrate that FARM can encodebetter features and generate high quality images as references toimprove recommendation performance.</p><h2><span id="contribution">contribution</span></h2><ul><li>We propose a neural co-supervision learning framework, FARM, for outfit recommendation that simultaneously yields recommendation and generation.</li><li>We propose a layer-to-layer matching mechanism that acts as a bridge between generation and recommendation, and improves recommendation by leveraging generation features.</li><li>Our proposed approach is shown to be effective in experiments on two large-scale datasets.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/14.JPG" alt><img src="/www2019/15.JPG" alt></p><h1><span id="10jointly-learning-explainable-rules-for-recommendation-with-knowledge-graph">10.Jointly Learning Explainable Rules for Recommendation with Knowledge Graph</span></h1><h2><span id="abstract">Abstract</span></h2><p>Explainability and effectiveness are two key aspects for building recommendersystems. Prior efforts mostly focus on incorporating sideinformation to achieve better recommendation performance.</p><p>However,these methods have some weaknesses: (1) prediction of neuralnetwork-based embedding methods are hard to explain and debug;(2) symbolic, graph-based approaches (e.g., meta path-based models)require manual efforts and domain knowledge to define patternsand rules, and ignore the item association types (e.g. substitutableand complementary).</p><p>In this paper, we propose a novel joint learningframework to integrate induction of explainable rules from knowledgegraph with construction of a rule-guided neural recommendationmodel. The framework encourages two modules to complementeach other in generating effective and explainable recommendation:</p><ol><li>inductive rules, mined from item-centric knowledge graphs,summarize common multi-hop relational patterns for inferring differentitem associations and provide human-readable explanationfor model prediction; 2) recommendation module can be augmentedby induced rules and thus have better generalization ability dealingwith the cold-start issue.</li></ol><p>Extensive experiments1 show that ourproposed method has achieved significant improvements in itemrecommendation over baselines on real-world datasets. Our modeldemonstrates robust performance over “noisy&quot; item knowledgegraphs, generated by linking item names to related entities.</p><h2><span id="contribution">contribution</span></h2><ul><li>We utilize a large-scale knowledge graph to derive rules between items from item associations.</li><li>We propose a joint optimization framework that induces rules from knowledge graphs and recommends items based on the rules at the same time.</li><li>We conduct extensive experiments on real-world datasets. Experimental results prove the effectiveness of our framework in accurate and explainable recommendation.</li></ul><h2><span id="framework">framework</span></h2><p><img src="/www2019/16.JPG" alt></p><h1><span id="11jointly-leveraging-intent-and-interaction-signals-to-predict-user-satisfaction-with-slate-recommendations">11.Jointly Leveraging Intent and Interaction Signals to Predict User Satisfaction with Slate Recommendations.</span></h1><h2><span id="abstract">Abstract</span></h2><p>Detecting and understanding implicit measures of user satisfactionare essential for enhancing recommendation quality. When usersinteract with a recommendation system, they leave behind finegrained traces of interaction signals, which contain valuable informationthat could help gauging user satisfaction. User interactionwith such systems is often motivated by a specific need or intent, oftennot explicitly specified by the user, but can nevertheless informon how the user interacts with, and the extent to which the user issatisfied by the recommendations served. In this work, we considera complex recommendation scenario, called Slate Recommendation,wherein a user is presented with an ordered set of collections, calledslates, in a specific page layout. We focus on the context of musicstreaming and leverage fine-grained user interaction signals totackle the problem of predicting user satisfaction.</p><p>We hypothesize that user interactions are conditional on thespecific intent users have when interacting with a recommendationsystem, and highlight the need for explicitly considering userintent when interpreting interaction signals. We present diverseapproaches to identify user intents (interviews, surveys and a quantitativeapproach) and identify a set of common intents users have ina music streaming recommendation setting. Additionally, we identifythe importance of shared learning across intents and propose amulti-level hierarchical model for user satisfaction prediction thatleverages user intent information alongside interaction signals. Ourfindings from extensive experiments on a large scale real world datademonstrate (i) the utility of considering different interaction signals,(ii) the role of intents in interpreting user interactions and (iii)the interplay between interaction signals and intents in predictinguser satisfaction.</p><h1><span id="12modeling-heart-rate-and-activity-data-for-personalized-fitness-recommendation健康推荐">12.Modeling Heart Rate and Activity Data for Personalized Fitness Recommendation(健康推荐)</span></h1><h2><span id="abstract">Abstract</span></h2><p>Activity logs collected from wearable devices (e.g. Apple Watch,Fitbit, etc.) are a promising source of data to facilitate a wide rangeof applications such as personalized exercise scheduling, workoutrecommendation, and heart rate anomaly detection.</p><p>However,such data are heterogeneous, noisy, diverse in scale and resolution,and have complex interdependencies, making them challenging tomodel.</p><p>In this paper, we develop context-aware sequential modelsto capture the personalized and temporal patterns of fitness data.</p><p>Specifically, we propose FitRec – an LSTM-based model that capturestwo levels of context information: context within a specificactivity, and context across a user’s activity history.</p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/17.JPG" alt></p><h2><span id="framework">framework</span></h2><p><img src="/www2019/18.JPG" alt></p><h1><span id="13modeling-item-specific-temporal-dynamics-of-repeat-consumption-for-recommender-systems">13.Modeling Item-Specific Temporal Dynamics of Repeat Consumption for Recommender Systems</span></h1><h2><span id="abstract">Abstract</span></h2><p>Repeat consumption is a common scenario in daily life, such asrepurchasing items and revisiting websites, and is a critical factorto be taken into consideration for recommender systems. Temporaldynamics play important roles in modeling repeat consumption.It is noteworthy that for items with distinct lifetimes, consumingtendency for the next one fluctuates differently with time. Forexample, users may repurchase milk weekly, but it is possible torepurchase mobile phone after a long period of time. Therefore,how to adaptively incorporate various temporal patterns of repeatconsumption into a holistic recommendation model has been a newand important problem.</p><p>In this paper, we propose a novel unified model with introducingHawkes Process into Collaborative Filtering (CF). Differentfrom most previous work which ignores various time-varying patternsof repeat consumption, the model explicitly addresses twoitem-specific temporal dynamics: (1) short-term effect and (2) lifetimeeffect, which is named as Short-Term and Life-Time RepeatConsumption (SLRC) model. SLRC learns importance of the twofactors for each item dynamically by interpretable parameters.</p><h2><span id="contribution">contribution</span></h2><p><img src="/www2019/19.JPG" alt></p><h1><span id="14multi-task-feature-learning-for-knowledge-graph-enhanced-recommendationcross-domain-recommendation">14.Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation(cross-domain recommendation)</span></h1><h2><span id="abstract">Abstract</span></h2><p>Collaborative filtering often suffers from sparsity and cold startproblems in real recommendation scenarios, therefore, researchersand engineers usually use side information to address the issuesand improve the performance of recommender systems.</p><p>In thispaper, we consider knowledge graphs as the source of side information.</p><p>We propose MKR, a Multi-task feature learning approachfor Knowledge graph enhanced Recommendation. MKR is a deepend-to-end framework that utilizes knowledge graph embeddingtask to assist recommendation task.</p><h2><span id="contribution">contribution</span></h2><p>We propose MKR, a Multi-task feature learning approachfor Knowledge graph enhanced Recommendation. MKR is a deepend-to-end framework that utilizes knowledge graph embeddingtask to assist recommendation task. knowledge graph embedding, as shown in theoretical analysis and experiment results.</p><h2><span id="framework">framework</span></h2><p><img src="/www2019/20.JPG" alt></p><h1><span id="15multimodal-review-generation-for-recommender-systems">15.Multimodal Review Generation for Recommender Systems</span></h1><h2><span id="abstract">Abstract</span></h2><p>Key to recommender systems is learning user preferences, whichare expressed through various modalities. In online reviews, forinstance, this manifests in numerical rating, textual content, as wellas visual images. In this work, we hypothesize that modelling thesemodalities jointly would result in a more holistic representation ofa review towards more accurate recommendations. Therefore, wepropose Multimodal Review Generation (MRG), a neural approachthat simultaneously models a rating prediction component and areview text generation component. We hypothesize that the shareduser and item representations would augment the rating predictionwith richer information from review text, while sensitizingthe generated review text to sentiment features based on user anditem of interest. Moreover, when review photos are available, visualfeatures could inform the review text generation further. Comprehensiveexperiments on real-life datasets from several major UScities show that the proposed model outperforms comparable multimodalbaselines, while an ablation analysis establishes the relativecontributions of the respective components of the joint model.</p><h2><span id="contribution">contribution</span></h2><p>We design the MRG model (see Section 3), whichjointly models rating prediction and text generation at the reviewlevel by incorporating LSTM cells with a novel fusion gate as akind of soft attention to weigh the relative contributions of sentimentfeatures and visual features that provide context to the textgeneration. We also describe the learning and inference algorithmsrespectively.</p><h2><span id="framework">framework</span></h2><p><img src="/www2019/21.JPG" alt></p><h1><span id="16personalized-bundle-list-recommendation">16.Personalized Bundle List Recommendation</span></h1><h2><span id="abstract">Abstract</span></h2><p>Product bundling, offering a combination of items to customers,is one of the marketing strategies commonly used in online ecommerceand offline retailers. A high-quality bundle generalizesfrequent items of interest, and diversity across bundles boosts theuser-experience and eventually increases transaction volume.</p><p>Inthis paper, we formalize the personalized bundle list recommendationas a structured prediction problem and propose a bundlegeneration network (BGN), which decomposes the problem intoquality/diversity parts by the determinantal point processes (DPPs).BGN uses a typical encoder-decoder framework with a proposedfeature-aware softmax to alleviate the inadequate representationof traditional softmax, and integrates the masked beam search andDPP selection to produce high-quality and diversified bundle listwith an appropriate bundle size.<img src="/www2019/22.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;1cross-domain-recommendation-without-sharing-user-relevant-data&quot;&gt;1.Cross-domain Recommendation Without Sharing User-relevant D
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
  </entry>
  
  <entry>
    <title>tensorflow学习率衰减</title>
    <link href="http://dinry.github.io/tensorflow%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/"/>
    <id>http://dinry.github.io/tensorflow学习率衰减/</id>
    <published>2019-08-16T03:01:00.000Z</published>
    <updated>2019-08-16T06:31:43.560Z</updated>
    
    <content type="html"><![CDATA[<p>在神经网络的训练过程中，学习率(learning rate)控制着参数的更新速度，tf.train类下面的五种不同的学习速率的衰减方法。</p><ul><li>tf.train.exponential_decay</li><li>tf.train.inverse_time_decay</li><li>tf.train.natural_exp_decay</li><li>tf.train.piecewise_constant</li><li>tf.train.polynomial_decay</li></ul><ol><li>首先使用较大学习率(目的：为快速得到一个比较优的解);</li><li>然后通过迭代逐步减小学习率(目的：为使模型在训练后期更加稳定);</li></ol><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.train.exponential_decay(</span><br><span class="line">    learning_rate,初始学习率</span><br><span class="line">    global_step,当前迭代次数</span><br><span class="line">    decay_steps,衰减速度（在迭代到该次数时学习率衰减为earning_rate * decay_rate）</span><br><span class="line">    decay_rate,学习率衰减系数，通常介于<span class="number">0</span><span class="number">-1</span>之间。</span><br><span class="line">    staircase=False,(默认值为False,当为True时，（global_step/decay_steps）则被转化为整数) ,选择不同的衰减方式。</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在神经网络的训练过程中，学习率(learning rate)控制着参数的更新速度，tf.train类下面的五种不同的学习速率的衰减方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf.train.exponential_decay&lt;/li&gt;
&lt;li&gt;tf.train.inverse_ti
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day24</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/"/>
    <id>http://dinry.github.io/西瓜书day24/</id>
    <published>2019-08-03T14:10:12.000Z</published>
    <updated>2019-08-03T14:27:51.510Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="主成分分析pca">主成分分析（PCA）</span></h1><p>PCA是一种最常用的降维方法</p><ul><li>最近重构性：样本点到这个超平面的距离都足够近</li><li>最大可分性：样本点在这个超平面上的投影能尽可能分开</li></ul><h4><span id="最近重构性">最近重构性</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/1.JPG" alt></p><h4><span id="最大可分性">最大可分性</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/2.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/3.JPG" alt></p><p>应用拉格朗日乘子法</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day24/4.JPG" alt></p><h4><span id="结果">结果</span></h4><p>降维导致 d-d'个特征值的特征向量被舍弃了，舍弃这部分信息能使样本的采样密度增大，另外，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将他们舍弃能在一定程度上起到去噪的作用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;主成分分析pca&quot;&gt;主成分分析（PCA）&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;PCA是一种最常用的降维方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近重构性：样本点到这个超平面的距离都足够近&lt;/li&gt;
&lt;li&gt;最大可分性：样本点在这个超平面上的投影能尽可能分开&lt;/li&gt;
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day21</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day21/"/>
    <id>http://dinry.github.io/西瓜书day21/</id>
    <published>2019-08-03T14:09:55.000Z</published>
    <updated>2019-08-03T14:09:55.073Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tf.multinomial</title>
    <link href="http://dinry.github.io/tf-multinomial/"/>
    <id>http://dinry.github.io/tf-multinomial/</id>
    <published>2019-08-03T13:31:58.000Z</published>
    <updated>2019-08-03T13:56:44.689Z</updated>
    
    <content type="html"><![CDATA[<p>明明按概率，亲测却非常随机</p><p>tf.multinomial(logits, num_samples, seed=None, name=None)</p><p>从multinomial分布中采样，样本个数是num_samples，每个样本被采样的概率由logits给出</p><h4><span id="parametrs">parametrs:</span></h4><ul><li><p>logits: 2-D Tensor with shape [batch_size, num_classes]. Each slice [i, :] represents the unnormalized log probabilities for all classes.2维量，shape是 [batch_size, num_classes]，每一行都是关于种类的未归一化的对数概率</p></li><li><p>num_samples: 0-D. Number of independent samples to draw for each row slice.标量，表示采样的个数，更重要的是，它限制了返回张量中元素的范围{：0，1，2，…，num_samples-1 }</p></li></ul><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">samples = tf.multinomial(tf.log([[<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>]]), <span class="number">5</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run(samples)</span><br><span class="line"></span><br><span class="line"># 运行结果：array([[2, 1, 2, 2, 0]])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;明明按概率，亲测却非常随机&lt;/p&gt;
&lt;p&gt;tf.multinomial(logits, num_samples, seed=None, name=None)&lt;/p&gt;
&lt;p&gt;从multinomial分布中采样，样本个数是num_samples，每个样本被采样的概率由logit
      
    
    </summary>
    
      <category term="deep learning" scheme="http://dinry.github.io/categories/deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow实现梯度下降各种方法</title>
    <link href="http://dinry.github.io/tensorflow%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%90%84%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/tensorflow实现梯度下降各种方法/</id>
    <published>2019-08-03T03:14:07.000Z</published>
    <updated>2019-08-03T04:54:33.110Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="不使用tensorflow任何梯度下降方法">不使用tensorflow任何梯度下降方法</span></h1><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf8 -*-</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"># Import MNIST data</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line">b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line">new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h1><span id="使用tfgradients实现梯度下降">使用tf.gradients实现梯度下降</span></h1><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 使用随机梯度下降</span><br><span class="line">vars=tf.trainable_variables()</span><br><span class="line">vars_grad=tf.gradients(loss_op,vars)</span><br><span class="line">vars_new=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(vars)):</span><br><span class="line">    vars_new.append(vars[i].assign(vars[i]-learning_rate*vars_grad[i])) # 权重更新</span><br><span class="line">sess.run(vars_new, feed_dict=&#123;<span class="attr">X</span>: batch_x, <span class="attr">Y</span>: batch_y, <span class="attr">keep_prob</span>: <span class="number">0.8</span>&#125;)</span><br></pre></td></tr></table></figure></p><p>minist:<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line">W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line"></span><br><span class="line">new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line">new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h1><span id="使用tensorflow内置优化器">使用tensorflow内置优化器</span></h1><h4><span id="minimize">minimize</span></h4><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line"># W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line">#</span><br><span class="line"># new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line"># new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line">train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            # _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h4><span id="compute_gradients与apply_gradients">compute_gradients与apply_gradients</span></h4><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'D'</span>):</span><br><span class="line">    # Set model weights</span><br><span class="line">    W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">    # Construct model</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line"># W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line">#</span><br><span class="line"># new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line"># new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"># train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"># optimizer=tf.train.AdamOptimizer()</span><br><span class="line"># gradients=optimizer.compute_gradients(cost)</span><br><span class="line"># clipped_gradients = [(tf.clip_by_value(_[0], -1, 1), _[1]) for _ in gradients] # _[0] 对应dw ,_[1]对应db</span><br><span class="line"># train_op = optimizer.apply_gradients(clipped_gradients)</span><br><span class="line"># 或</span><br><span class="line"># train_op = optimizer.apply_gradients(gradients)</span><br><span class="line"></span><br><span class="line">tvars = tf.trainable_variables()</span><br><span class="line">d_params = [v <span class="keyword">for</span> v <span class="keyword">in</span> tvars <span class="keyword">if</span> v.name.startswith(<span class="string">'D/'</span>)]</span><br><span class="line">trainerD = tf.train.AdamOptimizer(learning_rate=<span class="number">0.0002</span>, beta1=<span class="number">0.5</span>)</span><br><span class="line">d_grads = trainerD.compute_gradients(cost, d_params)#Only update the weights for the discriminator network.</span><br><span class="line">train_op = trainerD.apply_gradients(d_grads)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            # _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;不使用tensorflow任何梯度下降方法&quot;&gt;不使用tensorflow任何梯度下降方法&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;s
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
</feed>
