<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dinry</title>
  
  <subtitle>notebook</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://dinry.github.io/"/>
  <updated>2019-08-03T04:54:33.110Z</updated>
  <id>http://dinry.github.io/</id>
  
  <author>
    <name>dinry</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>tensorflow实现梯度下降各种方法</title>
    <link href="http://dinry.github.io/tensorflow%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%90%84%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/tensorflow实现梯度下降各种方法/</id>
    <published>2019-08-03T03:14:07.000Z</published>
    <updated>2019-08-03T04:54:33.110Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="不使用tensorflow任何梯度下降方法">不使用tensorflow任何梯度下降方法</span></h1><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf8 -*-</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"># Import MNIST data</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line">b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line">new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h1><span id="使用tfgradients实现梯度下降">使用tf.gradients实现梯度下降</span></h1><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 使用随机梯度下降</span><br><span class="line">vars=tf.trainable_variables()</span><br><span class="line">vars_grad=tf.gradients(loss_op,vars)</span><br><span class="line">vars_new=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(vars)):</span><br><span class="line">    vars_new.append(vars[i].assign(vars[i]-learning_rate*vars_grad[i])) # 权重更新</span><br><span class="line">sess.run(vars_new, feed_dict=&#123;<span class="attr">X</span>: batch_x, <span class="attr">Y</span>: batch_y, <span class="attr">keep_prob</span>: <span class="number">0.8</span>&#125;)</span><br></pre></td></tr></table></figure></p><p>minist:<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line">W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line"></span><br><span class="line">new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line">new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h1><span id="使用tensorflow内置优化器">使用tensorflow内置优化器</span></h1><h4><span id="minimize">minimize</span></h4><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line"># W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line">#</span><br><span class="line"># new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line"># new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line">train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            # _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p><h4><span id="compute_gradients与apply_gradients">compute_gradients与apply_gradients</span></h4><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./MNIST_data"</span>, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'D'</span>):</span><br><span class="line">    # Set model weights</span><br><span class="line">    W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">    b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">    # Construct model</span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W)+b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"># W_grad =  - tf.matmul ( tf.transpose(x) , y - pred)</span><br><span class="line"># b_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)</span><br><span class="line"># W_grad, b_grad=tf.gradients(cost,[W,b])</span><br><span class="line">#</span><br><span class="line"># new_W = W.assign(W - learning_rate * W_grad)</span><br><span class="line"># new_b = b.assign(b - learning_rate * b_grad)</span><br><span class="line"># train_op=tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"># optimizer=tf.train.AdamOptimizer()</span><br><span class="line"># gradients=optimizer.compute_gradients(cost)</span><br><span class="line"># clipped_gradients = [(tf.clip_by_value(_[0], -1, 1), _[1]) for _ in gradients] # _[0] 对应dw ,_[1]对应db</span><br><span class="line"># train_op = optimizer.apply_gradients(clipped_gradients)</span><br><span class="line"># 或</span><br><span class="line"># train_op = optimizer.apply_gradients(gradients)</span><br><span class="line"></span><br><span class="line">tvars = tf.trainable_variables()</span><br><span class="line">d_params = [v <span class="keyword">for</span> v <span class="keyword">in</span> tvars <span class="keyword">if</span> v.name.startswith(<span class="string">'D/'</span>)]</span><br><span class="line">trainerD = tf.train.AdamOptimizer(learning_rate=<span class="number">0.0002</span>, beta1=<span class="number">0.5</span>)</span><br><span class="line">d_grads = trainerD.compute_gradients(cost, d_params)#Only update the weights for the discriminator network.</span><br><span class="line">train_op = trainerD.apply_gradients(d_grads)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Fit training using batch data</span><br><span class="line">            # _, _, c = sess.run([new_W, new_b, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            _,c=sess.run([train_op,cost],feed_dict=&#123;<span class="attr">x</span>: batch_xs, <span class="attr">y</span>: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            # Compute average loss</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    # test</span><br><span class="line">    acc=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>)),tf.float32))</span><br><span class="line">    print(<span class="string">'test acc'</span>,acc.eval(&#123;<span class="attr">x</span>: mnist.test.images, <span class="attr">y</span>: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;不使用tensorflow任何梯度下降方法&quot;&gt;不使用tensorflow任何梯度下降方法&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;figure class=&quot;highlight js&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;s
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow中的参数初始化方法</title>
    <link href="http://dinry.github.io/tensorflow%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>http://dinry.github.io/tensorflow中的参数初始化方法/</id>
    <published>2019-08-03T02:10:57.000Z</published>
    <updated>2019-08-03T03:08:41.345Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="常量初始化">常量初始化</span></h1><p>tf中使用tf.constant_initializer(value)类生成一个初始值为常量value的tensor对象。constant_initializer类的构造函数定义：<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, value=<span class="number">0</span>, dtype=dtypes.float32, verify_shape=False):</span><br><span class="line">    self.value = value</span><br><span class="line">    self.dtype = dtypes.as_dtype(dtype)</span><br><span class="line">    self._verify_shape = verify_shape</span><br></pre></td></tr></table></figure></p><ul><li>value：指定的常量</li><li>dtype： 数据类型</li><li>verify_shape： 是否可以调整tensor的形状，默认可以调整</li></ul><p>example:<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">value = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">init = tf.constant_initializer(value)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">8</span>], initializer=init)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">#output:</span><br><span class="line">#[ 0.  1.  2.  3.  4.  5.  6.  7.]</span><br></pre></td></tr></table></figure></p><p>神经网络中经常使用常量初始化方法来初始化偏置值</p><p>当初始化一个维数很多的常量时，一个一个指定每个维数上的值很不方便，tf提供了 tf.zeros_initializer() 和 tf.ones_initializer() 类，分别用来初始化全0和全1的tensor对象。</p><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_zeros=tf.zeros_initializer()</span><br><span class="line">init_ones = tf.ones_initializer</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">8</span>], initializer=init_zeros)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">8</span>], initializer=init_ones)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#output:</span><br><span class="line"># [ 0.  0.  0.  0.  0.  0.  0.  0.]</span><br><span class="line"># [ 1.  1.  1.  1.  1.  1.  1.  1.]</span><br></pre></td></tr></table></figure></p><h1><span id="初始化为正态分布">初始化为正态分布</span></h1><p>初始化参数为正太分布在神经网络中应用的最多，可以初始化为标准正太分布和截断正太分布。</p><p>tf中使用 tf.random_normal_initializer() 类来生成一组符合标准正太分布的tensor。</p><p>tf中使用 tf.truncated_normal_initializer() 类来生成一组符合截断正太分布的tensor。</p><p>tf.random_normal_initializer 类和 tf.truncated_normal_initializer 的构造函数定义：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=None, dtype=dtypes.float32):</span><br><span class="line">    self.mean = mean</span><br><span class="line">    self.stddev = stddev</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = _assert_float_dtype(dtypes.as_dtype(dtype))</span><br></pre></td></tr></table></figure></p><ul><li>mean： 正太分布的均值，默认值0</li><li>stddev： 正太分布的标准差，默认值1</li><li>seed： 随机数种子，指定seed的值可以每次都生成同样的数据</li><li>dtype： 数据类型</li></ul><p>example:<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_random = tf.random_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line">init_truncated = tf.truncated_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_random)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_truncated)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line"></span><br><span class="line">#output:</span><br><span class="line"># [-0.40236568 -0.35864913 -0.94253045 -0.40153521  0.1552504   1.16989613</span><br><span class="line">#   0.43091929 -0.31410623  0.70080078 -0.9620409 ]</span><br><span class="line"># [ 0.18356581 -0.06860946 -0.55245203  1.08850253 -1.13627422 -0.1006074</span><br><span class="line">#   0.65564936  0.03948414  0.86558545 -0.4964745 ]</span><br></pre></td></tr></table></figure></p><h1><span id="初始化为均匀分布">初始化为均匀分布</span></h1><p>tf中使用 tf.random_uniform_initializer 类来生成一组符合均匀分布的tensor。</p><p>tf.random_uniform_initializer类构造函数定义：<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, minval=<span class="number">0</span>, maxval=None, seed=None, dtype=dtypes.float32):</span><br><span class="line">    self.minval = minval</span><br><span class="line">    self.maxval = maxval</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = dtypes.as_dtype(dtype)</span><br></pre></td></tr></table></figure></p><ul><li>minval: 最小值</li><li>maxval： 最大值</li><li>seed：随机数种子</li><li>dtype： 数据类型</li></ul><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_uniform = tf.random_uniform_initializer(minval=<span class="number">0</span>, maxval=<span class="number">10</span>, seed=None, dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_uniform)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [ 6.93343639  9.41196823  5.54009819  1.38017178  1.78720832  5.38881063</span><br><span class="line">#   3.39674473  8.12443542  0.62157512  8.36026382]</span><br></pre></td></tr></table></figure></p><p>从输出可以看到，均匀分布生成的随机数并不是从小到大或者从大到小均匀分布的，这里均匀分布的意义是每次从一组服从均匀分布的数里边随机抽取一个数。</p><p>tf中另一个生成均匀分布的类是 tf.uniform_unit_scaling_initializer()，构造函数是：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, factor=<span class="number">1.0</span>, seed=None, dtype=dtypes.float32):</span><br><span class="line">    self.factor = factor</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = _assert_float_dtype(dtypes.as_dtype(dtype))</span><br></pre></td></tr></table></figure></p><p>同样都是生成均匀分布，tf.uniform_unit_scaling_initializer 跟 tf.random_uniform_initializer 不同的地方是前者不需要指定最大最小值，是通过公式计算出来的：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max_val = math.sqrt(<span class="number">3</span> / input_size) * factor</span><br><span class="line">min_val = -max_val</span><br></pre></td></tr></table></figure></p><p>input_size是生成数据的维度，factor是系数。<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_uniform_unit = tf.uniform_unit_scaling_initializer(factor=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_uniform_unit)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [-1.65964031  0.59797513 -0.97036457 -0.68957627  1.69274557  1.2614969</span><br><span class="line">#   1.55491126  0.12639415  0.54466736 -1.56159735]</span><br></pre></td></tr></table></figure></p><h1><span id="初始化为变尺度正太-均匀分布">初始化为变尺度正太、均匀分布</span></h1><p>tf中tf.variance_scaling_initializer()类可以生成截断正太分布和均匀分布的tensor，增加了更多的控制参数。构造函数：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, scale=<span class="number">1.0</span>,</span><br><span class="line">               mode=<span class="string">"fan_in"</span>,</span><br><span class="line">               distribution=<span class="string">"normal"</span>,</span><br><span class="line">               seed=None,</span><br><span class="line">               dtype=dtypes.float32):</span><br><span class="line">    <span class="keyword">if</span> scale &lt;= <span class="number">0.</span>:</span><br><span class="line">      raise ValueError(<span class="string">"`scale` must be positive float."</span>)</span><br><span class="line">    <span class="keyword">if</span> mode not <span class="keyword">in</span> &#123;<span class="string">"fan_in"</span>, <span class="string">"fan_out"</span>, <span class="string">"fan_avg"</span>&#125;:</span><br><span class="line">      raise ValueError(<span class="string">"Invalid `mode` argument:"</span>, mode)</span><br><span class="line">    distribution = distribution.lower()</span><br><span class="line">    <span class="keyword">if</span> distribution not <span class="keyword">in</span> &#123;<span class="string">"normal"</span>, <span class="string">"uniform"</span>&#125;:</span><br><span class="line">      raise ValueError(<span class="string">"Invalid `distribution` argument:"</span>, distribution)</span><br><span class="line">    self.scale = scale</span><br><span class="line">    self.mode = mode</span><br><span class="line">    self.distribution = distribution</span><br><span class="line">    self.seed = seed</span><br><span class="line">    self.dtype = _assert_float_dtype(dtypes.as_dtype(dtype))</span><br></pre></td></tr></table></figure></p><ul><li>scale: 缩放尺度</li><li>mode： 有3个值可选，分别是 “fan_in”, “fan_out” 和 “fan_avg”，用于控制计算标准差 stddev的值</li><li>distribution： 2个值可选，”normal”或“uniform”，定义生成的tensor的分布是截断正太分布还是均匀分布</li></ul><p>distribution选‘normal’的时候，生成的是截断正太分布，标准差 stddev = sqrt(scale / n), n的取值根据mode的不同设置而不同：</p><ul><li>mode = &quot;fan_in&quot;， n为输入单元的结点数；</li><li>mode = &quot;fan_out&quot;，n为输出单元的结点数；</li><li>mode = &quot;fan_avg&quot;,n为输入和输出单元结点数的平均值;</li></ul><p>distribution选 ‘uniform’，生成均匀分布的随机数tensor，最大值 max_value和 最小值 min_value 的计算公式：</p><p>max_value = sqrt(3 * scale / n)</p><p>min_value = -max_value</p><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_variance_scaling_normal = tf.variance_scaling_initializer(scale=<span class="number">1.0</span>,mode=<span class="string">"fan_in"</span>,</span><br><span class="line">                                                        distribution=<span class="string">"normal"</span>,seed=None,dtype=tf.float32)</span><br><span class="line">init_variance_scaling_uniform = tf.variance_scaling_initializer(scale=<span class="number">1.0</span>,mode=<span class="string">"fan_in"</span>,</span><br><span class="line">                                                        distribution=<span class="string">"uniform"</span>,seed=None,dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_variance_scaling_normal)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_variance_scaling_uniform)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [ 0.55602223  0.36556259  0.39404872 -0.11241052  0.42891756 -0.22287074</span><br><span class="line">#   0.15629818  0.56271428 -0.15364751 -0.03651841]</span><br><span class="line"># [ 0.22965753 -0.1339919  -0.21013224  0.112804   -0.49030468  0.21375734</span><br><span class="line">#   0.24524075 -0.48397955  0.02254289 -0.07996771]</span><br></pre></td></tr></table></figure></p><h1><span id="其他初始化方式">其他初始化方式</span></h1><ul><li>tf.orthogonal_initializer() 初始化为正交矩阵的随机数，形状最少需要是二维的</li><li>tf.glorot_uniform_initializer() 初始化为与输入输出节点数相关的均匀分布随机数</li><li>tf.glorot_normal_initializer（） 初始化为与输入输出节点数相关的截断正太分布随机数</li></ul><p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_orthogonal = tf.orthogonal_initializer(gain=<span class="number">1.0</span>, seed=None, dtype=tf.float32)</span><br><span class="line">init_glorot_uniform = tf.glorot_uniform_initializer()</span><br><span class="line">init_glorot_normal = tf.glorot_normal_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">4</span>,<span class="number">4</span>], initializer=init_orthogonal)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_glorot_uniform)</span><br><span class="line">  z = tf.get_variable(<span class="string">'z'</span>, shape=[<span class="number">10</span>], initializer=init_glorot_normal)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line">  z.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br><span class="line">  print(z.eval())</span><br><span class="line"></span><br><span class="line"># output:</span><br><span class="line"># [[ 0.41819954  0.38149482  0.82090431  0.07541249]</span><br><span class="line">#  [ 0.41401231  0.21400851 -0.38360971  0.79726893]</span><br><span class="line">#  [ 0.73776144 -0.62585676 -0.06246936 -0.24517137]</span><br><span class="line">#  [ 0.33077344  0.64572859 -0.41839844 -0.54641217]]</span><br><span class="line"># [-0.11182356  0.01995623 -0.0083192  -0.09200105  0.49967837  0.17083591</span><br><span class="line">#   0.37086374  0.09727859  0.51015782 -0.43838671]</span><br><span class="line"># [-0.50223351  0.18181904  0.43594137  0.3390047   0.61405027  0.02597036</span><br><span class="line">#   0.31719241  0.04096413  0.10962497 -0.13165198]</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;常量初始化&quot;&gt;常量初始化&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;tf中使用tf.constant_initializer(value)类生成一个初始值为常量value的tensor对象。
constant_initializer类的构造函数定义：
&lt;figur
      
    
    </summary>
    
      <category term="deep learning" scheme="http://dinry.github.io/categories/deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>神经网络训练问题排查</title>
    <link href="http://dinry.github.io/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    <id>http://dinry.github.io/神经网络训练问题排查/</id>
    <published>2019-08-01T09:01:14.000Z</published>
    <updated>2019-08-01T11:06:18.611Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="数据标准化">数据标准化</span></h1><p>数据的分布情况如何？数据是否经过适当的缩放？总体上的规则是：</p><ul><li>如果数据是连续值：范围应当在-1到1、0到1，或者呈平均值为0、标准差为1的正态分布。实际的范围不用如此精确，但确保输入数据大致处于上述区间内会有助于训练。缩小过大的输入，放大过小的输入。</li><li>如果数据是离散的类别（以及对于分类问题的输出而言），则通常使用one-hot表示。也就是说，如果有三种类别，那么这三种不同类别的数据将分别以[1,0,0]、[0,1,0]、[0,0,1]的方式表示。</li></ul><p>请注意：训练数据和测试数据的标准化方法必须完全相同，这非常重要。</p><h1><span id="权重初始化">权重初始化</span></h1><p>您需要确保权重不会过大，也不会过小。Xavier权重初始化方法通常是比较好的选择。对于使用修正线性（relu）或带泄露的修正线性（leaky relu）激活函数的网络而言，RELU权重初始化方法比较合适。</p><h1><span id="epoch数量和迭代次数">Epoch数量和迭代次数</span></h1><p>一个epoch周期的定义是完整地遍历数据集一次。DL4J将迭代次数定义为每个微批次中的参数更新次数。</p><p>在训练中，一般应让训练持续多个epoch，而将迭代次数设为一次（.iterations(1)选项）；一般仅在对非常小的数据集进行完整批次的训练时才会采用大于1的迭代次数。</p><p>如果epoch数量太少，网络就没有足够的时间学会合适的参数；epoch数量太多则有可能导致网络对训练数据过拟合。选择epoch数量的方式之一是早停法。早停法还可以避免神经网络发生过拟合（即可以帮助网络更好地适应未曾见过的数据）。</p><h1><span id="学习速率">学习速率</span></h1><p>学习速率是最重要的超参数之一。如果学习速率过高或过低，网络可能学习效果非常差、学习速度非常慢，甚至完全没有进展。学习速率的取值范围一般在0.1到1e-6之间，最理想的速率通常取决于具体的数据（以及网络架构）。一种简单的建议是，一开始可以尝试三种不同的学习速率：1e-1、1e-3、1e-6，先大致了解一下应该设为怎样的值，然后再进一步微调。理想状态下，可以同时以不同的学习速率运行模型，以便节省时间。</p><p>选择合适的学习速率的常用方法是借助DL4J的可视化界面来将训练进程可视化。您需要关注损失随时间变化的情况以及更新值与参数的绝对值之比（通常可以先考虑1:1000左右的比例）。</p><h1><span id="策略与学习速率计划">策略与学习速率计划</span></h1><p>您可以选择为神经网络设定学习速率策略，让学习速率随着时间推移逐渐“放缓”，帮助网络收敛至更接近局部极小值的位置，进而取得更好的学习效果。一种常用的策略是学习速率计划（learning rate schedule）。</p><h1><span id="损失函数">损失函数</span></h1><p>神经网络不同层中的损失函数的作用包括预训练、学习改善权重，以及在分类问题中得出结果（位于输出层上）。（在上述例子中，分类发生在重写段中。）</p><p>网络目的决定了所用的损失函数类型。预训练可选择重构熵函数。分类可选择多类叉熵函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;数据标准化&quot;&gt;数据标准化&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;数据的分布情况如何？数据是否经过适当的缩放？总体上的规则是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果数据是连续值：范围应当在-1到1、0到1，或者呈平均值为0、标准差为1的正态分布。实际的范围不用如此精确
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="Deep learning" scheme="http://dinry.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习调参技巧</title>
    <link href="http://dinry.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7/"/>
    <id>http://dinry.github.io/深度学习调参技巧/</id>
    <published>2019-08-01T04:48:50.000Z</published>
    <updated>2019-08-01T09:02:44.427Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="初始化">初始化</span></h1><p>一次惨痛的教训是用normal初始化cnn的参数，最后acc只能到70%多，仅仅改成xavier，acc可以到98%。还有一次给word embedding初始化，最开始使用了TensorFlow中默认的initializer（即glorot_uniform_initializer，也就是大家经常说的无脑使用xavier），训练速度慢不说，结果也不好。改为uniform，训练速度飙升，结果也飙升。所以，初始化就跟黑科技一样，用对了超参都不用调；没用对，跑出来的结果就跟模型有bug一样不忍直视。</p><p>记得刚开始研究深度学习时，做过两个小例子。一个是用tensorflow构建了一个十分简单的只有一个输入层和一个softmax输出层的Mnist手写识别网络，第一次我对权重矩阵W和偏置b采用的是正态分布初始化，一共迭代了20个epoch，当迭代完第一个epoch时，预测的准确度只有10%左右（和随机猜一样，Mnist是一个十分类问题），当迭代完二十个epoch，精度也仅仅达到了60%的样子。然后我仅仅是将权重矩阵W初始化方法改成了全为0的初始化，其他的参数均保持不变，结果在训练完第一个epoch后预测精度就达到了85%以上，最终20个epoch后精度达到92%。另一个例子是回归问题的预测，当时采用的SGD优化器，一开始学习率设定的0.1，模型可以正常训练，只是训练速度有些慢，我试着将学习率调整到0.3，希望可以加速训练速度，结果没迭代几轮loss就变成Nan了。于是从那时起我就深刻的感受到参数调节在深度学习模型训练中的重要意义。</p><p>其实上述问题产生的原因也很好理解，对于参数初始化，因为我们学习的本来就是权重W与偏置b，如果初始化足够好，直接就初始化到最优解，那都不用进行训练了。良好的初始化，可以让参数更接近最优解，这可以大大提高收敛速度，也可以防止落入局部极小。</p><h4><span id="tensorflow常用的初始化方法">tensorflow常用的初始化方法</span></h4><h1><span id="激活函数选择">激活函数选择：</span></h1><p>常用的激活函数有relu、leaky-relu、sigmoid、tanh等。对于输出层，多分类任务选用softmax输出，二分类任务选用sigmoid输出，回归任务选用线性输出。而对于中间隐层，则优先选择relu激活函数（relu激活函数可以有效的解决sigmoid和tanh出现的梯度弥散问题，多次实验表明它会比其他激活函数以更快的速度收敛）。另外，构建序列神经网络（RNN）时要优先选用tanh激活函数。</p><h1><span id="学习率设定">学习率设定：</span></h1><p>一般学习率从0.1或0.01开始尝试。学习率设置太大会导致训练十分不稳定，甚至出现Nan，设置太小会导致损失下降太慢。学习率一般要随着训练进行衰减。衰减系数设0.1，0.3，0.5均可，衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后自动进行衰减。</p><h1><span id="防止过拟合">防止过拟合：</span></h1><p>一般常用的防止过拟合方法有使用L1正则项、L2正则项、dropout、提前终止、数据集扩充等。如果模型在训练集上表现比较好但在测试集上表现欠佳可以选择增大L1或L2正则的惩罚力度（L2正则经验上首选1.0，超过10很少见），或增大dropout的随机失活概率（经验首选0.5）；或者当随着训练的持续在测试集上不增反降时，使用提前终止训练的方法。当然最有效的还是增大训练集的规模，实在难以获得新数据也可以使用数据集增强的方法，比如CV任务可以对数据集进行裁剪、翻转、平移等方法进行数据集增强，这种方法往往都会提高最后模型的测试精度。</p><h1><span id="优化器选择">优化器选择：</span></h1><p>如果数据是稀疏的，就用自适应方法，即 Adagrad, Adadelta, RMSprop, Adam。整体来讲，Adam 是最好的选择。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。</p><h1><span id="残差块与bn层">残差块与BN层</span></h1><p>如果你希望训练一个更深更复杂的网络，那么残差块绝对是一个重要的组件，它可以让你的网络训练的更深。</p><p>BN层具有加速训练速度，有效防止梯度消失与梯度爆炸，具有防止过拟合的效果，所以构建网络时最好要加上这个组件。</p><h1><span id="自动调参方法">自动调参方法：</span></h1><ul><li>Grid Search：网格搜索，在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果。其原理就像是在数组里找最大值。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。</li><li>Random Search：经验上，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。另外Random Search往往会和由粗到细的调参策略结合使用，即在效果比较好的参数附近进行更加精细的搜索。</li><li>Bayesian Optimization：贝叶斯优化，考虑到了不同参数对应的 实验结果值，因此更节省时间，贝叶斯调参比Grid Search迭代次数少， 速度快；而且其针对非凸问题依然稳健。</li></ul><h1><span id="深度学习debug的流程策略">深度学习debug的流程策略</span></h1><p>既然消除模型中的错误很难，我们不如先从简单模型入手，然后逐渐增加模型的复杂度。</p><ul><li>从最简单模型入手；</li><li>成功搭建模型，重现结果；</li><li>分解偏差各项，逐步拟合数据；</li><li>用由粗到细随机搜索优化超参数；</li><li>如果欠拟合，就增大模型；如果过拟合，就添加数据或调整。</li></ul><p><img src="https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;初始化&quot;&gt;初始化&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;一次惨痛的教训是用normal初始化cnn的参数，最后acc只能到70%多，仅仅改成xavier，acc可以到98%。还有一次给word embedding初始化，最开始使用了TensorFlow中默认的
      
    
    </summary>
    
      <category term="Deep learning" scheme="http://dinry.github.io/categories/Deep-learning/"/>
    
    
      <category term="Deep learning" scheme="http://dinry.github.io/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>python-excel</title>
    <link href="http://dinry.github.io/python-excel/"/>
    <id>http://dinry.github.io/python-excel/</id>
    <published>2019-07-25T11:57:58.000Z</published>
    <updated>2019-07-25T12:42:39.803Z</updated>
    
    <content type="html"><![CDATA[<p>python存excel数据</p><pre><code class="language-js">import xlwtimport pdbworkbook=xlwt.Workbook(encoding='utf-8')booksheet=workbook.add_sheet('data', cell_overwrite_ok=True)DATA=(('学号','姓名','年龄','性别','成绩'),      ('1001','A','11','男','12'),      ('1002','B','12','女','22'),      ('1003','C','13','女','32'),      ('1004','D','14','男','52'),      )pdb.set_trace()for i,row in enumerate(DATA):    for j,col in enumerate(row):        booksheet.write(i,j,col)workbook.save('grade.xls')</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;python存excel数据&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;import xlwt
import pdb
workbook=xlwt.Workbook(encoding=&#39;utf-8&#39;)
booksheet=workbook.add_
      
    
    </summary>
    
      <category term="python" scheme="http://dinry.github.io/categories/python/"/>
    
    
      <category term="python语法" scheme="http://dinry.github.io/tags/python%E8%AF%AD%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day16，day17,day18,day19,day20(神经网络)</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/"/>
    <id>http://dinry.github.io/西瓜书day16/</id>
    <published>2019-07-24T07:14:48.000Z</published>
    <updated>2019-07-27T01:39:36.338Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="神经元模型">神经元模型</span></h1><p>神经网络中最基本的成分是神经元模型。</p><h2><span id="m-p神经元模型">M-P神经元模型</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/1.JPG" alt></p><h2><span id="激活函数">激活函数</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/2.JPG" alt></p><h1><span id="感知机与多层网络">感知机与多层网络</span></h1><h2><span id="感知机">感知机</span></h2><p>感知机由两层神经元组成，如图，<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/3.JPG" alt></p><p>感知机能够容易的实现逻辑与、或、非运算， $f(\sum_iw_ix_i-\theta)$, 假定f是阶跃函数，有</p><ul><li>与：令 $w_1=w_2=1,\theta=2$, 则 $y=f(1 \cdot x_1+1 \cdot x_2-2)$,仅在$x_1=x_2=1$ 时，y=1;</li><li>或：令 $w_1=w_2=1,\theta=0.5$, 则 $y=f(1 \cdot x_1+1 \cdot x_2-0.5)$,仅在$x_1=1 or x_2=1$ 时，y=1;</li><li>非：令令 $w_1=-06，w_2=0,\theta=-0.5$, 则 $y=f(-0.6 \cdot x_1+0 \cdot x_2+0.5)$,当$x_1=1$ 时，$y=0$,当 $x_1=0$ 时， $y=1$.</li></ul><p>给定训练数据集，权重与阈值可以通过学习得到。</p><p>感知机学习规则：对训练样例(x,y)，若当前感知机的输出为 $\hat{y}$, 则感知机权重将这样调整：</p><p>$w_i \leftarrow w_i +\Delta w_i$</p><p>$\Delta w_i=\eta(y-\hat{y})x_i$</p><p>其中 $\eta \in (0,1)$ 代表learning rate,.</p><h2><span id="感知机的局限性">感知机的局限性</span></h2><p>感知机只拥有一层功能神经元，学习能力非常有限，只能处理线性可分问题，不能解决抑或问题。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/4.JPG" alt>要解决非线性可分问题，需要考虑多层神经元，例如添加一个隐藏层的两层神经元可以解决“异或”问题。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/5.JPG" alt>神经网络的学习过程，就是根据训练数据来调整神经元之间的权重以及每个功能神经元的阈值。</p><h1><span id="误差逆传播算法bp">误差逆传播算法（BP）</span></h1><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/6.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/7.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/8.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day16/9.JPG" alt></p><h1><span id="全局最小与局部最小">全局最小与局部最小</span></h1><p>神经网络的训练过程可以看作参数寻优过程，在参数空间中寻找一组最优参数使神经网络在训练集上的误差达到最小。</p><p>局部最优解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的函数值</p><p>全局最小解：参数空间中所有点的误差函数值均不小于该点的函数值</p><p>基于梯度的搜索是使用最为广泛的参数寻优方法</p><h4><span id="局部最小跳出策略">局部最小跳出策略</span></h4><ul><li>以多组不同参数值初始化多个神经网络，取误差最小的参数</li><li>“模拟退火”：以一定概率接受比当前解更差的结果</li><li>随机梯度下降 在计算梯度时加入了随机因素，即使陷入局部极小，也有可能跳出。</li></ul><h1><span id="其他常见的神经网络">其他常见的神经网络</span></h1><h4><span id="rbf">RBF</span></h4><h4><span id="art">ART</span></h4><h4><span id="smo">SMO</span></h4><h4><span id="级联相关网络">级联相关网络</span></h4><h4><span id="elman网络">Elman网络</span></h4><h4><span id="boltzmann">Boltzmann</span></h4><h1><span id="神经网络">神经网络</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;神经元模型&quot;&gt;神经元模型&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;神经网络中最基本的成分是神经元模型。&lt;/p&gt;
&lt;h2&gt;&lt;span id=&quot;m-p神经元模型&quot;&gt;M-P神经元模型&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/%E8%A5%BF%E7%93%
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day13,day14,day15（贝叶斯分类器）</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/"/>
    <id>http://dinry.github.io/西瓜书day13/</id>
    <published>2019-07-19T23:55:41.000Z</published>
    <updated>2019-07-22T10:56:08.546Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="以多分类为例">以多分类为例</span></h1><h1><span id="贝叶斯判定准则">贝叶斯判定准则</span></h1><p>为最小化总体风险，只需在每个样本上选择能使条件风险 $R(c \mid x)$ 最小的类别标记，即为： $h^*(x)=argmin_{c \in y}R(c \mid x)=argmax_{c \in y}P(c \mid x)$,</p><p>此时，$h^*$ 为贝叶斯最优分类器。条件风险：</p><p>$R(c_i \mid x)=\sum_{j=1}^N \lambda_{ij}P(c_j \mid x)=1-P(c_i \mid x)$,</p><p>$\lambda_{ij}$ 是将一个真实标记为 $c_j$ 的样本误分类为 $c_i$ 所产生的损失。</p><h1><span id="多元正态分布的mle">多元正态分布的MLE</span></h1><p>概率模型的训练过程就是参数估计过程。</p><ul><li><p>频率主义学派：参数虽然未知，但却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值</p></li><li><p>贝叶斯学派：参数是为观察到的随机变量，其本身也可以有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。</p></li></ul><p>MLE源自频率主义学派。</p><p>$P(D_c \mid \theta_c)=\prod_{x \in D_c}P(x \mid \theta_c)$</p><p>其中 $D_c$ 表示训练集D中第c类样本组成的集合。</p><p>推导：</p><p>$LL(\theta_c)$</p><p>$=logP(D_c \mid \theta_c)$</p><p>$=\sum_{x \in D_c}logP(x \mid \theta_c)$</p><h2><span id="多元正态分布的概率密度函数">多元正态分布的概率密度函数：</span></h2><p>由于 $P(x \mid \theta_c)=P(x \mid c) -N(\mu_c,\sigma_c^2)$, 那么</p><p>$P(x \mid \theta_c)=\frac{1}{\sqrt{(2\pi)^d\mid\sum_c\mid}}exp(-\frac{1}{2}(x-\mu_c)^T\sum_c^{-1}(x-\mu_c))$</p><p>其中d表示xde维数，$\sum_c=\sigma_c^2$ 为对称正定协方差矩阵，$\mid\sum_c\mid$ 表示行列式，将上式代入对数似然函数可得</p><p>$LL(\theta_c)=\sum_{x \in D_c}ln[\frac{1}{\sqrt{(2\pi)^d\mid\sum_c\mid}}exp(-\frac{1}{2}(x-\mu_c)^T\sum_c^{-1}(x-\mu_c))]$</p><p>$=\sum_{i=1}^Nln[\frac{1}{\sqrt{(2\pi)^d\mid\sum_c\mid}}exp(-\frac{1}{2}(x_i-\mu_c)^T\sum_c^{-1}(x_i-\mu_c))]$</p><p>$=\sum_{i=1}^N{ln\frac{1}{\sqrt{(2\pi)^d}}+ln\frac{1}{\sqrt{\mid \sum_c \mid}}+ln[exp(-\frac{1}{2}(x_i-\mu_c)^T\sum_c^{-1}(x_i-\mu_c))]}$</p><p>$=-\frac{Nd}{2}ln(2\pi)-\frac{N}{2}ln\mid \sum_c\mid-\frac{1}{2}\sum_{i=1}^N(x_i-\mu_c)^T\sum_c^{-1}(x_i-\mu_c))$</p><p>由于参数 $\theta_c$ 的极大似然估计 $\hat{\theta_c}$ 为 $\hat{\theta_c}=argmax_{\theta_c}LL(\theta_c)$，</p><p>对 $LL(\theta_c)$ 关于 $\mu_c$ 求偏导：</p><p>$\frac{\partial LL(\theta_c)}{\partial \mu_c}=-\frac{1}{2}\sum_{i=1}^{N}\frac{\partial (x_i^T-\mu_c^T)\sum_c^{-1}(x_i-\mu_c)}{\partial \mu_c}$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}\frac{\partial [x_i^T\sum_c^{-1}x_i-x_i^T\sum_c^{-1}\mu_c-\mu_c^T\sum_c^{-1}x_i+\mu_c^T\sum_c^{-1}\mu_c]}{\partial \mu_c}$</p><p>由于 $x_i^T\sum_c^{-1}\mu_c=(x_i^T\sum_c^{-1}\mu_c)^T=\mu_c^T(\sum_c^T)^{-1}x_i=\mu_c^T\sum_c^{-1}x_i$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}\frac{\partial [x_i^T\sum_c^{-1}x_i-2x_i^T\sum_c^{-1}\mu_c+\mu_c^T\sum_c^{-1}\mu_c]}{\partial \mu_c}$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}[0-(2x_i^T\sum_c^{-1})^T+(\sum_c^{-1}+(\sum_c^{-1})^T)\mu_c]$</p><p>$=-\frac{1}{2}\sum_{i=1}^{N}[-(2\sum_c^{-1}x_i)+2\sum_c^{-1}\mu_c]$</p><p>$=\sum_{i=1}^{N}\Sigma_c^{-1}x_i-N\Sigma_c^{-1}\mu_c$</p><p>$=0$</p><p>$\hat{\mu_c}=\frac{\sum_{i=1}^Nx_i}{N}$</p><p>对 $LL(\theta_c)$ 关于 $\Sigma_c$ 求偏导：</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/4.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/5.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/6.JPG" alt></p><h2><span id="评估">评估</span></h2><p>这种参数化的方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。</p><h1><span id="朴素贝叶斯分类器">朴素贝叶斯分类器</span></h1><p>$h^*(x)=argmax P(c \mid x)=argmax \frac{P(c)P(x \mid c)}{P(x)}=argmaxP(c)P(x \mid c)$ ++++++属性条件独立性假设</p><p>属性条件独立性假设定义：$P(x \mid c)=P(x_1,x_2,...,x_d \mid c)=\prod_{i=1}^{d}P(x_i \mid c)$ -----(牺牲准确度换取计算效率)</p><h2><span id="naive-bayes">naive bayes</span></h2><p>$h^*(x)=argmaxP(c)\prod_{i=1}^dP(x_i \mid c)$</p><h2><span id="先验概率-pc">先验概率 $P(c)$</span></h2><p>$P(c)=\frac{\mid D_c \mid}{\mid D \mid}$</p><h2><span id="似然概率-px_i-mid-c">似然概率 $P(x_i \mid c)$</span></h2><h4><span id="连续值">连续值</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/2.JPG" alt></p><h4><span id="离散值">离散值</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/1.JPG" alt></p><h2><span id="laplacian-correction">Laplacian correction</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/3.JPG" alt>拉普拉斯修正避免了因训练集样本不充足而导致概率估值为0的问题，当训练集样本增大，这个误差会被忽略。</p><h1><span id="em算法">EM算法</span></h1><h2><span id="em算法的引入">EM算法的引入</span></h2><h4><span id="为什么需要em">为什么需要EM？</span></h4><p>训练样本含有隐变量Z</p><h2><span id="em算法的例子">EM算法的例子</span></h2><p>《统计学习方法》-三硬币模型  9.1</p><p>迭代求解参数,近似极大化</p><h2><span id="em算法的导出">EM算法的导出</span></h2><h4><span id="jensen-不等式">Jensen 不等式</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/7.JPG" alt></p><p>可以将a看作概率，则f表示为期望</p><p>Jensen不等式在概率论中的应用：$\varphi(E[X]) \leq E[\varphi(X)]$</p><h4><span id="推导">推导</span></h4><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/8.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/9.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/10.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/11.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/12.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day13/13.JPG" alt></p><h2><span id="em算法求解例子">EM算法求解例子</span></h2><p>用EM求解三硬币</p><ul><li>E:求Q</li><li>M：寻找参数最大化期望</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;以多分类为例&quot;&gt;以多分类为例&lt;/span&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;span id=&quot;贝叶斯判定准则&quot;&gt;贝叶斯判定准则&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;为最小化总体风险，只需在每个样本上选择能使条件风险 $R(c \mid x)$ 最小的类别标记，即为： $
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day9,day10,day11,day12(支持向量机)</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/"/>
    <id>http://dinry.github.io/西瓜书day9/</id>
    <published>2019-07-16T00:53:00.000Z</published>
    <updated>2019-07-19T03:21:05.443Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="预备知识">预备知识</span></h1><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/3.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/4.JPG" alt></p><h1><span id="间隔与支持向量">间隔与支持向量</span></h1><p>分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/1.JPG" alt></p><p>直观来说，应该找位于两类训练样本“正中间”的划分超平面，因为其对训练样本局部扰动的“容忍”性最好。</p><p>在训练样本中，划分超平面可以通过线性方程 $\mathbb{w}^T\mathbb{x}+b=0$ 来描述。其中 $\mathbb{w}=(w_1;w_2;...;d_d)$ 为法向量，决定了超平面的方向；b为位移，决定了超平面与原点之间的距离。样本空间中任意点 $x$ 到超平面$(\mathbb{w},b)$ 的距离可写为：</p><p>$r=\frac{\mid \mathbb{w}^T\mathbb{x}+b \mid}{\mid \mid \mathbb{w} \mid \mid}$.</p><p>证明：</p><p>任意取超平面上一个点 $x'$，则点 $x$ 到超平面的距离等于向量 $(x-x')$ 在法向量 $w$（参考预备2）的投影长度（参考预备1）:</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/5.JPG" alt>注意：上式推导过程中，分子之所有取绝对值是由于向量内积可能小于零；另外，由于 $x'$ 是超平上面的点，因此 $\mathbb{w}^T\mathbb{x'}+b=0$，即 $b=-\mathbb{w}^T\mathbb{x'}$。</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/6.JPG" alt></p><p>注意到，距离超平面最近的训练样本可以使上式的等号成立，由6.2知这些训练样本到超平面的距离为：</p><p>$dist=\frac{\mid \mathbb{w}^T\mathbb{x}+b \mid}{\mid \mid \mathbb{w} \mid \mid}=\frac{1}{\mid \mid w \mid \mid}$.</p><p>那么很容易知道，两个异类支持向量到超平面的距离之和是 $\frac{2}{\mid \mid w \mid \mid}$<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/2.JPG" alt></p><h1><span id="支持向量基本型">支持向量基本型</span></h1><p>最大间隔超平面条件等同于最小化如下公式：</p><p>$min_{w,b} \frac{1}{2} \mid \mid \mathbb{w} \mid \mid^2$</p><p>s.t. $y_i(\mathbb{w}^T\mathbb{x}_i+b) \ge 1$, i=1,2,...,m.</p><p>式(6.6)的约束条件意思是训练样本线性可分，也就是说不存在被分类错误的样本，因此也就不存在欠拟合问题；已知优化式(6.6)目标函数是在寻找“最大间隔”的划分超平面，而“最大间隔”划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强，因此可将式(6.6)优化目标进一步解释为寻找最不可能过拟合的分类超平面，这一点与正则化不谋而合。</p><h1><span id="对偶问题">对偶问题</span></h1><h2><span id="拉格朗日乘子法">拉格朗日乘子法</span></h2><p>此出假设优化问题一定有解<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/7.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/8.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/9.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/10.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/11.JPG" alt></p><h1><span id="核函数">核函数</span></h1><p>使训练样本在高维空间可分的映射函数。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/12.JPG" alt>$f(x)=\mathbb{w}^T \phi(x)+b$, 此时w的维度与 $\phi(x)$ 同。</p><p>核函数可以分解成两个向量的内积。要想了解某个核函数是如何将原始特征空间映射到更高维的特征空间的，只需要将核函数分解为两个表达形式完全一样的向量 $\phi(x_i)$ 和 $\phi(x_j)$ 即可（有时很难分解）。以下是LIBSVM中的几个核函数：<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/13.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/14.JPG" alt>遗留问题：核函数的几何意义是什么？核矩阵正定核函数就存在？</p><h1><span id="软间隔与正则化">软间隔与正则化</span></h1><p>软间隔的引入：</p><p>在前面的学习中，一直假设训练样本在样本空间或特征空间是线性可分的，要求所有样本都必须正确划分，称为“硬间隔”，然而现实中很难确定核函数使训练样本线性可分，缓解这一问题的方法是允许SVM在一些样本上出错，因此，引入软间隔：允许某些样本不满足约束 $y_i(w^Tx_i+b) \geq 1$<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/15.JPG" alt></p><h2><span id="预备知识替代损失函数">预备知识：替代损失函数</span></h2><ul><li>凸函数</li><li>连续函数<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/16.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/19.JPG" alt></li></ul><h2><span id="软间隔优化目标函数">软间隔优化目标函数</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/17.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/18.JPG" alt></p><p>引入松弛变量后的目标函数</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/20.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/21.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;预备知识&quot;&gt;预备知识&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/3.JPG&quot; alt&gt;
&lt;img src=&quot;/%E8%A5%BF%E7%93%9C%E4%B9%A6day9/4.
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>paper:AdamOptimizer</title>
    <link href="http://dinry.github.io/paper-AdamOptimizer/"/>
    <id>http://dinry.github.io/paper-AdamOptimizer/</id>
    <published>2019-07-12T12:20:39.000Z</published>
    <updated>2019-07-12T12:56:57.809Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="paperadam-a-method-for-stochastic-optimization">paper：Adam: A Method for Stochastic Optimization</span></h1><p>论文链接：<img src="https://arxiv.org/abs/1412.6980" alt></p><p><img src="/paper-AdamOptimizer/1.JPG" alt></p><p>如上算法所述，在确定了参数 $\alpha$,$\beta_1$,$\beta_2$和随机目标函数 $f(\theta)$ 之后，我们需要初始化参数向量、一阶矩向量、二阶矩向量和时间步。然后当参数 $\theta$ 没有收敛时，循环迭代地更新各个部分。即时间步 t 加 1、更新目标函数在该时间步上对参数 $\theta$ 所求的梯度、更新偏差的一阶矩估计和二阶原始矩估计，再计算偏差修正的一阶矩估计和偏差修正的二阶矩估计，然后再用以上计算出来的值更新模型的参数 $\theta$。</p><h1><span id="算法">算法</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;paperadam-a-method-for-stochastic-optimization&quot;&gt;paper：Adam: A Method for Stochastic Optimization&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;论文链接：&lt;img src=
      
    
    </summary>
    
      <category term="deep learning" scheme="http://dinry.github.io/categories/deep-learning/"/>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day5,day6,day7,day8(决策树)</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/"/>
    <id>http://dinry.github.io/西瓜书day5/</id>
    <published>2019-07-12T08:44:57.000Z</published>
    <updated>2019-07-15T06:05:15.300Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="决策树定义">决策树定义</span></h1><p>决策树是基于输结构来进行决策的一类常见的机器学习分类方法。</p><h1><span id="解决问题">解决问题</span></h1><p>“当前样本属于正类吗？”</p><p>“这是好瓜吗？”</p><p>“它的根蒂是什么形态”</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/1.JPG" alt></p><p>决策过程的最终结论（叶子节点）对应了我们所希望的判定结果：好瓜 or 坏瓜</p><h1><span id="结构">结构</span></h1><p>1.一个根节点和若干个内部节点：分别对应一个属性测试</p><p>2.若干个叶节点：对应决策结果</p><p>j决策树学习的目的是为了产生一棵泛化能力强的决策树，基本流程遵循“分而治之”：</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/2.JPG" alt></p><p>递归返回条件：</p><ul><li>当前节点包含的样本全属于同一类别，无需划分</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分</li><li>当前节点包含的样本集合为空，不能划分。</li></ul><h1><span id="id3">ID3</span></h1><p>信息熵：</p><p>熵是度量样本集合纯度最常用的一种指标，代表一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性，即信息熵越大。</p><p>假定当前样本集合D中第K类样本所占的比例为 $p_k(k=1,2,...,\mid y\mid)$, 则D的信息熵为：</p><p>$Ent(D)=-\sum_{k=1}^{\mid y \mid}p_{k}log_{2}p_{k}$</p><p>信息熵满足下列不等式：</p><p>$0 \leq Ent(D) \leq log_{2}\mid y \mid$ ,</p><p>$0 \leq p_k \leq 1$, $\sum_{k=1}^np_k=1$</p><p>其中$y$表示D中的样本类别数。</p><h2><span id="id3推导max-value">ID3推导(max-value)</span></h2><p>若令 $\mid y \mid=n,p_k=x_k$, 那么信息熵 $Ent(D)$ 可以看作一个n元实值函数，即</p><p>$Ent(D)=f(x_1,...,x_n)=-\sum_{k=1}^{n}x_{k}log_{2}x_{k}$, 其中：$0 \leq p_k \leq 1$, $\sum_{k=1}^np_k=1$.</p><p>引入拉格朗日乘子 $\lambda$求最值：</p><p>$L(x_1,...,x_n,\lambda)=-\sum_{k=1}^{n}x_{k}log_{2}x_{k}+\lambda(\sum_{k=1}^nx_k-1)$</p><p>$\frac{\partial L(x_1,...,x_n,\lambda)}{\partial x_1}=-log_2x_1-x_1 \cdot \frac{1}{x_{1}ln2}+\lambda=0$</p><p>$\lambda=log_2x_1+\frac{1}{ln2}$</p><p>同理：</p><p>$\lambda=log_2x_1+\frac{1}{ln2}=log_2x_2+\frac{1}{ln2}=...=log_2x_n+\frac{1}{ln2}$</p><p>so: $x_1=x_2=...=x_n=\frac{1}{n}$</p><p>最大值与最小值需要验证：</p><ul><li>$x_1=x_2=...=x_n=\frac{1}{n}$ 时：$f=-n \cdot \frac{1}{n}log_2\frac{1}{n}=log_2n$</li><li>$x_1=1,x_2=x_3=...=x_n=0$ 时： $f=0$</li></ul><p>所以为最大值</p><h2><span id="id3推导min-value">ID3推导(min-value)</span></h2><p>Assume:</p><p>$f(x_1,...x_n)=\sum_{k=1}^ng(x_k)$</p><p>$g(x_k)=-\sum_{k=1}^{n}x_{k}log_{2}x_{k}$, $0 \leq p_k \leq 1$</p><p>求 $g(x_1)$ 的最小值，首先求导：</p><p>$\frac{d(g(x_1))}{dx_1}=-log_2x_1-\frac{1}{ln2}$</p><p>$\frac{d^2(g(x_1))}{dx^2}=-\frac{1}{x_1ln2}$</p><p>在定义域$0 \leq p_k \leq 1$， 始终有 $\frac{d^2(g(x_1))}{dx^2}=-\frac{1}{x_1ln2} \leq 0$,所以最小值在边界处$x_1=0 或 x_1=1$取得：$min g(x_1)=0$</p><p>由推理可得$f(0,0,0,0,1,...,0)=0$</p><p>所以：</p><p>$0 \leq Ent(D) \leq log_{2}\mid y \mid$</p><h2><span id="信息增益">信息增益</span></h2><p>假定离散属性有V个可能的取值 {${a^1,a^2,...,a^V}$}, 如果使用特征a来对数据集D进行划分，则会产生V个分支结点，其中第 $v$ 个结点包含了数据集D中所有在特征a上取值为 $a^V$ 的样本总数，记为 $D^v$, 特征对样本集D进行划分的“样本增益”为：</p><p>$Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac{\mid D^V \mid}{\mid D \mid}Ent(C^v)$</p><h2><span id="缺点">缺点</span></h2><p>1.ID3没有考虑连续特征</p><p>2.ID3采用信息增益大的特征优先建立决策树的节点，取值比较多的特征比取值少的特征信息增益大</p><p>3.ID3算法对于缺失值的情况没有做考虑</p><p>4.没有考虑过拟合的情况</p><h1><span id="c45算法">C4.5算法</span></h1><p>增益率：</p><p>弥补ID3偏向于取值较多属性,C4.5算法不直接使用信息增益，而是使用一种叫增益率的方法来选择最优属性进行划分：</p><p>$Gain-ration(D,a)=\frac{Gain(D,a)}{IV(a)}$</p><p>$IV(a)$ 是属性 $a$ 的固有值：</p><p>$IV(a)=-\sum_{v=1}^V \frac{\mid D^v \mid}{\mid D \mid}log_2 \frac{\mid D^v \mid}{D}$</p><p>属性越多，熵越大，对分支过多的情况进行惩罚。</p><h2><span id="缺点">缺点</span></h2><p>1.C4.5生成的是多叉树，生成决策树的效率比较慢</p><p>2.C4.5只能用于分类</p><p>3.C4.5由于使用了熵模型，对数运算耗时。</p><h1><span id="cart">CART</span></h1><p>Gini值：</p><p>度量数据集的纯度，Gini(D)反应了从数据集中随机抽取两个样本,类别标记不一致的概率，数据越小，纯度越高。</p><p>$Gini(D)=\sum_{k=1}^{\mid y \mid}\sum_{k' \neq k}p_kp_{k'}=1-\sum_{k=1}^{\mid y \mid}p_k^2$</p><p>$Gini-index(D,a)=\sum_{v=1}^V \frac{\mid D^v \mid}{\mid D \mid}Gini(D^v)$</p><h1><span id="剪枝">剪枝</span></h1><p>减指是决策树学习算法处理过拟合的重要手段。决策树剪枝的基本策略有“预剪枝”和“后剪枝”。预剪枝是对每个节点在划分前进行估计，若不能带来泛化能力的提升，则停止划分并将该节点标记为叶节点。后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上计算泛化能力是否提升，否则标记为叶节点。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/3.JPG" alt></p><h2><span id="预剪枝">预剪枝</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/4.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/5.JPG" alt></p><h2><span id="后剪枝">后剪枝</span></h2><p>后剪枝先从训练集生成一棵完整的决策树如图4.5，该决策树的验证集精度为42.9%，后剪枝首先考察节点6，然后节点五，节点二，节点三，节点一，计算过程如图。<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/6.JPG" alt></p><h2><span id="对比">对比</span></h2><ul><li>后剪枝比预剪枝保留了更多的分支。</li><li>后剪枝欠拟合风险小，泛化能力优于预剪枝决策树</li><li>后剪枝是在生成决策树后进行的，自底向上对非叶节点逐个考察，训练时间与开销都比预剪枝大得多</li></ul><h1><span id="连续与缺失值">连续与缺失值</span></h1><h1><span id="连续值处理在决策树学习中使用连续属性进行决策">连续值处理：在决策树学习中使用连续属性进行决策</span></h1><p>方法：连续属性离散化：（eg,二分法)</p><p>给定样本集$D$ 和连续属性，划分点$t$可以将D分为子集 $D_t^-$ 与 $D_t^+$, $D_t^-$包含在属性 $a$ 上不大于 $t$ 的样本，候选划分集合：</p><p>$T_a={\frac{a^i+a^{i+1}}{2} \mid 1 \leq i \leq n-1}$</p><p>然后我们可以像离散属性一样来选择划分点：</p><p>$Gain(D,a)=max_{t \in T_a}Gain(D,a,t)=max_{t \in T_a}Ent(D)-\sum_{\lambda \in {-,+}}\frac{\mid D_t^{\lambda} \mid}{\mid D \mid}Ent(D_t^{\lambda})$</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/7.JPG" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/8.JPG" alt></p><h1><span id="缺失值处理">缺失值处理</span></h1><p>样本的某些属性值缺失如何进行划分属性的选择呢？</p><p>给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/9.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/10.JPG" alt></p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day5/11.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;决策树定义&quot;&gt;决策树定义&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;决策树是基于输结构来进行决策的一类常见的机器学习分类方法。&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;解决问题&quot;&gt;解决问题&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;“当前样本属于正类吗？”&lt;/p&gt;
&lt;p&gt;“这是好
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>paper:CFGAN</title>
    <link href="http://dinry.github.io/paper-CFGAN/"/>
    <id>http://dinry.github.io/paper-CFGAN/</id>
    <published>2019-07-11T14:41:11.000Z</published>
    <updated>2019-07-11T14:46:22.352Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客总结paper CFGAN。</p><p>《CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks》</p><p>from CIKM 2018</p><p>contribution:  GAN-based CF model</p><p>keyWords: Top-N recommendation, collaborative filtering, generative adversarial networks, implicit feedback</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客总结paper CFGAN。&lt;/p&gt;
&lt;p&gt;《CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks》&lt;/p&gt;
&lt;p&gt;from CIKM 
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
      <category term="Recommender systems" scheme="http://dinry.github.io/tags/Recommender-systems/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow loss分析</title>
    <link href="http://dinry.github.io/train-loss/"/>
    <id>http://dinry.github.io/train-loss/</id>
    <published>2019-07-11T13:19:46.000Z</published>
    <updated>2019-07-11T14:47:38.930Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="train-loss与test-loss结果分析">train loss与test loss结果分析</span></h1><p>train loss 不断下降，test loss不断下降，说明网络仍在学习;</p><p>train loss 不断下降，test loss趋于不变，说明网络过拟合;</p><p>train loss 趋于不变，test loss不断下降，说明数据集100%有问题;</p><p>train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;</p><p>train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。</p><h1><span id="loss和神经网络训练">Loss和神经网络训练</span></h1><h2><span id="训练前的检查工作">训练前的检查工作</span></h2><p>1.loss:在用很小的随机数初始化神经网络后，第一遍计算loss可以做一次检查(当然要记得把正则化系数设为0)。</p><p>2.接着把正则化系数设为正常的小值，加回正则化项，这时候再算损失/loss，应该比刚才要大一些。</p><p>3.试着去拟合一个小的数据集。最后一步，也是很重要的一步，在对大数据集做训练之前，先训练一个小的数据集，然后看看你的神经网络能够做到0损失/loss(当然，是指的正则化系数为0的情况下)，因为如果神经网络实现是正确的，在无正则化项的情况下，完全能够过拟合这一小部分的数据。</p><h2><span id="监控">监控</span></h2><p>开始训练之后，我们可以通过监控一些指标来了解训练的状态。我们还记得有一些参数是我们认为敲定的，比如学习率，比如正则化系数。</p><p>1.损失/loss随每轮完整迭代后的变化</p><p><img src="/train-loss/1.JPG" alt></p><p>合适的学习率可以保证每轮完整训练之后，loss都减小，且能在一段时间后降到一个较小的程度。太小的学习率下loss减小的速度很慢，如果太激进，设置太高的学习率，开始的loss减小速度非常可观，可是到了某个程度之后就不再下降了，在离最低点一段距离的地方反复，无法下降了。</p><p>2.训练集/验证集上的准确度:判断分类器所处的拟合状态。</p><p><img src="/train-loss/2.JPG" alt></p><p>随着时间推进，训练集和验证集上的准确度都会上升，如果训练集上的准确度到达一定程度后，两者之间的差值比较大，那就要注意一下，可能是过拟合现象，如果差值不大，那说明模型状况良好。</p><p>3.权重：权重更新幅度和当前权重幅度的比值权重更新部分是梯度和学习率的乘积，可以独立的检查这个比例，一个合适的比例大概是1e-3。如果得到的比例比这个值小很多，那么说明学习率设定太低了，反之则是设定太高了。</p><p>4.每一层的 激励/梯度值 分布：如果参数初始化不正确，那整个训练过程会越来越慢，甚至直接停掉。</p><h2><span id="关于参数更新部分的注意点">关于参数更新部分的注意点</span></h2><p>当确信解析梯度实现正确后，那就该在后向传播算法中使用它更新权重参数了。就单参数更新这个部分，也是有讲究的：</p><p>1.拿到梯度之后，乘以设定的学习率，用现有的权重减去这个部分，得到新的权重参数(因为梯度表示变化率最大的增大方向，减去这个值之后，损失函数值才会下降)。</p><p>2.在实际训练过程中，随着训练过程推进，逐渐衰减学习率是很有必要的。我们继续回到下山的场景中，刚下山的时候，可能离最低点很远，那我步子迈大一点也没什么关系，可是快到山脚了，我还激进地大步飞奔，一不小心可能就迈过去了。所以还不如随着下山过程推进，逐步减缓一点点步伐。不过这个『火候』确实要好好把握，衰减太慢的话，最低段震荡的情况依旧；衰减太快的话，整个系统下降的『动力』衰减太快，很快就下降不动了。下面提一些常见的学习率衰减方式：</p><ul><li>步伐衰减：这是很常见的一个衰减模式，每过一轮完整的训练周期(所有的图片都过了一遍)之后，学习率下降一些。比如比较常见的一个衰减率可能是每20轮完整训练周期，下降10%。不过最合适的值还真是依问题不同有变化。如果你在训练过程中，发现交叉验证集上呈现很高的错误率，还一直不下降，你可能就可以考虑考虑调整一下(衰减)学习率了。</li><li>指数级别衰减：需要自己敲定的超参数，是迭代轮数。</li><li>1/t衰减：有着数学形式为的衰减模式，其中是需要自己敲定的超参数，是迭代轮数。</li></ul><h2><span id="超参数的设定与优化">超参数的设定与优化</span></h2><p>神经网络的训练过程中，不可避免地要和很多超参数打交道，需要手动设定，大致包括：</p><p>1.初始学习率2.学习率衰减程度3.正则化系数/强度(包括l2正则化强度，dropout比例)</p><p>对于大的深层次神经网络而言，我们需要很多的时间去训练。因此在此之前我们花一些时间去做超参数搜索，以确定最佳设定是非常有必要的。最直接的方式就是在框架实现的过程中，设计一个会持续变换超参数实施优化，并记录每个超参数下每一轮完整训练迭代下的验证集状态和效果。实际工程中，神经网络里确定这些超参数，我们一般很少使用n折交叉验证，一般使用一份固定的交叉验证集就可以了。</p><p>一般对超参数的尝试和搜索都是在log域进行的。例如，一个典型的学习率搜索序列就是learning_rate = 10 ** uniform(-6, 1)。我们先生成均匀分布的序列，再以10为底做指数运算，其实我们在正则化系数中也做了一样的策略。比如常见的搜索序列为[0.5, 0.9, 0.95, 0.99]。另外还得注意一点，如果交叉验证取得的最佳超参数结果在分布边缘，要特别注意，也许取的均匀分布范围本身就是不合理的，也许扩充一下这个搜索范围会有更好的参数。</p><h2><span id="模型融合与优化">模型融合与优化：</span></h2><p>实际工程中，一个能有效提高最后神经网络效果的方式是，训练出多个独立的模型，在预测阶段选结果中的众数。模型融合能在一定程度上缓解过拟合的现象，对最后的结果有一定帮助，我们有一些方式可以得到同一个问题的不同独立模型：</p><ul><li>使用不同的初始化参数。先用交叉验证确定最佳的超参数，然后选取不同的初始值进行训练，结果模型能有一定程度的差别。</li><li>选取交叉验证排序靠前的模型。在用交叉验证确定超参数的时候，选取top的部分超参数，分别进行训练和建模。</li><li>选取训练过程中不同时间点的模型。神经网络训练确实是一件非常耗时的事情，因此有些人在模型训练到一定准确度之后，取不同的时间点的模型去做融合。不过比较明显的是，这样模型之间的差异性其实比较小，好处是一次训练也可以有模型融合的收益。</li></ul><p>检查你的初始权重是否合理，在关掉正则化项的系统里，是否可以取得100%的准确度。</p><p>在训练过程中，对损失函数结果做记录，以及训练集和交叉验证集上的准确度。</p><p>最常见的权重更新方式是SGD+Momentum，推荐试试RMSProp自适应学习率更新算法。</p><p>随着时间推进要用不同的方式去衰减学习率。</p><p>用交叉验证等去搜索和找到最合适的超参数。</p><p>记得也做做模型融合的工作，对结果有帮助。</p><h1><span id="loss保持常数的采坑记录">loss保持常数的采坑记录</span></h1><p>1.loss等于87.33这个问题是在对Inception-V3网络不管是fine-tuning还是train的时候遇到的，无论网络迭代多少次，网络的loss一直保持恒定。</p><p>原因（溢出）：</p><p>由于loss的最大值由FLT_MIN计算得到，FLT_MIN使其对应的自然对数正好是-87.3356，这也就对应上了loss保持87.3356了。这说明softmax在计算的过程中得到了概率值出现了零，由于softmax是用指数函数计算的，指数函数的值都是大于0的，所以应该是计算过程中出现了float溢出的异常，也就是出现了inf，nan等异常值导致softmax输出为0.当softmax之前的feature值过大时，由于softmax先求指数，会超出float的数据范围，成为inf。inf与其他任何数值的和都是inf，softmax在做除法时任何正常范围的数值除以inf都会变成0.然后求loss就出现了87.3356的情况。</p><p>solution:</p><p>由于softmax输入的feature由两部分计算得到：一部分是输入数据，另一部分是各层的权值等组成:</p><p>(1).减小初始化权重，以使得softmax的输入feature处于一个比较小的范围</p><p>(2).降低学习率，这样可以减小权重的波动范围</p><p>(3).如果有BN(batch normalization)层，finetune时最好不要冻结BN的参数，否则数据分布不一致时很容易使输出值变得很大(注意将batch_norm_param中的use_global_stats设置为false )。</p><p>(4).观察数据中是否有异常样本或异常label导致数据读取异常</p><h1><span id="loss不下降的常见原因">loss不下降的常见原因</span></h1><p>1）数据的输入是否正常，data和label是否一致。</p><p>2）网络架构的选择，一般是越深越好，也分数据集。 并且用不用在大数据集上pre-train的参数也很重要的。</p><p>3）loss 对不对。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;train-loss与test-loss结果分析&quot;&gt;train loss与test loss结果分析&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;train loss 不断下降，test loss不断下降，说明网络仍在学习;&lt;/p&gt;
&lt;p&gt;train loss 不断
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day2,day3,day4(线性模型)</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/"/>
    <id>http://dinry.github.io/西瓜书day2/</id>
    <published>2019-07-09T08:20:52.000Z</published>
    <updated>2019-07-11T11:07:25.218Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="基本形式">基本形式</span></h1><p>example:</p><p>$\mathbb{x}=(x_1;x_2;...;x_d)$</p><p>$x_i$是 $\mathbb{x}$ 在第i个属性上的取值线性模型试图学得$f(\mathbb{x})=\mathbb{w}^T\mathbb{x}+b$ 来预测函数，$\mathbb{w}$ 和 $b$ 为参数</p><p>线性模型优点：可解释性强</p><p>分类：</p><ul><li>回归</li><li>分类</li></ul><h1><span id="回归">回归</span></h1><p>均方误差是回归任务中最常用的性能度量</p><h2><span id="一元线性回归">一元线性回归</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/1.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/2.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/3.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/4.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/5.png" alt></p><h2><span id="多元线性回归">多元线性回归</span></h2><p>更一般的情形，样本由多个属性决定，此时$f(\mathbb{x}_i)=\mathbb{w}^T\mathbb{x}_i+b$, 称为多元线性回归</p><p>依旧用最小二乘法<img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/6.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/7.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/8.png" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/9.png" alt></p><h2><span id="对数几率回归">对数几率回归</span></h2><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/10.jpg" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/11.jpg" alt><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day2/12.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;基本形式&quot;&gt;基本形式&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;example:&lt;/p&gt;
&lt;p&gt;$\mathbb{x}=(x_1;x_2;...;x_d)$&lt;/p&gt;
&lt;p&gt;$x_i$是 $\mathbb{x}$ 在第i个属性上的取值线性模型试图学得$f(\math
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书day1（绪论）</title>
    <link href="http://dinry.github.io/%E8%A5%BF%E7%93%9C%E4%B9%A6day1/"/>
    <id>http://dinry.github.io/西瓜书day1/</id>
    <published>2019-07-08T07:30:32.000Z</published>
    <updated>2019-07-08T08:33:51.010Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="绪论">绪论</span></h1><p>机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。经验即数据。</p><p>机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”的算法，learning algorithms.再用模型来预测未来数据。</p><h1><span id="术语">术语</span></h1><p>记录：</p><p>数据集：记录的集合</p><p>训练：从数据中学习模型的过程</p><p>训练集：训练过程中使用的数据样本的集合</p><p>分类任务：预测的结果为离散值（好瓜，坏瓜）</p><p>回归任务：预测值是连续值</p><p>根据训练数据是否有label，学习任务可划分为：“监督学习”，“无监督学习”</p><p>泛化能力：学得模型适用于新样本的能力</p><h1><span id="归纳演绎">归纳演绎</span></h1><p>归纳：从特殊到一般（机器学习）</p><p>演绎：从一般到特殊</p><h1><span id="发展历程">发展历程</span></h1><p>机器学习是人工智能研究发展到一定阶段的必然产物。</p><table><thead><tr><th>年代</th><th>事件</th><th>代表工作</th></tr></thead><tbody><tr><td>二十世纪而五十年代到七十年代</td><td>人工智能的推理期</td><td>感知机、Adaline</td></tr><tr><td>五十年代中后期</td><td>符号主义蓬勃发展，决策理论。增强学习</td><td>结构学习系统，概念学习系统</td></tr><tr><td>八十年代</td><td>决策树学习</td><td>由于复杂度过高而陷入低潮</td></tr><tr><td>九十年代</td><td>基于神将网络的连接学习</td><td>hopfield,BP,产生黑箱模型</td></tr><tr><td>九十年代中期</td><td>统计学习</td><td>SVM, 核方法</td></tr><tr><td>二十一世纪初</td><td>深度学习</td><td>对数据，硬件要求高</td></tr></tbody></table><h1><span id="应用现状">应用现状</span></h1><p>今天，在计算机学科的诸多分支学科中，无论是多媒体，图形学，还是网络通信，软件工程，体系结构，芯片设计都能找到机器学习的身影，尤其是CV与NLP。</p><p>交叉学科：生物信息学</p><p>大数据时代的三大技术：机器学习，云计算，众包</p><p>数据挖掘与机器学习的关系：</p><p>数据挖掘技术在二十世纪九十年代形成，受数据库，机器学习，统计学影响最大。数据挖掘是从海量知识中发掘知识，这就必然涉及对海量数据的管理分析。数据库领域的研究为数据挖掘提供数据管理技术，而机器学习和统计学研究为数据挖掘提供数据分析技术，统计学主要是通过机器学习对数据挖掘发挥影响，机器学习领域与数据库领域是数据挖掘的两大支撑。</p><p><img src="/%E8%A5%BF%E7%93%9C%E4%B9%A6day1/1.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;绪论&quot;&gt;绪论&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。经验即数据。&lt;/p&gt;
&lt;p&gt;机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”的算法，learning algorithms.再
      
    
    </summary>
    
      <category term="ML" scheme="http://dinry.github.io/categories/ML/"/>
    
    
      <category term="西瓜书" scheme="http://dinry.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow深度学习(2):tf.nn.top_k()</title>
    <link href="http://dinry.github.io/tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-tf-nn-top-k/"/>
    <id>http://dinry.github.io/tensorflow深度学习-2-tf-nn-top-k/</id>
    <published>2019-07-05T03:42:07.000Z</published>
    <updated>2019-07-05T03:47:15.683Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="introduction">introduction</span></h1><p>def top_k(input, k=1, sorted=True, name=None)</p><p>Finds values and indices of the k largest entries for the last dimension.</p><p>If the input is a vector (rank=1), finds the k largest entries in the vector and outputs their values and indices as vectors.Thus values[j] is the j-th largest entry in input, and its index is indices[j].</p><p>For matrices (resp. higher rank input), computes the top k entries in each row (resp. vector along the last dimension).Thus, values.shape = indices.shape = input.shape[:-1] + [k]</p><p>If two elements are equal, the lower-index element appears first.</p><h1><span id="parameters">parameters</span></h1><p><img src="/tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-tf-nn-top-k/1.JPG" alt></p><h1><span id="code">code</span></h1><p><img src="/tensorflow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-tf-nn-top-k/2.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;introduction&quot;&gt;introduction&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;def top_k(input, k=1, sorted=True, name=None)&lt;/p&gt;
&lt;p&gt;Finds values and indices of the
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>temsorflow常用集合(colection)</title>
    <link href="http://dinry.github.io/tensorflow%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/"/>
    <id>http://dinry.github.io/tensorflow常用集合/</id>
    <published>2019-07-05T02:55:07.000Z</published>
    <updated>2019-07-05T03:23:05.388Z</updated>
    
    <content type="html"><![CDATA[<p>tensorflow 用集合collection组织不同类别的对象，tf.GraphKeys中包含了所有默认集合的名称。</p><p>collection在对应的scope内提供了“零存整取”的思想：任意位置，任意层次的对象，统一提取。</p><p>tf.optimizer只优化tf.GraphKeys.TRAINABLE_VARIABLES中的变量</p><h2><span id="常用集合">常用集合</span></h2><ul><li>Variable集合：模型参数</li><li>summary 集合：监测</li><li>自定义集合</li></ul><h1><span id="variable">Variable</span></h1><p>Variable被收集在tf.GraphKeys.VARIABLES的collection中</p><h2><span id="定义">定义</span></h2><p>k=tf.Variable()<img src="/tensorflow%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/1.JPG" alt></p><h1><span id="summary">summary</span></h1><p>Summary被收集在名为tf.GraphKeys.SUMMARIES的collection中</p><h2><span id="define">define</span></h2><p>对网络中tensor取值进行监测</p><p>调用tf.scalar_summary系列函数，会向默认的collection中添加一个operation</p><p><img src="/tensorflow%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/2.JPG" alt></p><h1><span id="自定义">自定义</span></h1><p>tf.add_to_collection(&quot;losses&quot;,l1)losses=tf.get_collection('losses')</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;tensorflow 用集合collection组织不同类别的对象，tf.GraphKeys中包含了所有默认集合的名称。&lt;/p&gt;
&lt;p&gt;collection在对应的scope内提供了“零存整取”的思想：任意位置，任意层次的对象，统一提取。&lt;/p&gt;
&lt;p&gt;tf.optimiz
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://dinry.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之线性模型推导</title>
    <link href="http://dinry.github.io/machine-learning-2/"/>
    <id>http://dinry.github.io/machine-learning-2/</id>
    <published>2019-07-02T05:50:36.000Z</published>
    <updated>2019-07-02T06:24:26.420Z</updated>
    
    <content type="html"><![CDATA[<p>西瓜书，南瓜书</p><h1><span id="一元线性回归">一元线性回归</span></h1><p>最小二乘法推导</p><h2><span id="b的公式推导3638">b的公式推导（3.6，3.8）</span></h2><p>（二元函数求最值）</p><p>1.由最小二乘法导出损失函数E（w,b）</p><p>2.证明损失函数是关于w,b的凸函数</p><p>3.对损失函数关于B求偏导数</p><p>4.另一接偏导数为0求b</p><p>由最小二乘法导出损失函数：</p><p>$E_{w,b}=\sum_{i=1}^m$</p><h2><span id="w的公式推导3537">w的公式推导（3.5，3.7）</span></h2><h1><span id="多元线性回归">多元线性回归</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;西瓜书，南瓜书&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;一元线性回归&quot;&gt;一元线性回归&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;最小二乘法推导&lt;/p&gt;
&lt;h2&gt;&lt;span id=&quot;b的公式推导3638&quot;&gt;b的公式推导（3.6，3.8）&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;（二元函数求最值）&lt;/p
      
    
    </summary>
    
    
      <category term="machine learning" scheme="http://dinry.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>machine learning(1)</title>
    <link href="http://dinry.github.io/machine-learning-1/"/>
    <id>http://dinry.github.io/machine-learning-1/</id>
    <published>2019-07-02T01:36:05.000Z</published>
    <updated>2019-07-02T05:47:00.271Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="机器学习的四大应用领域及其应用">机器学习的四大应用领域及其应用</span></h1><h2><span id="数据挖掘发现数据之间的关系">数据挖掘：发现数据之间的关系</span></h2><p>1.回归问题</p><p>2.分类问题</p><p>根据已知数据，学习函数。</p><h2><span id="计算机视觉像人一样看懂世界">计算机视觉：像人一样看懂世界</span></h2><p>图像分类</p><p>目标检测（无人驾驶）</p><p>语义分割（无人驾驶）</p><p>场景理解（无人驾驶）</p><h2><span id="nlp像人一样看懂文字">NLP：像人一样看懂文字</span></h2><p>文本分类（新闻分类）</p><p>自动生成文本摘要</p><p>翻译</p><p>QA</p><p>人机对话（小冰）</p><p>image to text</p><p>end to end级自动驾驶</p><h2><span id="机器人决策像人一样具有决策能力">机器人决策：像人一样具有决策能力</span></h2><p>TORCS平台（玩赛车游戏）：增强学习（agent,action）</p><p>机器人开门（自动执行）：增强学习</p><h1><span id="机器学习理论分类">机器学习理论分类</span></h1><p>常用的三类：</p><p>1.传统的监督学习（分类，回归）</p><p>2.深度学习（视觉，NLP）</p><p>3.强化学习（机器人）</p><p>三种分类学习按标号顺序循序渐进</p><h1><span id="先重点再难点">先重点，再难点</span></h1><p><img src="/machine-learning-1/1.jpg" alt></p><p><img src="/machine-learning-1/2.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;机器学习的四大应用领域及其应用&quot;&gt;机器学习的四大应用领域及其应用&lt;/span&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;span id=&quot;数据挖掘发现数据之间的关系&quot;&gt;数据挖掘：发现数据之间的关系&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;1.回归问题&lt;/p&gt;
&lt;p&gt;2.分类问题&lt;/
      
    
    </summary>
    
    
      <category term="machine learning" scheme="http://dinry.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>调参技巧汇总</title>
    <link href="http://dinry.github.io/%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/"/>
    <id>http://dinry.github.io/调参技巧汇总/</id>
    <published>2019-06-28T02:33:02.000Z</published>
    <updated>2019-06-28T03:31:40.553Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>推荐系统评估指标(Rank)</title>
    <link href="http://dinry.github.io/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    <id>http://dinry.github.io/推荐系统评估指标/</id>
    <published>2019-06-24T11:09:54.000Z</published>
    <updated>2019-06-24T11:54:46.854Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="mean-average-precision-map">Mean Average Precision (MAP)</span></h1><p>$AP=\frac {\sum_{j=1}^{n_i}P(j)\cdot y_{i,j}}{\sum_{j=1}^{n_i}y_{i,j}}$</p><p>其中，$y_{i,j}$: 排序中第j个元素对于查询i是否是相关的；相关为1，不相关为0。</p><p>$P(j)=\frac {\sum_{k:\pi_{i}(k)\leq\pi_{i}(j)} y_{i,k}}{\pi_{i}(j)}$</p><p>其中，$\pi_{i}(j)$为J的排序位置。</p><p>例如：</p><table><thead><tr><th>rank_no</th><th>是否相关</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>0</td></tr><tr><td>3</td><td>1</td></tr><tr><td>4</td><td>0</td></tr><tr><td>5</td><td>1</td></tr><tr><td>6</td><td>0</td></tr></tbody></table><p>则根据AP计算公式：$AP=(1*1 + (1/2) *0+ (2/3)*1 + (2/4)*0 + (3/5)*0 + (3/6)*0) /3$</p><p>AP的最大值是1，MAP就是对所有user求均值。</p><h1><span id="mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</span></h1><p>$MRR=\frac{1}{\mid Q \mid} \sum_{i=1}^{\mid Q \mid} \frac{1}{rank_i}$</p><p>其中|Q|是查询个数，ranki是第i个查询，第一个相关的结果所在的排列位置。</p><p>例如：</p><table><thead><tr><th>Query</th><th>Result</th><th>Correct response</th><th>Rank</th><th>Reciprocal rank</th></tr></thead><tbody><tr><td>cat</td><td>catten,cati,cats</td><td>cats</td><td>3</td><td>1/3</td></tr><tr><td>tori</td><td>torii,tori,toruses</td><td>tori</td><td>2</td><td>1/2</td></tr><tr><td>virus</td><td>viruses,virii,viri</td><td>viruses</td><td>1</td><td>1</td></tr></tbody></table><p>对于三个查询，每个查询的ranki分别为3、2、1。所以，MRR=1/3∗(1/3+1/2+1/1)</p><h1><span id="ndcgprerec的计算较为简单已在csdn中介绍这里省略">NDCG，pre,rec的计算较为简单，已在CSDN中介绍，这里省略。</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;mean-average-precision-map&quot;&gt;Mean Average Precision (MAP)&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;$AP=\frac {\sum_{j=1}^{n_i}P(j)\cdot y_{i,j}}{\sum_{j=
      
    
    </summary>
    
      <category term="recommender systems" scheme="http://dinry.github.io/categories/recommender-systems/"/>
    
    
      <category term="评估指标（Rec）" scheme="http://dinry.github.io/tags/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%EF%BC%88Rec%EF%BC%89/"/>
    
  </entry>
  
</feed>
