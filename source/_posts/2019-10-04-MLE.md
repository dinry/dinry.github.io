---
layout: post/
title: 极大似然估计，误差的高斯分布与最小二乘估计的等价性,最优化
date: 2019-10-04 10:42:22
tags: deep learning
categories:
  花书
---
# 极大似然估计
假设随机变量 $X-P(x;\theta)$

现有样本 $x_1$, $x_2$, ... $x_N$

定义似然函数为 $L=P(x_1;\theta)P(x_2;\theta) ... P(x_N;\theta)$

对数似然函数为 $\hat{L}=ln[P(x_1;\theta)P(x_2;\theta) ... P(x_N;\theta)]$

极大似然估计为 max L

高斯分布 $P(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$

$L=ln[\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_1-\mu)^2}{2\sigma^2}},\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_2-\mu)^2}{2\sigma^2}},...,\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x_N-\mu)^2}{2\sigma^2}}]$

$\frac{\partial L}{\partial \mu}=0  \to \mu=\frac{x_1+x_2+...+x_N}{N}$

$\frac{\partial L}{\partial \sigma}=0 \to \sigma^2=\frac{\sum_{i=1}^N(x_i-\mu)^2}{N}$ (有偏方差)

# 误差的高斯分布与最小二乘估计的等价性
$x_1,x_2,...,x_N, x_i \in \mathbb{R}^n$

$y_1,y_2,...,y_N, y_i \in \mathbb{R}^n$

$\hat{y_i}=w^\top x_i, w \in \mathbb{R}^n$

拟合误差： $e_i=y_i - w^\top x_i$

若设$e_i-\frac{1}{\sqrt{2\pi}}e^{\frac{e_i^2}{2}}$

似然函数： $L=ln[\frac{1}{\sqrt{2\pi}}e^{\frac{e_1^2}{2}} \frac{1}{\sqrt{2\pi}}e^{\frac{e_2^2}{2}}...\frac{1}{\sqrt{2\pi}}e^{\frac{e_N^2}{2}}]=-Nln\sqrt{2\pi}-\frac{1}{2}(e_1^2+e_2^2+...+e_N^2)$

最大化L等价于最小化 $e_1^2+e_2^2+...+e_N^2$

$min(y_1-w^\top x_1)^2+(y_2-w^\top x_2)^2+...+(y_N-w^\top x_N)^2=J$

$\frac{\partial J}{\partial w}=0 \to \sum_{i=1}^{N}x_iy_i=\sum_{i=1}^N w^\top x_i x_i$

# 无约束优化
无约束优化问题是机器学习中最普遍，最简单的优化问题： $x^*=min_{x}f(x)$

## 梯度下降
沿负梯度方向
## 牛顿法
$f(x_{t+1})=f(x_t)+f^"(x_t)(x_{t+1}-x_t)$
## 拟牛顿
## 共轭梯度
## 收敛速度
梯度下降是一次收敛，牛顿是二次收敛（速度快，但接近最优点才收敛,否则发散）
# 有约束最优化
## 拉格朗日乘子法(约束为等式)
$min_x f(x)$

$s.t.g(x)=0$

$L(x,\lambda)=f(x)+\lambda g(x)$

## KKT （约束为不等式，表示一个范围）
